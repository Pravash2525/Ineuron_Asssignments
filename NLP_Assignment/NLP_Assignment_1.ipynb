{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "969f761b",
   "metadata": {},
   "source": [
    "<h1> <u> <font color= green > NLP_Assignment_1 </font> </u> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084d7261",
   "metadata": {},
   "source": [
    "## 1. Explain One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e21b24",
   "metadata": {},
   "source": [
    "> **One-hot encoding** is a technique used to convert categorical data into a numerical format that can be used by machine learning algorithms. **Categorical data** is data that can be categorized into different groups, such as colors, countries, or species. **Machine learning algorithms** typically require numerical data, so one-hot encoding is used to represent categorical data in a way that the algorithm can understand.\n",
    "\n",
    "> One-hot encoding works by **creating a new binary variable** for each unique category in the categorical data. For example, if the categorical data is a column of colors with the values \"red\", \"green\", and \"blue\", then one-hot encoding would create three new binary variables: one for \"red\", one for \"green\", and one for \"blue\". Each of these new binary variables would be set to 1 if the corresponding category is present in the original data, and 0 if it is not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a68588",
   "metadata": {},
   "source": [
    "## 2. Explain Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9241a76d",
   "metadata": {},
   "source": [
    "> The **bag-of-words (BoW) model** is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a **text** (such as a sentence or a document) is represented as the **bag** (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. \n",
    "\n",
    "> To create a **BoW** representation of a text, the first step is to **tokenize the text** into words. This means breaking the text down into individual words, and removing any punctuation or other non-word characters. Once the text has been tokenized, the next step is to **create a vocabulary** of all the unique words that appear in the text. The vocabulary is typically a list of the words in the text, sorted by frequency.\n",
    "\n",
    "> The final step is to **create a bag-of-words** representation of the text. This is done by creating a vector that represents the number of times each word in the vocabulary appears in the text. The **vector** is typically a binary vector, where 1 indicates that the word appears in the text and 0 indicates that it does not.\n",
    "\n",
    "> For example, the following text:\n",
    "```\"The quick brown fox jumps over the lazy dog.\"```\n",
    "\n",
    "> would be tokenized into the following words:\n",
    "```\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"```\n",
    "\n",
    "> The vocabulary for this text would be the following list of words, sorted by frequency:\n",
    "\n",
    "                        \"the\"\n",
    "                        \"quick\"\n",
    "                        \"brown\"\n",
    "                        \"fox\"\n",
    "                        \"jumps\"\n",
    "                        \"over\"\n",
    "                        \"lazy\"\n",
    "                        \"dog\"\n",
    "\n",
    "> The bag-of-words representation of this text would be the following vector:\n",
    "```[1, 1, 1, 1, 1, 1, 1, 1]```\n",
    "\n",
    "> This vector indicates that the word \"the\" **appears 1 time in the text** , the word \"quick\" appears 1 time in the text, and so on.\n",
    "\n",
    "> The BoW model is a simple and effective way to represent text for machine learning algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ffabb4",
   "metadata": {},
   "source": [
    "## 3. Explain Bag of N-Grams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0699db2c",
   "metadata": {},
   "source": [
    "> A **bag-of-n-grams model** is a type of bag-of-words model that represents a text as a bag of n-grams. An **n-gram** is a sequence of n words, so a bag-of-n-grams model counts the number of times each n-gram appears in a text.\n",
    "\n",
    "> For example, a **bag-of-bigrams model** would represent the text ```\"The quick brown fox jumps over the lazy dog\"``` as a bag of the following bigrams:\n",
    "\n",
    "                    \"The quick\"\n",
    "                    \"quick brown\"\n",
    "                    \"brown fox\"\n",
    "                    \"fox jumps\"\n",
    "                    \"jumps over\"\n",
    "                    \"over the\"\n",
    "                    \"the lazy\"\n",
    "                    \"lazy dog\"\n",
    "\n",
    "> The **bag-of-n-grams model** is a simple and effective way to represent text for machine learning algorithms. It is easy to understand and implement, and it is relatively insensitive to the order of words in a document. This makes it a good choice for tasks such as document classification, text clustering, and information retrieval.\n",
    "\n",
    "> However, the **bag-of-n-grams model** does not take into account the order of words in a document. This can be a limitation for tasks that require the order of words to be preserved, such as sentiment analysis and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30baa6d",
   "metadata": {},
   "source": [
    "## 4. Explain TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c848d1",
   "metadata": {},
   "source": [
    "> **TF-IDF** stands for **term frequencyâ€“inverse document frequency**. It is a statistical measure that is used to evaluate how relevant a word is to a document in a collection of documents.\n",
    "\n",
    "> TF-IDF is calculated by multiplying two different metrics:\n",
    "\n",
    "                Term frequency (TF): This is the number of times a word appears in a document.\n",
    "                Inverse document frequency (IDF): This is a measure of how common a word is in a collection of documents.\n",
    "\n",
    "> The TF-IDF score for a word is calculated as follows: <br>\n",
    "```TF-IDF = TF * IDF```\n",
    "\n",
    "                where:\n",
    "\n",
    "                    TF is the term frequency of the word in the document.\n",
    "                    IDF is the inverse document frequency of the word in the collection of documents.\n",
    "\n",
    "> The **term frequency** is simply the number of times the word appears in the document. The **inverse document frequency** is a measure of how common the word is in the collection of documents. \n",
    "\n",
    "> The **TF-IDF score** is a measure of how important a word is to a document in a collection of documents. Words that appear frequently in a document but are also common in the collection of documents will have a **low TF-IDF score**. Words that appear infrequently in a document but are rare in the collection of documents will have a **high TF-IDF score**.\n",
    "\n",
    "> **TF-IDF** is a popular measure for text analysis tasks such as document classification, text clustering, and information retrieval. It is a simple and effective way to measure the importance of words in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a21a2b9",
   "metadata": {},
   "source": [
    "## 5. What is OOV problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5f9b33",
   "metadata": {},
   "source": [
    "> **Out-of-vocabulary (OOV) problem** is a problem that occurs when a word or phrase does not appear in the vocabulary of a machine learning model. This can happen when the model is trained on a dataset that does not contain the word or phrase, or when the word or phrase is a new word that has been created since the model was trained.\n",
    "\n",
    "> The OOV problem can have a significant impact on the performance of a machine learning model. If a word or phrase is not in the vocabulary, the model will not be able to represent it, and this can lead to errors in the model's predictions.\n",
    "\n",
    "> There are a number of techniques that can be used to **address the OOV problem**. \n",
    "> * One technique is to simply **ignore words that are not in the vocabulary**. However, this can lead to a loss of information, and it can also make the model less accurate.\n",
    "> * Another technique is to **replace words that are not in the vocabulary with similar words** that are in the vocabulary. This can be done using a technique called word embeddings. **Word embeddings** are a way of representing words as vectors of numbers. These vectors are typically trained on a large corpus of text, and they capture the meaning of words in a way that is independent of the context in which they appear.\n",
    "> * Another technique is to **subsample the training data**. This means that the model is only trained on a subset of the training data. This can help to reduce the number of OOV words in the training data.\n",
    "\n",
    "> Here are some of the most common ways to handle OOV words:\n",
    "> * **Treating OOV words as UNK**: This is the simplest approach, and it involves simply replacing OOV words with a special token called UNK. This token typically represents \"unknown\" or \"unseen\".\n",
    "> * **Using a word embedding model**: This approach involves using a word embedding model to represent OOV words as vectors of numbers. These vectors can then be used by the machine learning model to represent the meaning of OOV words.\n",
    "> * **Using a back-off model**: This approach involves using a combination of a word embedding model and a simple model that treats OOV words as UNK. The back-off model is used when the word embedding model is unable to represent an OOV word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f1fb6f",
   "metadata": {},
   "source": [
    "## 6. What are word embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaed3c60",
   "metadata": {},
   "source": [
    "> **Word embeddings** are a way of representing words as vectors of numbers. These vectors are typically **trained on a large corpus of text**, and they capture the meaning of words in a way that is independent of the context in which they appear.\n",
    "\n",
    "> **Word embeddings** are a powerful tool for natural language processing (NLP). They can be used for a variety of tasks, such as:\n",
    "> * **Text classification**: Word embeddings can be used to represent text as a vector of numbers. This vector can then be used to classify the text into different categories, such as spam or ham, or positive or negative sentiment.\n",
    "> * **Machine translation**: Word embeddings can be used to represent words in different languages as similar vectors. This can then be used to translate text from one language to another.\n",
    "> * **Question answering**: Word embeddings can be used to represent questions and answers as vectors of numbers. This can then be used to answer questions by comparing the vectors of the question and the answer.\n",
    "\n",
    "> Here are some of the most common **word embedding models**:\n",
    "> * **Word2Vec**: Word2Vec is a neural network model that was developed by Google. It is a popular choice for word embedding because it is relatively simple to train and it produces good results.\n",
    "> * **Glove**: GloVe is a statistical model that was developed by Stanford University. It is a popular choice for word embedding because it is able to capture long-range dependencies between words.\n",
    "> * **FastText**: FastText is a neural network model that was developed by Facebook. It is a popular choice for word embedding because it is able to handle out-of-vocabulary words.\n",
    "\n",
    "> There are a number of different ways to train word embeddings, but the most common approach is to use a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a540d2d6",
   "metadata": {},
   "source": [
    "## 7. Explain Continuous bag of words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9938b51b",
   "metadata": {},
   "source": [
    "> **Continuous bag-of-words (CBOW)** is a neural network model for natural language processing tasks such as language translation and text classification. It predicts a target word based on the context of the surrounding words. The CBOW model is a simplified version of the **skip-gram model**. \n",
    "\n",
    "> In the **CBOW model**, the input to the neural network is a window of surrounding words, and the output is the target word. The **neural network learns** to predict the target word by predicting the probability of each word in the vocabulary given the context words.\n",
    "\n",
    "> Here is an example of how the CBOW model works:\n",
    "> * Consider the sentence ```\"The quick brown fox jumps over the lazy dog.\"```\n",
    "> * The CBOW model would take a window of surrounding words, such as \"quick brown fox\", and try to predict the target word, \"jumps\".\n",
    "> * The neural network would learn to predict the target word by **predicting the probability of each word in the vocabulary** given the context words.\n",
    "> * For example, the neural network might learn that the probability of the word \"jumps\" is high given the context words \"quick brown fox\".\n",
    "> * Once the neural network is trained, it can be used to **represent words as vectors** of numbers. These vectors can then be used for a variety of tasks, such as <b><i> text classification, machine translation, and question answering </i></b>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c792ed0c",
   "metadata": {},
   "source": [
    "## 8. Explain SkipGram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e50b269",
   "metadata": {},
   "source": [
    "> **Skip-gram** is a neural network model for natural language processing tasks such as language translation and text classification. It predicts the context words given a target word.\n",
    "\n",
    "> The **skip-gram model** is a more complex version of the CBOW model. In the **skip-gram model**, the input to the neural network is the target word, and the output is a window of surrounding words. The neural network learns to predict the context words by predicting the probability of each word in the vocabulary given the target word.\n",
    "\n",
    "> The skip-gram model is a **more powerful model than the CBOW model** because it takes into account the order of words in a sentence. This makes it better at capturing the semantic relationships between words.\n",
    "\n",
    "> Here is an example of how the skip-gram model works:\n",
    "> * Consider the sentence ```\"The quick brown fox jumps over the lazy dog.\"```\n",
    "> * The skip-gram model would take the target word \"jumps\" as input, and try to predict the context words, \"quick brown fox\" and \"lazy dog\".\n",
    "> * The neural network would learn to predict the context words by predicting the probability of each word in the vocabulary given the target word.\n",
    "> * For example, the neural network might learn that the probability of the word \"quick\" is high given the target word \"jumps\".\n",
    "> * Once the neural network is trained, it can be used to represent words as vectors of numbers. These vectors can then be used for a variety of tasks, such as text classification, machine translation, and question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fb4ec6",
   "metadata": {},
   "source": [
    "## 9. Explain Glove Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961ed95c",
   "metadata": {},
   "source": [
    "> **GloVe (Global Vectors for Word Representation)** is a statistical method for learning word embeddings. It is a **count-based model** that uses global co-occurrence statistics to train word vectors.\n",
    "\n",
    "> **GloVe** is a popular choice for word embedding because it is able to capture long-range dependencies between words. This is because GloVe uses global co-occurrence statistics, which means that it considers all of the words that appear in a corpus, not just the words that appear in a window around a target word.\n",
    "\n",
    "> GloVe is also a relatively efficient model to train. This is because it does **not require the use of a neural network**, which can be computationally expensive.\n",
    "\n",
    "> Here is <b><i> how GloVe works </i></b>:\n",
    "> * First, a corpus of text is tokenized into words.\n",
    "> * Then, a co-occurrence matrix is created. The co-occurrence matrix is a matrix that shows how often each word appears in the same context as each other word.\n",
    "> * The co-occurrence matrix is used to train a word embedding model. The word embedding model learns to predict the probability of a word appearing in a context.\n",
    "> * The word embedding model is used to create word vectors. The word vectors are a representation of the meaning of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60f3342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b57b36a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
