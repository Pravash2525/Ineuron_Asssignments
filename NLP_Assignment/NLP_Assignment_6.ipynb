{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "130dc62d",
   "metadata": {},
   "source": [
    "<h1> <u> <font color= green > NLP_Assignment_6 </font> </u> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9369a14",
   "metadata": {},
   "source": [
    "## 1. What are Vanilla autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac85cd4f",
   "metadata": {},
   "source": [
    "> A **vanilla autoencoder** is a type of neural network that is used to learn the latent representation of data. It is a simple but effective model that can be used for a variety of tasks, such as dimensionality reduction, anomaly detection, and image compression.\n",
    "\n",
    "> A **vanilla autoencoder** consists of two parts: an encoder and a decoder. \n",
    "> * The **encoder** is responsible for learning the latent representation of the data. The encoder typically consists of a stack of layers that are used to compress the data into a lower-dimensional representation.\n",
    "> * while the **decoder** is responsible for reconstructing the original data from the latent representation. The decoder typically consists of a stack of layers that are used to reconstruct the original data from the latent representation.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30146cea",
   "metadata": {},
   "source": [
    "## 2. What are Sparse autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e175a",
   "metadata": {},
   "source": [
    "> **Sparse autoencoders** are a type of autoencoder that is designed to learn a sparse representation of the data. A **sparse representation** is a representation in which most of the elements are zero, and only a few elements are non-zero.\n",
    "\n",
    "> **Sparse autoencoders** are typically used for dimensionality reduction and feature extraction. They can also be used for anomaly detection and image compression.\n",
    "\n",
    "> **Sparse autoencoders work** by adding a penalty to the loss function that encourages the hidden representation to be sparse. This penalty is typically a regularization term that is added to the loss function. The **regularization term** penalizes the encoder for using too many non-zero elements in the hidden representation. This encourages the encoder to learn a sparse representation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad010ce",
   "metadata": {},
   "source": [
    "## 3. What are Denoising autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6df6c8",
   "metadata": {},
   "source": [
    "> **Denoising autoencoders** are a type of autoencoder that is used to learn a robust representation of data in the presence of noise. They are typically used for image denoising, but they can also be used for other tasks such as text denoising and audio denoising.\n",
    "\n",
    "> **Denoising autoencoders work** by adding noise to the input data before it is fed into the encoder. The encoder then learns to reconstruct the original data from the noisy input. The **decoder** is then used to reconstruct the original data from the latent representation.\n",
    "\n",
    "> The **noise that is added** to the input data can be Gaussian noise, salt and pepper noise, or any other type of noise. The type of noise that is added depends on the task that the denoising autoencoder is being used for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2752e9a",
   "metadata": {},
   "source": [
    "## 4. What are Convolutional autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8846d455",
   "metadata": {},
   "source": [
    "> **Convolutional autoencoders (CAEs)** are a type of autoencoder that uses convolutional neural networks (CNNs) to learn the latent representation of data. CAEs are typically used for image processing tasks, such as image denoising, image compression, and image reconstruction.\n",
    "\n",
    "> **CAEs work** by first using a convolutional encoder to extract features from the input image. The convolutional encoder typically consists of a stack of convolutional layers, followed by a max-pooling layer. The max-pooling layer helps to reduce the size of the feature map while preserving the most important features.\n",
    "\n",
    "> The **extracted features** are then passed to a decoder, which is also a convolutional neural network. The decoder typically consists of a stack of convolutional layers, followed by a upsampling layer. The upsampling layer helps to reconstruct the original image from the latent representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f9b3a2",
   "metadata": {},
   "source": [
    "## 5. What are Stacked autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb22aaa2",
   "metadata": {},
   "source": [
    "> **Stacked autoencoders** are a type of autoencoder that consists of multiple autoencoders stacked on top of each other. Each autoencoder in the stack learns a latent representation of the data from the previous autoencoder. The **final autoencoder** in the stack reconstructs the original data from the latent representation of the last autoencoder.\n",
    "\n",
    "> **Stacked autoencoder**s are typically used for dimensionality reduction, feature extraction, and anomaly detection. They can also be used for image compression and image reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bf23bd",
   "metadata": {},
   "source": [
    "## 6. Explain how to generate sentences using LSTM autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bafbb6",
   "metadata": {},
   "source": [
    "> generate sentences using LSTM autoencoders by applying following steps:\n",
    "> * **Encode the input sentence**: The first step is to encode the input sentence into a latent representation. This is done by passing the sentence through an LSTM autoencoder. The LSTM autoencoder will learn a latent representation of the sentence that captures the most important information in the sentence.\n",
    "> * **Generate the output sentence**: The next step is to generate the output sentence from the latent representation. This is done by passing the latent representation through the decoder of the LSTM autoencoder. The decoder will generate the output sentence one word at a time, using the latent representation to predict the next word.\n",
    "> * **Repeat**: The process of encoding and decoding can be repeated to generate multiple sentences. This can be useful for tasks such as text summarization and machine translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b85c5c8",
   "metadata": {},
   "source": [
    "## 7. Explain Extractive summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c6940",
   "metadata": {},
   "source": [
    "> **Extractive summarization** is a type of text summarization that selects important sentences from a text document and combines them to create a summary. **Extractive summarization** is one of the most common types of text summarization, and it is relatively easy to implement.\n",
    "\n",
    "> There are two main steps involved in extractive summarization:\n",
    "> 1. **Sentence extraction**: The first step is to extract the most important sentences from the text document. This can be done using a variety of methods, such as:\n",
    "    * **Relevance**: The sentences that are most relevant to the topic of the document are typically the most important.\n",
    "    * **Length**: The sentences that are the most concise are typically the most important.\n",
    "    * **Position**: The sentences that are located at the beginning or end of the document are typically the most important.\n",
    "> 2. **Sentence combination**: The next step is to combine the extracted sentences to create a summary. This can be done using a variety of methods, such as:\n",
    "    * **Concatenation**: The sentences can simply be concatenated together to create a summary.\n",
    "    * **Paraphrasing**: The sentences can be paraphrased to create a more concise and coherent summary.\n",
    "    * **Tagging**: The sentences can be tagged with keywords to help with the combination process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd739a60",
   "metadata": {},
   "source": [
    "## 8. Explain Abstractive summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec2dc5c",
   "metadata": {},
   "source": [
    "> **Abstractive summarization** is a type of text summarization that creates a new summary that is more concise and informative than the original text. **Abstractive summarization** is more challenging than extractive summarization, but it can produce summaries that are more accurate and engaging.\n",
    "\n",
    "> There are two main steps involved in abstractive summarization:\n",
    "> 1. **Text understanding**: The first step is to understand the meaning of the text document. This can be done using a variety of methods, such as:\n",
    "    * **Natural language processing**: Natural language processing techniques can be used to identify the key concepts in the text document.\n",
    "    * **Machine translation**: Machine translation techniques can be used to translate the text document into a different language, which can help to reveal new insights into the meaning of the text.\n",
    "> 2. **Summary generation**: The next step is to generate a summary that captures the main points of the text document. This can be done using a variety of methods, such as:\n",
    "    * **Rule-based summarization**: Rule-based summarization techniques can be used to generate summaries based on a set of rules.\n",
    "    * **Machine learning**: Machine learning techniques can be used to train a model to generate summaries that are similar to human-written summaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1124864b",
   "metadata": {},
   "source": [
    "## 9. Explain Beam search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452b16cb",
   "metadata": {},
   "source": [
    "> **Beam search** is a decoding algorithm that is used in natural language processing to find the most likely sequence of words given a sequence of input tokens. Beam search is a greedy algorithm, which means that it always chooses the most likely next word at each step.\n",
    "\n",
    "> **Beam search works** by maintaining a beam of hypotheses, which are possible sequences of words. The **beam** is initialized with a single hypothesis, which is the empty sequence. At each step, the beam is expanded by adding the most likely next word to each hypothesis in the beam. The beam is then pruned to the top k hypotheses, where k is the beam width. The process is repeated until the end of the input sequence is reached.\n",
    "\n",
    "> The **beam width** is a hyperparameter that controls the number of hypotheses that are considered at each step. A **larger beam width** will result in more accurate decoding, but it will also be more computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b206f0",
   "metadata": {},
   "source": [
    "## 10. Explain Length normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f9eb7e",
   "metadata": {},
   "source": [
    "> **Length normalization** is a technique used in natural language processing to normalize the length of sequences before they are processed by a model. This is done to prevent longer sequences from having an unfair advantage over shorter sequences.\n",
    "\n",
    "> There are a few different ways to **perform length normalization**. \n",
    "> * One common approach is to **divide the score of each sequence** by its length. This ensures that the scores of all sequences are on the same scale, regardless of their length.\n",
    "> * Another approach to length normalization is to **use a sliding window**. This involves dividing the sequence into a fixed number of windows, and then normalizing the score of each window. This approach can be more effective than simply dividing by the length of the sequence, as it allows the model to take into account the relative importance of different parts of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a068f0",
   "metadata": {},
   "source": [
    "## 11. Explain Coverage normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27905a0",
   "metadata": {},
   "source": [
    "> **Coverage normalization** is a technique used in natural language processing to normalize the coverage of the target sequence by a model. This is done to prevent models from generating outputs that are simply a repetition of the input sequence.\n",
    "\n",
    "> **Coverage normalization** is typically used in machine translation and text summarization tasks. \n",
    "> * **In machine translation**, coverage normalization ensures that the model generates outputs that are not simply a word-for-word translation of the input sequence. \n",
    "> * **In text summarization**, coverage normalization ensures that the model generates outputs that cover the main points of the input sequence.\n",
    "\n",
    "> There are a few different ways to **perform coverage normalization**. One common approach is to **use a sliding window**. This involves dividing the target sequence into a fixed number of windows, and then normalizing the coverage of each window. This approach can be more effective than simply dividing by the length of the target sequence, as it allows the model to take into account the relative importance of different parts of the target sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4bba19",
   "metadata": {},
   "source": [
    "## 12. Explain ROUGE metric evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e521c23e",
   "metadata": {},
   "source": [
    "> **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)** is a set of metrics for evaluating the quality of text summarization. It is a recall-based metric, which means that it measures the overlap between the generated summary and the reference summary.\n",
    "\n",
    "> There are a few different ROUGE metrics, but the most common are <b><i> ROUGE-N, ROUGE-L, and ROUGE-S </i></b>. \n",
    "> * **ROUGE-N** measures the overlap between the generated summary and the reference summary at the n-gram level. \n",
    "> * **ROUGE-L** measures the overlap between the generated summary and the reference summary at the longest common subsequence (LCS) level. \n",
    "> * **ROUGE-S** measures the overlap between the generated summary and the reference summary at the skip-gram level.\n",
    "\n",
    "> The **ROUGE metrics are calculated** by dividing the number of overlapping n-grams, LCS, or skip-grams by the total number of n-grams, LCS, or skip-grams in the reference summary. The scores are then averaged over all n-grams, LCS, or skip-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6125d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909964f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
