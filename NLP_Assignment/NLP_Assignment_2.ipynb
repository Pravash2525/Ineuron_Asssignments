{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f79c2d04",
   "metadata": {},
   "source": [
    "<h1> <u> <font color= green > NLP_Assignment_2 </font> </u> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d62b48",
   "metadata": {},
   "source": [
    "## 1. What are Corpora?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd8cd07",
   "metadata": {},
   "source": [
    "> A **corpus** is a large and structured collection of text. **Corpora** are used in natural language processing (NLP) to train machine learning models. They can also be used to study the properties of language.\n",
    "\n",
    "> There are **different types of corpora**, but they typically fall into one of two categories: monolingual or multilingual. **Monolingual corpora** are corpora that contain text in a single language. **Multilingual corpora** contain text in multiple languages.\n",
    "\n",
    "> Corpora can be collected from a variety of sources, that may includes:\n",
    "> * **Published text**: This includes books, articles, and other documents that have been published.\n",
    "> * **Web text**: This includes text that is found on the web.\n",
    "> * **Speech**: This includes audio recordings of speech.\n",
    "> * **Code**: This includes source code and other computer-generated text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037018dd",
   "metadata": {},
   "source": [
    "## 2. What are Tokens?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69da9f60",
   "metadata": {},
   "source": [
    "> In natural language processing (NLP), a **token** is a unit of text that is used to represent a word, phrase, or other meaningful element of a text. Tokens are typically created by breaking down a text into smaller units, such as words, punctuation marks, and numbers.\n",
    "\n",
    "> There are a different **ways to tokenize text**. \n",
    "> * One common approach is to use a regular expression to match patterns of characters that represent tokens. For example, a regular expression could be used to match all of the words in a text, as well as punctuation marks and numbers.\n",
    "> * Once a text has been tokenized, the tokens can be used to represent the text in a machine-readable format. This can be useful for a variety of NLP tasks, such as text classification, machine translation, and question answering.\n",
    "\n",
    "> Here are some of the **benefits of using tokens**:\n",
    "> * They can be used to represent text in a machine-readable format.\n",
    "> * They can be used to analyze the structure of text.\n",
    "> * They can be used to represent the meaning of text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e89ce1",
   "metadata": {},
   "source": [
    "## 3. What are Unigrams, Bigrams, Trigrams?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb0b3a2",
   "metadata": {},
   "source": [
    "> In natural language processing, unigrams, bigrams, and trigrams are terms **used to describe sequences of words**. A **unigram** is a single word, a **bigram** is a sequence of two words, and a **trigram** is a sequence of three words.\n",
    "\n",
    "> These terms are often used in the context of bag-of-words models, which represent text as a **collection of words** without considering the order in which they appear. \n",
    "> * For example, the sentence ```\"The quick brown fox jumps over the lazy dog\"``` would be represented as a bag-of-words with 11 unigrams: \"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\", and \".\".\n",
    "\n",
    "> Bigrams and trigrams can be used to capture the order of words in a text. \n",
    "> * For example, the **bigram** \"quick brown\" would be used to represent the fact that the words \"quick\" and \"brown\" often appear together in a text. \n",
    "> * Similarly, the **trigram** \"brown fox jumps\" would be used to represent the fact that the words \"brown\", \"fox\", and \"jumps\" often appear together in a text.\n",
    "\n",
    "> Bigrams and trigrams can be used to **improve the accuracy** of natural language processing models. For example, a model that is trained on a corpus of text that includes bigrams and trigrams is likely to be more accurate at predicting the next word in a sequence than a model that is only trained on unigrams.\n",
    "\n",
    "> Here are some examples of unigrams, bigrams, and trigrams:\n",
    "> * **Unigrams**: the, quick, brown, fox, jumps, over, the, lazy, dog, .\n",
    "> * **Bigrams:** quick brown, brown fox, fox jumps, jumps over, over the, the lazy, lazy dog, dog .\n",
    "> * **Trigrams**: quick brown fox, brown fox jumps, fox jumps over, jumps over the, over the lazy, the lazy dog.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def23c7c",
   "metadata": {},
   "source": [
    "## 4. How to generate n-grams from text?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8797f0b",
   "metadata": {},
   "source": [
    "> Here are the steps on how to generate n-grams from text:\n",
    "> * **Tokenize the text**: This means breaking the text down into individual words or tokens.\n",
    "> * **Create a list of all possible n-grams**: This can be done by starting with the first n words in the text and then adding the next word to each n-gram until the end of the text is reached.\n",
    "> * **Remove any n-grams that appear less than a certain number of times**: This is called filtering. The number of times an n-gram must appear before it is not filtered out is called the minimum frequency.\n",
    "> * **Sort the n-grams by frequency**: This will give you the most common n-grams first.\n",
    "\n",
    "> Here is an example of how to generate bigrams from the text ```\"The quick brown fox jumps over the lazy dog\":```\n",
    "\n",
    "<b><i> 1. Tokenize the text: </i></b> \n",
    "```\n",
    "tokens = [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
    "```\n",
    "<b><i> 2. Create a list of all possible bigrams: </i></b> \n",
    "```\n",
    "bigrams = []\n",
    "for i in range(len(tokens) - 1):\n",
    "     bigram = tokens[i] + \" \" + tokens[i + 1]\n",
    "     bigrams.append(bigram)\n",
    "```\n",
    "<b><i> 3. Remove any bigrams that appear less than a certain number of times: </i></b> \n",
    "```\n",
    "min_frequency = 2\n",
    "filtered_bigrams = []\n",
    "for bigram in bigrams:\n",
    "    if bigrams.count(bigram) >= min_frequency:\n",
    "        filtered_bigrams.append(bigram)\n",
    "```\n",
    "<b><i> 4. Sort the bigrams by frequency: </i></b> \n",
    "```\n",
    "filtered_bigrams.sort(key=lambda bigram: bigrams.count(bigram), reverse=True)\n",
    "```\n",
    "\n",
    "\n",
    "> The output of this code would be a list of the most common bigrams in the text, sorted by frequency. For example, the first bigram in the list might be \"the quick\", followed by \"quick brown\", and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccfd852",
   "metadata": {},
   "source": [
    "<b><i> Python code that can generate n-grams from text: </i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f118fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generate_ngrams(text, n):\n",
    "    tokens = re.split(\" \", text)\n",
    "    bigrams = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        bigram = \" \".join(tokens[i:i + n])\n",
    "        bigrams.append(bigram)\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "bigrams = generate_ngrams(text, 3)\n",
    "print(bigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450f839e",
   "metadata": {},
   "source": [
    "## 5. Explain Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6603cd92",
   "metadata": {},
   "source": [
    "> **Lemmatization** is the process of grouping together different inflected forms of a word so they can be analyzed as a single item. For example, the words \"runs,\" \"ran,\" and \"running\" would all be lemmatized to the word \"run.\"\n",
    "\n",
    "> Lemmatization is **similar to stemming**, but it is more sophisticated. **Stemming simply removes** the inflectional endings of words, while **lemmatization takes into account the meaning** of the words. This means that lemmatization can group together words that have different inflectional endings but have the same meaning.\n",
    "\n",
    "> Here are some of the benefits of using lemmatization:\n",
    "> * It can help to improve the accuracy of NLP tasks.\n",
    "> * It can help to reduce the ambiguity of words.\n",
    "> * It can help to make text more consistent.\n",
    "\n",
    "> examples of lemmatization:\n",
    "> * The word \"runs\" can be lemmatized to \"run\".\n",
    "> * The word \"was\" can be lemmatized to \"be\".\n",
    "> * The word \"ate\" can be lemmatized to \"eat\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58467561",
   "metadata": {},
   "source": [
    "## 6. Explain Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19222d93",
   "metadata": {},
   "source": [
    "> **Stemming** is a process of reducing inflected words to their word stem, **base or root form**. The stem is the part of the word that carries the basic meaning of the word. For example, the words \"running\", \"ran\", and \"runner\" all have the stem \"run\".\n",
    "\n",
    "> Here are some of the benefits of using stemming:\n",
    "> * It can help to improve the accuracy of NLP tasks.\n",
    "> * It can help to reduce the size of the vocabulary that needs to be stored.\n",
    "> * It can help to make text more consistent.\n",
    "\n",
    "> examples of stemming:\n",
    "> * The word \"running\" can be stemmed to \"run\".\n",
    "> * The word \"was\" can be stemmed to \"was\".\n",
    "> * The word \"ate\" can be stemmed to \"eat\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005828bd",
   "metadata": {},
   "source": [
    "## 7. Explain Part-of-speech (POS) tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f5a08b",
   "metadata": {},
   "source": [
    "> **Part-of-speech (POS) tagging** is the process of assigning a part-of-speech tag to each word in a sentence. **Part-of-speech tags** are labels that indicate the syntactic category of a word, such as noun, verb, adjective, adverb, preposition, conjunction, pronoun, or interjection.\n",
    "\n",
    "> **POS tagging** is a fundamental task in natural language processing (NLP). It is used in a variety of NLP tasks, such as text classification, machine translation, and question answering.\n",
    "\n",
    "> There are two main **types of POS tagging**: rule-based and statistical. \n",
    "> * **Rule-based POS taggers** use a set of rules to assign part-of-speech tags to words. \n",
    "> * **Statistical POS taggers** use a statistical model to learn how to assign part-of-speech tags to words.\n",
    "\n",
    "> Here are some of the benefits of using POS tagging:\n",
    "> * It can help to improve the accuracy of NLP tasks.\n",
    "> * It can help to make text more consistent.\n",
    "> * It can help to understand the meaning of text.\n",
    "\n",
    "> examples of POS tags:\n",
    "> * **Noun**: dog, cat, table, chair\n",
    "> * **Verb**: run, jump, eat, sleep\n",
    "> * **Adjective**: big, small, red, blue\n",
    "> * **Adverb**: quickly, slowly, loudly, quietly\n",
    "> * **Preposition**: in, on, under, over\n",
    "> * **Conjunction**: and, or, but, yet\n",
    "> * **Pronoun**: I, you, he, she, it\n",
    "> * **Interjection**: oh, wow, ouch, shh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d758765",
   "metadata": {},
   "source": [
    "## 8. Explain Chunking or shallow parsing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea9353c",
   "metadata": {},
   "source": [
    "> **Chunking or shallow parsing** is a natural language processing task that involves grouping words together into phrases or chunks based on their grammatical relationships. For example, the sentence ```\"The quick brown fox jumps over the lazy dog\"``` could be chunked into the following phrases:\n",
    "\n",
    "                    The quick brown fox\n",
    "                    jumps over\n",
    "                    the lazy dog\n",
    "\n",
    "> Chunking is a less complex task than full parsing, which involves **identifying the syntactic structure** of an entire sentence. However, chunking can still be useful for a variety of natural language processing tasks, such as text classification, machine translation, and question answering.\n",
    "\n",
    "> There are two main types of chunkers: rule-based and statistical. \n",
    "> * **Rule-based chunkers** use a set of rules to identify chunks. \n",
    "> * **Statistical chunkers** use a statistical model to learn how to identify chunks.\n",
    "\n",
    "> Here are some of the benefits of using chunking:\n",
    "> * It can help to improve the accuracy of NLP tasks.\n",
    "> * It can help to make text more consistent.\n",
    "> * It can help to understand the meaning of text.\n",
    "\n",
    "> some examples of chunks:\n",
    "> * **Noun phrase**: The quick brown fox\n",
    "> * **Verb phrase**: jumps over\n",
    "> * **Prepositional phrase**: over the lazy dog\n",
    "> * **Adjective phrase**: lazy dog\n",
    "\n",
    "> Chunks can be performed using a variety of tools and libraries. Some popular tools for chunking include:\n",
    "> * **Stanford CoreNLP** is a natural language processing toolkit that includes a chunker for English.\n",
    "> * **SpaCy** is a natural language processing library that includes a chunker for English.\n",
    "> * **NLTK** is a natural language processing library that includes a chunker for a variety of languages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01ed974",
   "metadata": {},
   "source": [
    "## 9. Explain Noun Phrase (NP) chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787a6c27",
   "metadata": {},
   "source": [
    "> **Noun phrase (NP) chunking** is a type of shallow parsing that involves grouping words together into noun phrases (NPs). NPs are phrases that contain a noun as their head word. For example, the sentence ```\"The quick brown fox jumps over the lazy dog\"``` could be chunked into the following NPs:\n",
    "\n",
    "                        The quick brown fox\n",
    "                        jumps over\n",
    "                        the lazy dog\n",
    "\n",
    "> There are two main types of NP chunkers: rule-based and statistical. \n",
    "> * **Rule-based NP chunkers** use a set of rules to identify NPs. \n",
    "> * **Statistical NP chunkers** use a statistical model to learn how to identify NPs.\n",
    "\n",
    "> examples of NPs:\n",
    "> * **Noun phrase**: The quick brown fox\n",
    "> * **Adjective phrase**: lazy dog\n",
    "> * **Prepositional phrase**: over the lazy dog\n",
    "> * **Verb phrase**: jumps over\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59275a97",
   "metadata": {},
   "source": [
    "## 10. Explain Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4762eb",
   "metadata": {},
   "source": [
    "> **Named Entity Recognition (NER)** is a natural language processing task that involves identifying named entities in text. **Named entities** are words or phrases that refer to specific things, such as <b><i> people, organizations, locations, dates, and times </i></b>. For example, the sentence ```\"The quick brown fox jumps over the lazy dog\"``` contains the following named entities:\n",
    "\n",
    "                        The quick brown fox: A person\n",
    "                        The lazy dog: A person\n",
    "\n",
    "> **NER** is a useful task for a variety of natural language processing tasks, such as information extraction, machine translation, and question answering. \n",
    "For example, \n",
    "> * **In information extraction**, NER can be used to extract information about people, organizations, and locations from text. \n",
    "> * **In machine translation**, NER can be used to ensure that named entities are translated correctly. \n",
    "> * **In question answering**, NER can be used to identify the entities that are being asked about in a question.\n",
    "\n",
    "> There are two main types of NER systems: rule-based and statistical. \n",
    "> * **Rule-based NER systems** use a set of rules to identify named entities. \n",
    "> * **Statistical NER systems** use a statistical model to learn how to identify named entities.\n",
    "\n",
    "> examples of named entities:\n",
    "> * **Person**: John Smith, Jane Doe\n",
    "> * **Organization**: Google, Microsoft, Amazon\n",
    "> * **Location**: New York City, Paris, London\n",
    "> * **Date**: February 25, 2023, 10:00 AM\n",
    "> * **Time**: 10:00 AM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7951dcf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a4c6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
