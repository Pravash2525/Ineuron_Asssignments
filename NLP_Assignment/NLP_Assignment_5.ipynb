{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f79c2d04",
   "metadata": {},
   "source": [
    "<h1> <u> <font color= green > NLP_Assignment_5 </font> </u> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581289c7",
   "metadata": {},
   "source": [
    "## 1. What are Sequence-to-sequence models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6665bf7f",
   "metadata": {},
   "source": [
    "> **Sequence-to-sequence models** are a type of recurrent neural network **(RNN)** that can be used to learn the relationship between two sequences of data. For example, a sequence-to-sequence model could be used to learn the relationship between a sentence in one language and its translation in another language.\n",
    "\n",
    "> **Sequence-to-sequence models work** by first encoding the input sequence into a fixed-length vector representation. This vector representation is then decoded into the output sequence. The decoding process is typically done using another RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed1da83",
   "metadata": {},
   "source": [
    "## 2. What are the Problem with Vanilla RNNs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a68a86",
   "metadata": {},
   "source": [
    "> problems with vanilla RNNs:\n",
    "> * **Vanishing gradients**: Vanilla RNNs are prone to the vanishing gradient problem. This means that the gradients can become very small as the RNN goes deeper, which can make it difficult for the RNN to learn long-term dependencies.\n",
    "> * **Exploding gradients**: Vanilla RNNs are also prone to the exploding gradient problem. This means that the gradients can become very large as the RNN goes deeper, which can cause the RNN to diverge.\n",
    "> * **Short-term memory**: Vanilla RNNs have a short-term memory. This means that they can only remember a limited amount of information from the past. This can be a problem for tasks that require long-term memory, such as machine translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73783357",
   "metadata": {},
   "source": [
    "## 3. What is Gradient clipping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51429c0f",
   "metadata": {},
   "source": [
    "> **Gradient clipping** is a technique used to prevent the gradients from becoming too large or too small. This can help to prevent the RNN from diverging or becoming stuck in a local minimum. **Gradient clipping works** by limiting the size of the gradients. This is done by setting a threshold on the maximum value of the gradients. If a gradient exceeds the threshold, it is clipped to the threshold value.\n",
    "\n",
    "> **Gradient clipping** is a simple but effective technique for preventing the vanishing and exploding gradient problems. It is often used in conjunction with other techniques, such as LSTMs and GRUs, to improve the performance of RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c30329a",
   "metadata": {},
   "source": [
    "## 4. Explain Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c19c80f",
   "metadata": {},
   "source": [
    "> **Attention mechanisms** are a way to allow RNNs to focus on specific parts of an input sequence. This can be helpful for tasks where the RNN needs to understand the context of the input sequence, such as machine translation and text summarization.\n",
    "\n",
    "> There are a few different ways to implement attention mechanisms. \n",
    "> * One common approach is to use a soft attention mechanism. A **soft attention mechanism works** by assigning a weight to each element of the input sequence. The weights are then used to compute a weighted sum of the elements of the input sequence. This weighted sum is then used as the input to the RNN.\n",
    "> * Another approach to implementing attention mechanisms is to use a hard attention mechanism. A **hard attention mechanism works** by selecting a single element of the input sequence. The selected element is then used as the input to the RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8579685",
   "metadata": {},
   "source": [
    "## 5. Explain Conditional random fields (CRFs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b2062e",
   "metadata": {},
   "source": [
    "> **Conditional random fields (CRFs)** are a type of statistical model that can be used for sequence labeling tasks. Sequence labeling tasks are tasks where the goal is to assign a label to each element in a sequence. For example, in part-of-speech tagging, the goal is to assign a part-of-speech tag to each word in a sentence.\n",
    "\n",
    "> **CRFs work by** modeling the conditional probability of a sequence of labels given a sequence of inputs. This means that CRFs can model the probability of a particular label occurring at a particular position in the sequence given the labels that have already occurred.\n",
    "\n",
    "> CRFs helps in the task like:\n",
    "> * **Part-of-speech tagging**: CRFs have been used to improve the accuracy of part-of-speech tagging models.\n",
    "> * **Named entity recognition**: CRFs have been used to improve the accuracy of named entity recognition models.\n",
    "> * **Chunking**: CRFs have been used to improve the accuracy of chunking models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c38a03",
   "metadata": {},
   "source": [
    "## 6. Explain self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ced5da0",
   "metadata": {},
   "source": [
    "> **Self-attention** is a mechanism that allows a model to attend to different parts of itself. This can be helpful for tasks where the model needs to understand the context of its own input, such as machine translation and text summarization.\n",
    "\n",
    "> **Self-attention works** by computing a weighted sum of the model's own outputs. The weights are computed using a similarity function, such as the dot product. The similarity function measures the similarity between the model's outputs at different positions.\n",
    "\n",
    "> The **weighted sum of the model's outputs** is then used as the input to the next layer of the model. This allows the model to focus on the most important parts of its own input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd38a45",
   "metadata": {},
   "source": [
    "## 7. What is Bahdanau Attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b4cef5",
   "metadata": {},
   "source": [
    "> **Bahdanau attention** is a soft attention mechanism, which means that it assigns a weight to each element in the input sequence. The weights are then used to compute a weighted sum of the elements of the input sequence. This **weighted sum** is then used as the input to the RNN.\n",
    "\n",
    "> **Bahdanau attention works** by computing a similarity score between the hidden state of the RNN and each element in the input sequence. The **similarity score** is computed using a function such as the dot product. The similarity scores are then normalized to obtain a probability distribution over the input sequence. The **probability distribution** is then used to compute the weighted sum of the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b2fc19",
   "metadata": {},
   "source": [
    "## 8. What is a Language Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e103a7f",
   "metadata": {},
   "source": [
    "> A **language model** is a statistical method that predicts the next word in a sequence of words. Language models are trained on large corpora of text, and they can be used for a variety of tasks, such as machine translation, text summarization, and question answering.\n",
    "\n",
    "> There are two main types of language models: statistical language models and neural language models. \n",
    "> * **Statistical language models** are based on the idea that the probability of a word occurring in a sequence is influenced by the words that have already occurred.  **Statistical language models** are typically trained using the **maximum likelihood estimation (MLE) algorithm**. MLE works by maximizing the likelihood of the training data. This means that the model is trained to predict the next word in the sequence that is most likely to occur given the words that have already occurred.\n",
    "> * **Neural language models**, on the other hand, are based on the idea that the probability of a word occurring in a sequence is influenced by the entire sequence. **Neural language models** are typically trained using the **backpropagation algorithm**. Backpropagation works by adjusting the weights of the neural network so that the model predicts the next word in the sequence more accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760e5d8b",
   "metadata": {},
   "source": [
    "## 9. What is Multi-Head Attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59052dcb",
   "metadata": {},
   "source": [
    "> **Multi-head attention** is a type of attention mechanism that allows the model to attend to different parts of the input sequence in different ways. This can be helpful for tasks where the model needs to understand the context of the input sequence, such as machine translation and text summarization.\n",
    "\n",
    "> **Multi-head attention works** by dividing the input sequence into multiple \"heads\". Each head attends to a different part of the input sequence. The outputs of the different heads are then combined to produce a single output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb26393f",
   "metadata": {},
   "source": [
    "## 10. What is Bilingual Evaluation Understudy (BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be08cfa",
   "metadata": {},
   "source": [
    "> **Bilingual Evaluation Understudy (BLEU)** is a metric for evaluating the quality of machine translation. BLEU is a **measure of the similarity between a machine-translated text and a human-translated text**. BLEU is calculated by comparing the n-grams of the machine-translated text to the n-grams of the human-translated text. BLEU has become the standard metric for evaluating machine translation, and it is used by many different machine translation systems.\n",
    "\n",
    "> BLEU is calculated using the following formula:<br>\n",
    "```BLEU = (1 + brevity penalty) * (n-gram precision) ** (weight)```\n",
    "\n",
    "        where:\n",
    "\n",
    "            brevity penalty: The brevity penalty is a factor that is used to penalize machine translations that are shorter than the human translations.\n",
    "            n-gram precision: The n-gram precision is the percentage of n-grams in the machine-translated text that also occur in the human-translated text.\n",
    "            weight: The weight is a factor that is used to weight the different n-grams.\n",
    "\n",
    "> The default n-grams for BLEU are 1-grams, 2-grams, and 3-grams. The **default weight** is 1.\n",
    "\n",
    "> BLEU has several advantages:\n",
    "> * It is a relatively simple metric to calculate.\n",
    "> * It is a good measure of the overall quality of a machine translation.\n",
    "> * It is widely used, so it is easy to compare the performance of different machine translation systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71665279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e70eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
