{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "130dc62d",
   "metadata": {},
   "source": [
    "<h1> <u> <font color= green > NLP_Assignment_3 </font> </u> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188584ec",
   "metadata": {},
   "source": [
    "## 1. Explain the basic architecture of RNN cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4166cb",
   "metadata": {},
   "source": [
    "> The basic architecture of an **RNN cell consists** of the following components:\n",
    "> * **Input gate**: This gate controls how much of the current input is passed to the cell's state.\n",
    "> * **Forget gate**: This gate controls how much of the cell's previous state is forgotten.\n",
    "> * **Output gate**: This gate controls how much of the cell's state is output.\n",
    "> * **Cell state**: This is the internal state of the cell, which is used to store information about the past inputs.\n",
    "\n",
    "> * The input gate, forget gate, and output gate are all implemented as **sigmoid neural networks**. The sigmoid function outputs a value between 0 and 1, which represents the amount of information that is passed through the gate.\n",
    "> * The **cell state** is updated using a function called the update gate. The **update gate** is also implemented as a sigmoid neural network, and it determines how much of the current input and the previous cell state is used to update the cell state.\n",
    "> * The output of the RNN cell is the **output gate**, which is a linear combination of the cell state and the current input.\n",
    "\n",
    "> The following diagram shows the <b><i> basic architecture of an RNN cell </i></b>:\n",
    "\n",
    "                    Input gate   Forget gate   Output gate   Update gate\n",
    "\n",
    "> The input gate, forget gate, and output gate are all connected to the **cell state**. The **update gate** is also connected to the current input. The output of the RNN cell is the output gate.\n",
    "\n",
    "> The **RNN cell** is a simple but powerful unit that can be used to learn long-term dependencies in sequences. By controlling how much of the past inputs and state are remembered, the RNN cell can learn to remember important information about the past and use it to make predictions about the future.\n",
    "\n",
    "> Here are some of the most common RNN cells:\n",
    "> * **Elman RNN**: This is the simplest type of RNN cell. It has a single sigmoid gate that controls how much of the current input is passed to the cell's state.\n",
    "> * **LSTM**: This is a more complex type of RNN cell that has three gates: input gate, forget gate, and output gate. LSTM cells are better at learning long-term dependencies than Elman RNN cells.\n",
    "> * **GRU**: This is a simplified version of the LSTM cell. It has two gates: update gate and reset gate. GRU cells are faster to train than LSTM cells, but they are not as good at learning long-term dependencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d14911b",
   "metadata": {},
   "source": [
    "## 2. Explain Backpropagation through time (BPTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893fdf27",
   "metadata": {},
   "source": [
    "> **Backpropagation through time (BPTT)** is an algorithm used to train recurrent neural networks (RNNs). **RNNs** are a type of neural network that can process sequential data, such as text or speech.\n",
    "\n",
    "> **BPTT works by** unrolling the RNN over time. This means that the RNN is treated as a feedforward neural network, but the input and output are sequences of data. The error at each time step is then backpropagated through the unrolled RNN, and the weights of the RNN are updated accordingly.\n",
    "\n",
    "> Here is an example of how BPTT works. \n",
    "> * Let's say we have an RNN that is trained to predict the next word in a sentence. \n",
    "> * The RNN is given the first word in the sentence as input, and it predicts the next word. The error between the predicted word and the actual word is then backpropagated through the RNN. \n",
    "> * This process is repeated for each word in the sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609221c9",
   "metadata": {},
   "source": [
    "## 3. Explain Vanishing and exploding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aefcec",
   "metadata": {},
   "source": [
    "> **Vanishing and exploding gradients** are two problems that can occur when training deep neural networks. They can make it difficult for the network to learn, and can lead to poor performance.\n",
    "\n",
    "> * **Vanishing gradients** occur when the gradients of the loss function with respect to the network weights become very small. This can happen when the activation function has a narrow range, such as the sigmoid function. When the gradients are very small, the updates to the network weights are also very small, which can make it difficult for the network to learn.\n",
    "> * **Exploding gradients** occur when the gradients of the loss function with respect to the network weights become very large. This can happen when the activation function has a wide range, such as the hyperbolic tangent function. When the gradients are very large, the updates to the network weights can also be very large, which can lead to instability in the training process.\n",
    "\n",
    "> Both vanishing and exploding gradients can make it difficult for the network to learn. **Vanishing gradients** can make it difficult for the network to learn long-term dependencies, while **exploding gradients** can make the training process unstable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63466db",
   "metadata": {},
   "source": [
    "## 4. Explain Long short-term memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bdc1e7",
   "metadata": {},
   "source": [
    "> **Long short-term memory (LSTM)** is a type of recurrent neural network (RNN) that is specifically designed to handle long-term dependencies. LSTM cells have three gates that control the flow of information into and out of the cell:\n",
    "\n",
    "> * **Input gate**: This gate controls how much of the current input is passed to the cell's state.\n",
    "> * **Forget gate**: This gate controls how much of the cell's previous state is forgotten.\n",
    "> * **Output gate**: This gate controls how much of the cell's state is output.\n",
    "\n",
    "> The LSTM cell also has a **cell state**, which is used to store information about the past inputs. The cell state is updated at each **time step**, and it can be used to make predictions about the future. LSTM cells are able to learn **long-term dependencies** because they have the ability to selectively forget information from the past. This is done by the **forget gate**, which can decide to forget some of the cell state and keep other parts of the cell state.\n",
    "\n",
    "> Here is a diagram of an LSTM cell:\n",
    "\n",
    "                    Input gate   Forget gate   Output gate   Cell state\n",
    "\n",
    "> The input gate, forget gate, and output gate are all connected to the cell state. The **cell state** is also connected to the current input. The output of the LSTM cell is the output gate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788d72fb",
   "metadata": {},
   "source": [
    "## 5. Explain Gated recurrent unit (GRU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db00a2f",
   "metadata": {},
   "source": [
    "> **Gated recurrent units (GRUs)** are a type of recurrent neural network (RNN) that are similar to LSTM cells, but they have a simpler architecture. GRU cells have two gates:\n",
    "\n",
    "> * **Update gate**: This gate controls how much of the current input is used to update the cell state.\n",
    "> * **Reset gate**: This gate controls how much of the cell's previous state is reset.\n",
    "\n",
    "> The **GRU cell** also has a cell state, which is used to store information about the past inputs. The **cell state** is updated at each time step, and it can be used to make predictions about the future. GRU cells are able to **learn long-term dependencies** because they have the ability to selectively update the cell state. This is done by the **update gate**, which can decide to update some of the cell state and keep other parts of the cell state. GRU cells are **simpler than LSTM cells**, so they are faster to train. However, they are not as good at learning long-term dependencies as LSTM cells.\n",
    "\n",
    "> Here is a diagram of a GRU cell:\n",
    "\n",
    "                    Update gate   Reset gate   Cell state\n",
    "\n",
    "> The update gate, reset gate, and cell state are all connected to the **current input**. The output of the GRU cell is the cell state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86734539",
   "metadata": {},
   "source": [
    "## 6. Explain Peephole LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb7b66d",
   "metadata": {},
   "source": [
    "> **Peephole LSTM** is a variant of the LSTM architecture that adds a \"peephole\" connection to each of the gates. This means that the gates can see the current cell state, as well as the previous hidden state. The **peephole connection** is a simple addition to the LSTM architecture, but it can have a significant impact on the performance of the model. \n",
    "\n",
    "> Here is a diagram of a peephole LSTM cell:\n",
    "\n",
    "                        Input gate   Forget gate   Output gate   Cell state\n",
    "\n",
    "> The input gate, forget gate, and output gate are all connected to the **cell state**. The cell state is also connected to the current input. The peephole connection is shown as the dashed line between the cell state and the forget gate.\n",
    "\n",
    "> The **advantage of peephole LSTMs** is that they can learn long-term dependencies more effectively than LSTMs without peephole connections. This is because the peephole connection allows the gates to see the current cell state, which can help them to decide how much of the past information to remember.\n",
    "\n",
    "> The **disadvantage of peephole LSTMs** is that they can be more difficult to train than LSTMs without peephole connections. This is because the peephole connection can introduce additional parameters into the model, which can make the model more prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480779a6",
   "metadata": {},
   "source": [
    "## 7. Bidirectional RNNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06b0633",
   "metadata": {},
   "source": [
    "> **Bidirectional recurrent neural networks (BRNNs)** are a type of recurrent neural network (RNN) that can process input sequences in both the **forward and backward directions**. This allows the BRNN to capture information from both the past and future contexts in its predictions. BRNNs are **composed of two RNNs**\n",
    "\n",
    "> Here is a diagram of a bidirectional RNN:\n",
    "\n",
    "                                    Forward RNN   Backward RNN\n",
    "\n",
    "> The forward RNN processes the input sequence in the forward direction. The backward RNN processes the input sequence in the backward direction. The outputs of the two RNNs are then combined to produce the final output.\n",
    "\n",
    "> The **advantage of BRNNs** is that they can capture information from both the past and future contexts in their predictions. This can be helpful for tasks where it is important to consider both the past and future, such as machine translation and speech recognition.\n",
    "\n",
    "> The **disadvantage of BRNNs** is that they can be more computationally expensive than RNNs that only process the input sequence in one direction. This is because the BRNN has to process the input sequence twice, once in the forward direction and once in the backward direction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c288ef",
   "metadata": {},
   "source": [
    "## 8. Explain the gates of LSTM with equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7967efa4",
   "metadata": {},
   "source": [
    "> The gates of LSTM are:\n",
    "\n",
    "> * **Input gate**: This gate controls how much of the current input is passed to the cell's state.\n",
    "> * **Forget gate**: This gate controls how much of the cell's previous state is forgotten.\n",
    "> * **Output gate**: This gate controls how much of the cell's state is output.\n",
    "\n",
    "> The equations for the gates of LSTM are as follows:\n",
    " <b><i> Input gate </i></b>:\n",
    "\n",
    "```i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i)```\n",
    "\n",
    "                            where:\n",
    "\n",
    "                                it​ is the input gate at time t\n",
    "                                xt​ is the current input at time t\n",
    "                                ht−1​ is the previous hidden state at time t−1\n",
    "                                Wi​ is the input weight matrix\n",
    "                                Ui​ is the input gate recurrent weight matrix\n",
    "                                bi​ is the input gate bias\n",
    " <b><i> Forget gate </i></b>:\n",
    "\n",
    "```f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f)```\n",
    "\n",
    "                            where:\n",
    "\n",
    "                                ft​ is the forget gate at time t\n",
    "                                Wf​ is the forget weight matrix\n",
    "                                Uf​ is the forget gate recurrent weight matrix\n",
    "                                bf​ is the forget gate bias\n",
    " <b><i> Output gate </i></b>:\n",
    "\n",
    "```o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o)```\n",
    "\n",
    "                            where:\n",
    "\n",
    "                                ot​ is the output gate at time t\n",
    "                                Wo​ is the output weight matrix\n",
    "                                Uo​ is the output gate recurrent weight matrix\n",
    "                                bo​ is the output gate bias\n",
    "\n",
    "> The equations for the gates of LSTM are based on the sigmoid function σ. The sigmoid function is a non-linear function that outputs a value between 0 and 1. The sigmoid function is used to control how much information is passed through the gates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4950d6f4",
   "metadata": {},
   "source": [
    "## 9. Explain BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df728f8",
   "metadata": {},
   "source": [
    "> **Bidirectional long short-term memory (BiLSTM)** is a type of recurrent neural network (RNN) that can process input sequences in both the forward and backward directions. This allows the BiLSTM to capture information from both the past and future contexts in its predictions. BiLSTMs are **composed of two LSTMs**, one that processes the input sequence in the forward direction and one that processes the input sequence in the backward direction. The outputs of the two LSTMs are then combined to produce the final output.\n",
    "\n",
    "> Here is a diagram of a BiLSTM:\n",
    "\n",
    "                                Forward LSTM   Backward LSTM\n",
    "\n",
    "> The **advantage of BiLSTMs** is that they can capture information from both the past and future contexts in their predictions. This can be helpful for tasks where it is important to consider both the past and future, such as machine translation and speech recognition.\n",
    "\n",
    "> The **disadvantage of BiLSTMs** is that they can be more computationally expensive than LSTMs that only process the input sequence in one direction. This is because the BiLSTM has to process the input sequence twice, once in the forward direction and once in the backward direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f514fdc",
   "metadata": {},
   "source": [
    "## 10. Explain BiGRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19260c85",
   "metadata": {},
   "source": [
    "> **Bidirectional gated recurrent units (BiGRUs)** are a type of recurrent neural network (RNN) that can process input sequences in both the forward and backward directions. This allows the BiGRU to capture information from both the past and future contexts in its predictions. BiGRUs are **composed of two GRUs**, one that processes the input sequence in the forward direction and one that processes the input sequence in the backward direction. The outputs of the two GRUs are then combined to produce the final output.\n",
    "\n",
    "> Here is a diagram of a BiGRU:\n",
    "\n",
    "                                    Forward GRU   Backward GRU\n",
    "\n",
    "> The **advantage of BiGRUs** is that they can capture information from both the past and future contexts in their predictions. This can be helpful for tasks where it is important to consider both the past and future, such as machine translation and speech recognition.\n",
    "\n",
    "> The **disadvantage of BiGRUs** is that they can be more computationally expensive than GRUs that only process the input sequence in one direction. This is because the BiGRU has to process the input sequence twice, once in the forward direction and once in the backward direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0eb52e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded4f6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
