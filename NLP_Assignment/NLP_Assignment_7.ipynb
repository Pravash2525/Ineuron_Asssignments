{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "969f761b",
   "metadata": {},
   "source": [
    "<h1> <u> <font color= green > NLP_Assignment_7 </font> </u> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b21d66",
   "metadata": {},
   "source": [
    "## 1. Explain the architecture of BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fca3982",
   "metadata": {},
   "source": [
    "> **BERT, or Bidirectional Encoder Representations from Transformers**, is a neural network architecture that was introduced by Google AI in 2018. BERT is a powerful tool for natural language processing tasks, such as text classification, question answering, and natural language inference. The **Transformer model** is a self-attention model, which means that it learns to attend to different parts of the input sequence.\n",
    "\n",
    "> BERT consists of two Transformer encoders, which are stacked on top of each other. \n",
    "> * The **first encoder** takes the input sequence as input and produces a hidden representation of the input sequence. \n",
    "> * The **second encoder** takes the hidden representation of the input sequence as input and produces a final hidden representation of the input sequence.\n",
    "\n",
    "> The **final hidden representation** of the input sequence is used for downstream tasks, such as text classification, question answering, and natural language inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c5d7e",
   "metadata": {},
   "source": [
    "## 2. Explain Masked Language Modeling (MLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9609935e",
   "metadata": {},
   "source": [
    " > **Masked language modeling (MLM)** is a technique used to pre-train language models. **MLM works** by randomly masking out words in a text sequence, and then asking the model to predict the masked words. This helps the model to learn the context of words, and to understand the relationships between words.\n",
    "\n",
    "> There are a few different ways to mask words in a text sequence. \n",
    "> * One common approach is to **randomly mask** out 15% of the words in the sequence. \n",
    "> * Another approach is to **mask out words** that are likely to be important, such as pronouns, verbs, and nouns.\n",
    "\n",
    "> The **MLM objective** is to predict the masked words in the text sequence. This is done by using a neural network to calculate the probability of each possible word at each masked position. The model is then trained to maximize the probability of predicting the correct words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc48a5b2",
   "metadata": {},
   "source": [
    "## 3. Explain Next Sentence Prediction (NSP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef03a7d",
   "metadata": {},
   "source": [
    "> **Next sentence prediction (NSP)** is a technique used to pre-train language models. **NSP works** by randomly pairing two sentences together, and then asking the model to predict whether the second sentence follows the first sentence. This helps the model to learn the relationship between sentences, and to understand the flow of text.\n",
    "\n",
    "> There are a few different ways to pair sentences together. \n",
    "> * One common approach is to **randomly pair sentences** that are next to each other in a corpus. \n",
    "> * Another approach is to **pair sentences that are likely** to be next to each other, such as sentences that are about the same topic.\n",
    "\n",
    "> The **NSP objective** is to predict whether the second sentence follows the first sentence. This is done by using a neural network to calculate the probability of each possible outcome. The model is then trained to maximize the probability of predicting the correct outcome.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2c95a1",
   "metadata": {},
   "source": [
    "## 4. What is Matthews evaluation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d711a62",
   "metadata": {},
   "source": [
    "> **Matthews evaluation, or Matthews correlation coefficient (MCC)**, is a metric used to evaluate the performance of binary classifiers. MCC is a more robust metric than accuracy, as it takes into account the true positive rate (TPR), the false positive rate (FPR), and the true negative rate (TNR).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae9cdb8",
   "metadata": {},
   "source": [
    "## 5. What is Matthews Correlation Coefficient (MCC)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff30e8ac",
   "metadata": {},
   "source": [
    "> **Matthews Correlation Coefficient (MCC)** is a metric used to evaluate the performance of binary classifiers. MCC is a more robust metric than accuracy, as it takes into account the true positive rate (TPR), the false positive rate (FPR), and the true negative rate (TNR).\n",
    "\n",
    "> MCC is calculated as follows: <br>\n",
    "```MCC = (TP * TN) - (FP * FN) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))```\n",
    "\n",
    "                            Where:\n",
    "\n",
    "                                TP: True positive\n",
    "                                TN: True negative\n",
    "                                FP: False positive\n",
    "                                FN: False negative\n",
    "\n",
    "> MCC has a **range of [-1, 1]**. A value of 1 indicates perfect prediction, a value of 0 indicates random prediction, and a value of -1 indicates inverse prediction.\n",
    "\n",
    "> MCC is a **more robust metric than accuracy** because it takes into account the true positive rate, the false positive rate, and the true negative rate. Accuracy is only concerned with the true positive rate and the false positive rate. This means that accuracy can be misleading if the true negative rate is low.\n",
    "\n",
    "> For example, consider a classifier that predicts that all instances are positive. This classifier will have a high accuracy, but it will also have a high false positive rate. MCC will be lower than accuracy for this classifier, as it will take into account the true negative rate.\n",
    "> **MCC is a good metric** to use when the true negative rate is important. It is also a good metric to use when the classifier is imbalanced, as accuracy can be misleading in imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a56efb",
   "metadata": {},
   "source": [
    "## 6. Explain Semantic Role Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f39aaa7",
   "metadata": {},
   "source": [
    "> **Semantic role labeling (SRL)** is a task in natural language processing that assigns semantic roles to words in a sentence. A **semantic role** is a relationship between a word and another word or phrase in a sentence. For example, in the sentence \"The cat chased the mouse,\" the word \"chased\" has the semantic role of agent, which means that it is the entity that is doing the action.\n",
    "\n",
    "> **SRL is a challenging task** because it requires understanding the meaning of words and their relationships to each other. There are a number of different approaches to SRL, but most of them involve training a statistical model on a corpus of text that has been manually annotated with semantic roles.\n",
    "\n",
    "> Applications of SRL are: \n",
    "> * **Question answering**: SRL can be used to answer questions about the meaning of sentences. For example, the question \"Who chased the mouse?\" could be answered by finding the word with the semantic role of agent in the sentence \"The cat chased the mouse.\"\n",
    "> * **Machine translation**: SRL can be used to improve the accuracy of machine translation. For example, if a machine translation system translates the sentence \"The cat chased the mouse\" as \"The mouse chased the cat,\" SRL can be used to identify the incorrect semantic roles and correct the translation.\n",
    "> * **Text summarization**: SRL can be used to improve the accuracy of text summarization. For example, if a text summarization system summarizes the sentence \"The cat chased the mouse\" as \"The cat and the mouse played together,\" SRL can be used to identify the incorrect semantic roles and correct the summary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d595cb34",
   "metadata": {},
   "source": [
    "## 7. Why Fine-tuning a BERT model takes less time than pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cff7462",
   "metadata": {},
   "source": [
    "> **Fine-tuning a BERT model** takes less time than pretraining (i.e much faster process) because it only needs to be trained on a smaller dataset that is specific to the task that the model is being used for. This can take hours or even minutes to complete.\n",
    "\n",
    "> **BERT is a large language model** that is pre-trained on a massive dataset of text and code. This **pre-training process** is very computationally expensive, and it can take weeks or even months to complete.\n",
    "\n",
    "<b><i> Here is a table that summarizes the difference between pre-training and fine-tuning BERT models: </i></b>\n",
    "\n",
    "> <table style=\"border:1px solid black;\">\n",
    "  <tr style=\"border:1px solid black\">\n",
    "    <th style=\"border:1px solid black\">Task</th>\n",
    "    <th style=\"border:1px solid black\">Data</th>\n",
    "    <th style=\"border:1px solid black\">Time</th>\n",
    "  </tr>\n",
    "  <tr style=\"border:1px solid black\">\n",
    "    <td style=\"border:1px solid black\">Pre-training\t</td>\n",
    "    <td style=\"border:1px solid black\">Massive dataset of text and code</td>\n",
    "    <td style=\"border:1px solid black\">Weeks or months</td>\n",
    "  <tr style=\"border:1px solid black\">\n",
    "    <td style=\"border:1px solid black\">Fine-tuning</td>\n",
    "    <td style=\"border:1px solid black\">Small dataset that is specific to the task</td>\n",
    "    <td style=\"border:1px solid black\">Minutes or Hours</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ebfcc5",
   "metadata": {},
   "source": [
    "## 8. Recognizing Textual Entailment (RTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4aaea7",
   "metadata": {},
   "source": [
    "> **Recognizing Textual Entailment (RTE)** is a task in natural language processing that involves determining whether one text fragment entails another. **An entailment** is a relationship between two text fragments where the meaning of the second fragment is implied by the meaning of the first fragment. For example, the sentence \"The cat is on the mat\" entails the sentence \"The mat has a cat on it.\"\n",
    "\n",
    "> **RTE is a challenging** task because it requires understanding the meaning of text fragments and their relationships to each other. There are a number of different approaches to RTE, but most of them involve **training a statistical model** on a corpus of text that has been manually annotated with entailment relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2532d792",
   "metadata": {},
   "source": [
    "## 9. Explain the decoder stack of GPT models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da724718",
   "metadata": {},
   "source": [
    "> The **decoder stack of GPT models** is a stack of self-attention layers that are used to generate text. The **decoder stack** is made up of a series of self-attention layers, which are responsible for attending to different parts of the input sequence and generating the next word in the output sequence.\n",
    "\n",
    "> The **decoder stack** is a critical component of GPT models, as it allows them to generate text that is both coherent and grammatically correct. The **self-attention layers** in the decoder stack are able to attend to different parts of the input sequence, which allows the model to understand the context of the text and generate the next word that is most likely to be correct.\n",
    "\n",
    "> The decoder stack is also able to **learn long-range dependencies** in the input sequence. This means that the model can understand how the words in the input sequence are related to each other, even if they are far apart. This is important for generating text that is both coherent and grammatically correct.\n",
    "\n",
    "<b><i> advantages of the decoder stack: </i></b>\n",
    "> * It allows GPT models to generate text that is both coherent and grammatically correct.\n",
    "> * It allows GPT models to learn long-range dependencies in the input sequence.\n",
    "> * It is a relatively simple architecture that is easy to understand and implement.\n",
    "\n",
    "<b><i> disadvantages to the decoder stack: </i></b>\n",
    "> * It can be computationally expensive to train.\n",
    "> * It can be difficult to tune the hyperparameters of the decoder stack.\n",
    "> * It can be difficult to prevent the model from generating repetitive or nonsensical text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee7b7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8be387b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5382b3d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
