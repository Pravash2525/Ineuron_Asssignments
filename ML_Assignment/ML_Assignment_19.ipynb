{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "130dc62d",
   "metadata": {},
   "source": [
    "<h1> <u> <font color= green > ML_Assignment_19 </font> </u> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d40366",
   "metadata": {},
   "source": [
    "## 1. A set of one-dimensional data points is given to you: 5, 10, 15, 20, 25, 30, 35. Assume that k = 2 and that the first set of random centroid is 15, 32, and that the second set is 12, 30.\n",
    "\n",
    "a) Using the k-means method, create two clusters for each set of centroid described above. <br>\n",
    "b) For each set of centroid values, calculate the SSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064fb733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def k_means(data, k, centroids):\n",
    "    clusters = np.zeros(len(data), dtype=int)\n",
    "    for i in range(len(data)):\n",
    "        min_distance = np.inf\n",
    "        min_cluster = -1\n",
    "        for j in range(k):\n",
    "            distance = np.abs(data[i] - centroids[j])\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                min_cluster = j\n",
    "        clusters[i] = min_cluster\n",
    "    return clusters\n",
    "\n",
    "def calculate_sse(data, clusters, centroids):\n",
    "    sse = 0\n",
    "    for i in range(len(data)):\n",
    "        cluster = clusters[i]\n",
    "        centroid = centroids[cluster]\n",
    "        distance = np.abs(data[i] - centroid)\n",
    "        sse += distance**2\n",
    "    return sse\n",
    "\n",
    "data = np.array([5, 10, 15, 20, 25, 30, 35])\n",
    "centroids_1 = np.array([15, 32])\n",
    "centroids_2 = np.array([12, 30])\n",
    "clusters_1 = k_means(data, 2, centroids_1)\n",
    "clusters_2 = k_means(data, 2, centroids_2)\n",
    "\n",
    "sse_1 = calculate_sse(data, clusters_1, centroids_1)\n",
    "sse_2 = calculate_sse(data, clusters_2, centroids_2)\n",
    "\n",
    "print(\"SSE for centroids 1:\", sse_1)\n",
    "print(\"SSE for centroids 2:\", sse_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48dad2c",
   "metadata": {},
   "source": [
    "## 2. Describe how the Market Basket Research makes use of association analysis concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d9aad",
   "metadata": {},
   "source": [
    "> **Market Basket Analysis (MBA)** is a data mining technique that analyzes customer transactional data to identify patterns in customer behavior. This analysis can be used to identify association rules, which are statements that describe how two or more items tend to be purchased together.\n",
    "\n",
    "> One of the most common **association rule mining algorithms** is Apriori. The **Apriori algorithm** works by generating a set of candidate frequent itemsets and then checking each candidate to see if it is actually frequent. The **Apriori algorithm** uses a pruning technique to reduce the number of candidate frequent itemsets that need to be checked.\n",
    "\n",
    "> The **pruning technique** is based on the Apriori property, which states that if an itemset is not frequent, then no subset of that itemset can be frequent. This means that the Apriori algorithm can safely ignore any candidate frequent itemset that does not contain any frequent itemsets as a subset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256d869e",
   "metadata": {},
   "source": [
    "## 3. Give an example of the Apriori algorithm for learning association rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8457ed29",
   "metadata": {},
   "source": [
    "> The **Apriori algorithm** is a popular algorithm used for learning association rules in data mining and market basket analysis. It discovers frequent itemsets and generates association rules based on their support and confidence. Here's an example to illustrate how the Apriori algorithm works:\n",
    "\n",
    "> Consider a transaction database containing records of customer purchases at a grocery store. Each record represents a transaction, and items purchased are listed within each transaction. The dataset looks like this:\n",
    "\n",
    "                Transaction 1: {Milk, Bread, Eggs}\n",
    "                Transaction 2: {Bread, Butter}\n",
    "                Transaction 3: {Milk, Bread, Butter}\n",
    "                Transaction 4: {Milk, Bread}\n",
    "                Transaction 5: {Bread, Eggs}\n",
    "\n",
    "> **Step 1: Generating frequent itemsets**: <br>\n",
    "The Apriori algorithm starts by finding frequent itemsets, which are sets of items that appear together in a significant number of transactions.\n",
    "\n",
    "> * 1-itemset generation: Count the occurrence of each item individually in the transactions. Items with sufficient support are considered frequent.\n",
    "\n",
    "                Frequent 1-itemsets: {Milk, Bread, Eggs, Butter}\n",
    "\n",
    "> * 2-itemset generation: Create candidate 2-itemsets by combining frequent 1-itemsets. Count the occurrence of each candidate in the transactions and keep only those with sufficient support.\n",
    "\n",
    "                Candidate 2-itemsets: {Milk, Bread},{Milk, Eggs},{Milk, Butter},{Bread, Eggs},{Bread, Butter},{Eggs, Butter}\n",
    "                Frequent 2-itemsets: {Milk, Bread}, {Bread, Eggs}, {Bread, Butter}\n",
    "\n",
    "> * 3-itemset generation: Create candidate 3-itemsets by combining frequent 2-itemsets. Count the occurrence of each candidate in the transactions and keep only those with sufficient support.\n",
    "\n",
    "                Candidate 3-itemsets: {Milk, Bread, Eggs},{Milk, Bread, Butter},{Bread, Eggs, Butter}\n",
    "                Frequent 3-itemsets: {Milk, Bread, Butter}\n",
    "\n",
    "> **Step 2: Generating association rules**:\n",
    "Association rules are generated based on the frequent itemsets. Each rule consists of an antecedent (the items on the left-hand side) and a consequent (the items on the right-hand side).\n",
    "\n",
    "                Association rule example:\n",
    "                {Milk, Bread} => {Butter}\n",
    "\n",
    "---\n",
    "> To evaluate the generated rules, two metrics are commonly used:\n",
    "> * **Support**: Measures the frequency of occurrence of an itemset in the transactions. It is calculated as the number of transactions containing the itemset divided by the total number of transactions.\n",
    "> * **Confidence**: Measures the conditional probability of finding the consequent in a transaction given that the antecedent is present. It is calculated as the number of transactions containing both the antecedent and consequent divided by the number of transactions containing the antecedent.\n",
    "\n",
    "                For example, if the support threshold is set to 40% and the confidence threshold is set to 60%, we can calculate the support and confidence for the generated rule:\n",
    "\n",
    "                Support: Number of transactions containing {Milk, Bread, Butter} / Total number of transactions\n",
    "                Confidence: Number of transactions containing {Milk, Bread, Butter} / Number of transactions containing {Milk, Bread}\n",
    "\n",
    "> By applying the support and confidence thresholds, we can filter out rules that meet the desired level of significance and generate meaningful association rules from the frequent itemsets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f3f4be",
   "metadata": {},
   "source": [
    "## 4. In hierarchical clustering, how is the distance between clusters measured? Explain how this metric is used to decide when to end the iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a776a2",
   "metadata": {},
   "source": [
    "> In hierarchical clustering, the **distance between clusters** is measured using a linkage criterion. There are several different linkage criteria that can be used, but the most common ones are:\n",
    "> 1. **Single linkage**: The distance between two clusters is the minimum distance between any two data points in the clusters.\n",
    "> 2. **Complete linkage**: The distance between two clusters is the maximum distance between any two data points in the clusters.\n",
    "> 3. **Average linkage**: The distance between two clusters is the average distance between all pairs of data points in the clusters.\n",
    "\n",
    "> The **linkage criterion** is used to decide when to end the iteration of the hierarchical clustering algorithm. The algorithm will continue to iterate until the distance between any two clusters is greater than a threshold value. This threshold value is typically set by the user.\n",
    "---\n",
    "> Here is an example of how the distance between clusters is measured using the single linkage criterion:\n",
    "> Consider a dataset with four data points: ```(1, 2), (3, 4), (5, 6), and (7, 8)```.\n",
    "\n",
    "> 1. The **single linkage distance** between the clusters {(1, 2)} and {(3, 4)} is the minimum distance between any two data points in the clusters. The minimum distance between the two clusters is 1, so the distance between the clusters is 1.\n",
    "> The single linkage distance between the clusters {(1, 2), (3, 4)} and {(5, 6), (7, 8)} is the minimum distance between any two data points in the clusters. The minimum distance between the two clusters is 3, so the distance between the clusters is 3.\n",
    "\n",
    "> 2. The **complete linkage criterion** is the opposite of the single linkage criterion. The complete linkage distance between two clusters is the maximum distance between any two data points in the clusters. This means that the complete linkage criterion will tend to merge clusters that are very far apart.\n",
    "\n",
    "> 3. The **average linkage criterion** is a compromise between the single linkage criterion and the complete linkage criterion. The average linkage distance between two clusters is the average distance between all pairs of data points in the clusters. This means that the average linkage criterion will tend to merge clusters that are neither very close together nor very far apart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f82e4b",
   "metadata": {},
   "source": [
    "## 5. In the k-means algorithm, how do you recompute the cluster centroids?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a466397f",
   "metadata": {},
   "source": [
    ">  In the k-means algorithm, the **cluster centroids** are recomputed by finding the mean of all the data points in the cluster. The mean is the sum of all the data points in the cluster divided by the number of data points in the cluster.\n",
    "\n",
    "> For example, consider a dataset with four data points: (1, 2), (3, 4), (5, 6), and (7, 8). If these data points are assigned to cluster 1, then the cluster centroid is recomputed as follows: <br>\n",
    "```mean = (1 + 3 + 5 + 7) / 4 = 4```\n",
    "\n",
    "> The **mean of the cluster** is 4, so the cluster centroid is 4.\n",
    "\n",
    "> The k-means algorithm will continue to **recompute the cluster centroids** until the centroids do not change significantly. This means that the algorithm has converged to a set of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8759a1a",
   "metadata": {},
   "source": [
    "## 6. At the start of the clustering exercise, discuss one method for determining the required number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2400b5f3",
   "metadata": {},
   "source": [
    "> There are a number of different methods for determining the required number of clusters in a clustering exercise. Here are a few of the most common methods:\n",
    "> * **The elbow method**: This method plots the inertia of the clusters against the number of clusters. The inertia is a measure of how well the data points are clustered. The elbow method works by finding the point on the graph where the inertia starts to decrease significantly. This point is typically considered to be the optimal number of clusters.\n",
    "> * **The silhouette coefficient**: This method calculates a coefficient for each data point that measures how well the data point fits into its cluster. The silhouette coefficient is typically between -1 and 1. The silhouette coefficient can be used to identify the optimal number of clusters by finding the number of clusters that maximizes the average silhouette coefficient.\n",
    "> * **The gap statistic**: This method calculates a statistic that measures the separation between the clusters. The gap statistic is typically compared to the gap statistic for a random dataset. The optimal number of clusters is the number of clusters that maximizes the gap statistic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58961ed1",
   "metadata": {},
   "source": [
    "## 7. Discuss the k-means algorithm's advantages and disadvantages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7130234",
   "metadata": {},
   "source": [
    "<b><i> Advantages: </i></b>\n",
    "> * Simple and efficient to implement.\n",
    "> * Can be used on large datasets.\n",
    "> * Works well with numerical data.\n",
    "> * Can be used to cluster data with different shapes.\n",
    "\n",
    "<b><i> Disadvantages: </i></b>\n",
    "> * Sensitive to the initialization of the centroids.\n",
    "> * Can be unstable for small datasets.\n",
    "> * Can produce clusters with irregular shapes.\n",
    "> * Not suitable for clustering data with mixed data types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe55eab1",
   "metadata": {},
   "source": [
    "## 8. Draw a diagram to demonstrate the principle of clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c69e84f",
   "metadata": {},
   "source": [
    "<img src = \"https://www.researchgate.net/publication/324152730/figure/fig1/AS:963443037335571@1606714128395/Illustration-of-the-clustering-problem-using-a-pigeon-hole-principle-with-dataset-S2-The.gif\" >\n",
    "\n",
    "> The diagram shows a dataset of 10 data points. The **data points** are represented as black dot circles, and the **clusters** are represented as White circle. The algorithm starts by randomly assigning the data points to clusters. In this case, the **data points** are randomly assigned to two clusters. The algorithm then iterates until the clusters do not change significantly. In each iteration, the algorithm does the following:\n",
    "> * Assigns each data point to the cluster with the closest centroid.\n",
    "> * Recomputes the cluster centroids as the mean of the data points in the cluster.\n",
    "\n",
    "> In this example, the algorithm converges after 3 iterations. The final clusters are shown in the diagram.\n",
    "\n",
    "> The **principle of clustering** is to group data points that are similar together. The algorithm does this by iteratively assigning data points to clusters and recomputing the cluster centroids. The algorithm converges when the clusters do not change significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae311072",
   "metadata": {},
   "source": [
    "## 9. In a software project, the team is attempting to determine if software flaws discovered during testing are identical. Based on the text analytics of the defect details, they decided to build 5 clusters of related defects. Any new defect formed after the 5 clusters of defects have been identified must be listed as one of the forms identified by clustering. A simple diagram can be used to explain this process. Assume you have 20 defect data points that are clustered into 5 clusters and you used the k-means algorithm.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af96bcb1",
   "metadata": {},
   "source": [
    "> <img src = \"https://www.mdpi.com/applsci/applsci-10-01745/article_deploy/html/images/applsci-10-01745-g001-550.jpg\">\n",
    "\n",
    "> The diagram shows 20 defect data points that are clustered into 5 clusters. The clusters are labeled C1, C2, C3, C4, and C5. The data points in each cluster are similar to each other in terms of their text analytics.\n",
    "\n",
    "> The **k-means algorithm** was used to cluster the data points. The k-means algorithm **works by iteratively assigning data points** to clusters and recomputing the cluster centroids. The algorithm converges when the clusters do not change significantly.\n",
    "\n",
    "> In this example, the k-means algorithm converged after 5 iterations. The final clusters are shown in the diagram. <br>\n",
    "<img src = \"https://www.mdpi.com/applsci/applsci-10-01745/article_deploy/html/images/applsci-10-01745-g002-550.jpg\" >\n",
    "\n",
    "> Any new defect formed after the 5 clusters of defects have been identified must be listed as one of the forms identified by clustering. This is because the k-means algorithm has identified the 5 most common forms of defects. Any new defect that is not similar to any of the 5 clusters will be considered to be a new form of defect.\n",
    "\n",
    "> The **clustering of software defects** can be a useful tool for software testing. By clustering defects, the team can identify the most common forms of defects. This information can then be used to improve the quality of the software by fixing the most common defects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5ca2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c432cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
