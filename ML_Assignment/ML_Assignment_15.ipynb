{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8279765f",
   "metadata": {},
   "source": [
    "<h1> <u> <font color= green > ML_Assignment_15 </font> </u> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e4a288",
   "metadata": {},
   "source": [
    "## 1. Recognize the differences between supervised, semi-supervised, and unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9568f78c",
   "metadata": {},
   "source": [
    "> The differences between supervised, semi-supervised, and unsupervised learning:\n",
    "> * **Supervised learning** is a type of machine learning where the model is trained on labeled data. This means that the data has both input and output values. The model learns to map the input values to the output values. Supervised learning is often used for classification and regression tasks.\n",
    "> * **Semi-supervised learning** is a type of machine learning where the model is trained on both labeled and unlabeled data. This means that the model has some information about the output values, but not all of the data is labeled. Semi-supervised learning can be used to improve the performance of supervised learning models.\n",
    "> * **Unsupervised learning** is a type of machine learning where the model is trained on unlabeled data. This means that the data does not have any output values. The model learns to find patterns in the data without any guidance. Unsupervised learning is often used for clustering and dimensionality reduction tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d2b00f",
   "metadata": {},
   "source": [
    "## 2. Describe in detail any five examples of classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4092df7",
   "metadata": {},
   "source": [
    "> five examples of classification problems:\n",
    "> * **Spam filtering**: Spam filtering is a classification problem where the goal is to classify emails as spam or not spam. This can be done by using a machine learning model to learn the features that distinguish spam emails from non-spam emails.\n",
    "> * **Fraud detection**: Fraud detection is a classification problem where the goal is to identify fraudulent transactions. This can be done by using a machine learning model to learn the features that distinguish fraudulent transactions from legitimate transactions.\n",
    "> * **Image classification**: Image classification is a classification problem where the goal is to classify images into different categories. This can be done by using a machine learning model to learn the features that distinguish different types of images.\n",
    "> * **Text classification**: Text classification is a classification problem where the goal is to classify text into different categories. This can be done by using a machine learning model to learn the features that distinguish different types of text.\n",
    "> * **Medical diagnosis**: Medical diagnosis is a classification problem where the goal is to diagnose diseases. This can be done by using a machine learning model to learn the features that distinguish different diseases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00de843c",
   "metadata": {},
   "source": [
    "## 3. Describe each phase of the classification process in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb610c",
   "metadata": {},
   "source": [
    "> the phases of the classification process are:\n",
    "> * **Data preparation**: The first phase of the classification process is to prepare the data. This involves cleaning the data, removing outliers, and transforming the data into a format that can be used by the machine learning model.\n",
    "> * **Feature selection**: The next phase is to select the features that will be used to train the model. This is an important step, as the features that are selected will have a significant impact on the performance of the model.\n",
    "> * **Model training**: The third phase is to train the model. This involves using the selected features to train the model on a labeled dataset. The model will learn to map the features to the output values.\n",
    "> * **Model evaluation**: The fourth phase is to evaluate the model. This involves using the model to make predictions on a test dataset. The predictions are then compared to the ground truth labels to assess the performance of the model.\n",
    "> * **Model deployment**: The fifth and final phase is to deploy the model. This involves making the model available to users so that they can use it to make predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f203f0e",
   "metadata": {},
   "source": [
    "## 4. Go through the SVM model in depth using various scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d763e1",
   "metadata": {},
   "source": [
    "> **Support Vector Machines (SVMs)** are a type of supervised machine learning algorithm that can be used for classification and regression tasks. SVMs work by finding the hyperplane that best separates the data into two or more classes. The **hyperplane** is a line or curve that divides the data so that the classes are on opposite sides of the line.\n",
    "\n",
    "> examples:\n",
    "> * **Spam filtering**: SVMs can be used to classify emails as spam or not spam. The model would learn to find the hyperplane that best separates spam emails from non-spam emails.\n",
    "> * **Fraud detection**: SVMs can be used to identify fraudulent transactions. The model would learn to find the hyperplane that best separates fraudulent transactions from legitimate transactions.\n",
    "> * **Image classification**: SVMs can be used to classify images into different categories. The model would learn to find the hyperplane that best separates images of different categories.\n",
    "> * **Text classification**: SVMs can be used to classify text into different categories. The model would learn to find the hyperplane that best separates text of different categories.\n",
    "\n",
    "> advantages:\n",
    "> * **Robust to noise**: SVMs are relatively robust to noise in the data. This means that they can still perform well even if the data contains some noise.\n",
    "> * **Scalable**: SVMs are scalable to large datasets. This means that they can be used to train models on large datasets.\n",
    "> * **Interpretable**: SVMs are relatively interpretable. This means that we can understand how the model makes its predictions.\n",
    "\n",
    "> some additional details about SVMs:\n",
    "> * **The hyperplane**: The hyperplane is the key to SVMs. The hyperplane is a line or curve that divides the data into two or more classes. The goal of SVMs is to find the hyperplane that maximizes the margin between the two classes. The margin is the distance between the hyperplane and the closest points of each class.\n",
    "> * **Kernel functions**: SVMs can use kernel functions to map the data into a higher dimensional space. This can be helpful if the data is not linearly separable in the original space. There are a number of different kernel functions that can be used with SVMs, such as the linear kernel, the polynomial kernel, and the Gaussian kernel.\n",
    "> * **Regularization**: SVMs can be regularized to prevent overfitting. Regularization adds a penalty to the model's complexity, which helps to prevent the model from fitting the noise in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41590398",
   "metadata": {},
   "source": [
    "## 5. What are some of the benefits and drawbacks of SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b05158",
   "metadata": {},
   "source": [
    "<b><i> Benefits: </i></b> \n",
    "> * **Robust to noise**: SVMs are relatively robust to noise in the data. This means that they can still perform well even if the data contains some noise.\n",
    "> * **Scalable**: SVMs are scalable to large datasets. This means that they can be used to train models on large datasets.\n",
    "> * **Interpretable**: SVMs are relatively interpretable. This means that you can understand how the model makes its predictions.\n",
    "> * **Good for small datasets**: SVMs can be effective even with small datasets. This is because SVMs focus on the most important features of the data, which can help to improve the performance of the model even with limited data.\n",
    "\n",
    "<b><i> Drawbacks: </i></b> \n",
    "> * **Can be computationally expensive**: SVMs can be computationally expensive to train, especially for large datasets.\n",
    "> * **Can be sensitive to the choice of kernel**: The performance of SVMs can be sensitive to the choice of kernel function. This means that it is important to choose the kernel function that is most appropriate for the data.\n",
    "> * **Can be overfitting**: SVMs can be prone to overfitting, especially if the data is not well-labeled. This means that the model can learn the noise in the data instead of the true underlying patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe3f5be",
   "metadata": {},
   "source": [
    "## 6. Go over the kNN model in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72207beb",
   "metadata": {},
   "source": [
    "> **k-Nearest Neighbors (kNN)** is a type of supervised machine learning algorithm that can be used for classification and regression tasks. kNN works by finding the k most similar points to a new point and then predicting the class of the new point based on the classes of the k nearest neighbors.\n",
    "\n",
    "> **kNN** is a simple algorithm that is easy to understand and implement. However, it can be computationally expensive, especially for large datasets.\n",
    "\n",
    "> some examples:\n",
    "> * **Spam filtering**: kNN can be used to classify emails as spam or not spam. The model would find the k most similar emails to a new email and then predict the class of the new email based on the classes of the k nearest neighbors.\n",
    "> * **Fraud detection**: kNN can be used to identify fraudulent transactions. The model would find the k most similar transactions to a new transaction and then predict the class of the new transaction based on the classes of the k nearest neighbors.\n",
    "> * **Image classification**: kNN can be used to classify images into different categories. The model would find the k most similar images to a new image and then predict the class of the new image based on the classes of the k nearest neighbors.\n",
    "> * **Text classification**: kNN can be used to classify text into different categories. The model would find the k most similar text documents to a new text document and then predict the class of the new text document based on the classes of the k nearest neighbors.\n",
    "\n",
    "> some additional details about kNN:\n",
    "> * **The k value**: The k value is the number of nearest neighbors that are used to make a prediction. The k value can be a small number, such as 3, or a large number, such as 100. The k value will affect the performance of the model.\n",
    "> * **The distance metric**: The distance metric is used to measure the similarity between two points. There are a number of different distance metrics that can be used with kNN, such as the Euclidean distance, the Manhattan distance, and the Minkowski distance. The distance metric will affect the performance of the model.\n",
    "> * **The weighting scheme**: The weighting scheme is used to weight the votes of the k nearest neighbors. There are a number of different weighting schemes that can be used with kNN, such as the uniform weighting scheme and the inverse distance weighting scheme. The weighting scheme will affect the performance of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c66926a",
   "metadata": {},
   "source": [
    "## 7. Discuss the kNN algorithm's error rate and validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ad9980",
   "metadata": {},
   "source": [
    "> * **Error rate**: The error rate of a kNN model is the percentage of predictions that are incorrect. The error rate can be calculated by dividing the number of incorrect predictions by the total number of predictions.\n",
    "> * **Validation error**: The validation error of a kNN model is the error rate of the model on a validation dataset. The validation dataset is a separate dataset that is not used to train the model. The validation error is a good measure of how well the model will generalize to new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c029ed",
   "metadata": {},
   "source": [
    "## 8. For kNN, talk about how to measure the difference between the test and training results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f6a522",
   "metadata": {},
   "source": [
    "> The difference between the test and training results can be measured by comparing the <b><i> MAE, MSE, RMSE, or R-squared scores </i></b> for the test and training datasets. A larger difference between the scores indicates that the model is not generalizing well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f0532c",
   "metadata": {},
   "source": [
    "## 9. Create the kNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edae388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_neighbors(data, new_point, k):\n",
    "    distances = []\n",
    "    for point in data:\n",
    "        distance = calculate_distance(point, new_point)\n",
    "        distances.append((distance, point[-1]))\n",
    "\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    neighbors = distances[:k]\n",
    "\n",
    "    votes = {}\n",
    "    for neighbor in neighbors:\n",
    "        class_label = neighbor[1]\n",
    "        if class_label in votes:\n",
    "            votes[class_label] += 1\n",
    "        else:\n",
    "            votes[class_label] = 1\n",
    "\n",
    "    most_common_class = max(votes, key=votes.get)\n",
    "    return most_common_class\n",
    "\n",
    "def calculate_distance(point1, point2):\n",
    "    distance = 0\n",
    "    for i in range(len(point1) - 1):\n",
    "        difference = point1[i] - point2[i]\n",
    "        distance += difference ** 2\n",
    "\n",
    "    return distance ** 0.5\n",
    "\n",
    "\n",
    "data = [[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7], [7, 8], [8, 9]]\n",
    "test_point = [3, 4]\n",
    "\n",
    "neighbors = k_nearest_neighbors(data,test_point, 3)\n",
    "print(neighbors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0e1536",
   "metadata": {},
   "source": [
    "## 10. What is a decision tree, exactly? What are the various kinds of nodes? Explain all in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2105bfca",
   "metadata": {},
   "source": [
    "> A **decision tree** is a supervised machine learning model that can be used for classification and regression tasks. Decision trees work by splitting the data into smaller and smaller subsets until each subset belongs to a single class.\n",
    "\n",
    "> A **decision tree** is made up of nodes and edges. The **nodes** represent decisions, and the **edges** represent the results of those decisions. The **root node** is the top node of the tree, and the **leaf nodes** are the bottom nodes of the tree.\n",
    "\n",
    "> There are two main types of nodes in a decision tree:\n",
    "> * **Decision nodes**: Decision nodes represent a decision that needs to be made. The decision node will have a number of branches, each representing a possible outcome of the decision.\n",
    "> * **Leaf nodes**: Leaf nodes represent a leaf of the tree. The leaf node will have a class label, which is the class that all the data in the leaf node belongs to.\n",
    "\n",
    "> The **decision tree** is built by recursively splitting the data into smaller and smaller subsets. The **splitting process** is repeated until each subset belongs to a single class. The **splitting process** is typically done using a greedy algorithm, which means that the algorithm chooses the best split at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1b249d",
   "metadata": {},
   "source": [
    "## 11. Describe the different ways to scan a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9963cda",
   "metadata": {},
   "source": [
    "> The different ways to scan a decision tree are:\n",
    "> * **Depth-first**: The depth-first scan is the most common way to scan a decision tree. It is a recursive algorithm that starts at the root node and recursively explores the tree one level at a time. The depth-first scan is typically used when the goal is to find the path to a specific leaf node.\n",
    "> * **Breadth-first**: The breadth-first scan is a less common way to scan a decision tree. It is a non-recursive algorithm that explores the tree one level at a time, starting at the root node and then moving to the next level, and so on. The breadth-first scan is typically used when the goal is to explore the entire tree.\n",
    "> * **Post-order**: The post-order scan is a less common way to scan a decision tree. It is a recursive algorithm that starts at the leaf nodes and recursively explores the tree back to the root node. The post-order scan is typically used when the goal is to collect information about the tree, such as the number of leaf nodes or the depth of the tree.\n",
    "> * **In-order**: The in-order scan is a less common way to scan a decision tree. It is a recursive algorithm that starts at the root node and recursively explores the tree, visiting the left child node, then the current node, and then the right child node. The in-order scan is typically used when the goal is to traverse the tree in a specific order, such as the order in which the data was collected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f208eb09",
   "metadata": {},
   "source": [
    "## 12. Describe in depth the decision tree algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5323f65",
   "metadata": {},
   "source": [
    "> The **decision tree algorithm** is a recursive algorithm. It starts at the **root node** of the tree and recursively splits the data into two subsets. The splitting process is repeated until each subset belongs to a single class. The **splitting process** is typically done using a greedy algorithm, which means that the algorithm chooses the best split at each step.\n",
    "\n",
    "> The **decision tree algorithm** is typically used for classification tasks. However, it can also be used for regression tasks. In classification tasks, the decision tree algorithm is used to predict the class of a new data point. In regression tasks, the decision tree algorithm is used to predict the value of a continuous variable.\n",
    "\n",
    "> Here are the steps involved in the decision tree algorithm:\n",
    "> * **Initialize the tree**: The decision tree is initialized with a single node, the root node.\n",
    "> * **Split the data: The data is split into two subsets based on a decision rule. The decision rule is typically a threshold on a feature value.\n",
    "> * **Recursively build the tree**: The algorithm recursively builds the tree by splitting the subsets into smaller and smaller subsets. The splitting process is repeated until each subset belongs to a single class.\n",
    "> * **Predict the class of a new data point**: The class of a new data point is predicted by following the decision rules in the tree until a leaf node is reached. The class of the leaf node is the predicted class of the new data point.\n",
    "\n",
    "<b><i> Benefits: </i></b>\n",
    "> * **Easy to understand and interpret**: Decision trees are relatively easy to understand and interpret. This makes them a good choice for tasks where the results need to be explained to stakeholders.\n",
    "> * **Robust to noise**: Decision trees are relatively robust to noise in the data. This means that they can still perform well even if the data contains some noise.\n",
    "> * **Scalable**: Decision trees can be scaled to large datasets. This makes them a good choice for tasks where the data is large.\n",
    "\n",
    "<b><i> Drawbacks: </i></b>\n",
    "> * **Can be overfitting**: Decision trees can be prone to overfitting, especially if the data is not well-labeled. This means that the model can learn the noise in the data instead of the true underlying patterns.\n",
    "> * **Not as accurate as other models**: Decision trees are not as accurate as some other machine learning models, such as support vector machines (SVMs).\n",
    "> * **Can be computationally expensive**: Decision trees can be computationally expensive to train, especially for large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e68cdd",
   "metadata": {},
   "source": [
    "## 13. In a decision tree, what is inductive bias? What would you do to stop overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3c1265",
   "metadata": {},
   "source": [
    "> **Inductive bias** is a concept in machine learning that refers to the assumptions that a learning algorithm makes about the data. These assumptions help the algorithm to learn more efficiently and generalize better to new data.\n",
    "\n",
    "> In decision trees, **inductive bias** is typically encoded in the splitting criteria that are used to build the tree. For example, a decision tree might be built by splitting the data on the feature that has the highest information gain. This means that the algorithm is assuming that the data is more likely to be divided into two groups based on the value of this feature.\n",
    "\n",
    "> **Overfitting** is a problem that can occur in machine learning when the model learns the noise in the training data too well. This can lead to the model performing poorly on new data.\n",
    "\n",
    "> There are a number of things that can be done to prevent overfitting in decision trees. Some of these include:\n",
    "> * **Pruning**: Pruning is a technique that removes some of the branches from a decision tree. This can help to prevent the model from learning the noise in the training data.\n",
    "> * **Early stopping**: Early stopping is a technique that stops the training of the model early, before it has had a chance to learn the noise in the training data.\n",
    "> * **Regularization**: Regularization is a technique that penalizes the model for being too complex. This can help to prevent the model from overfitting the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c931fff",
   "metadata": {},
   "source": [
    "## 14.Explain advantages and disadvantages of using a decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a96df7",
   "metadata": {},
   "source": [
    "<b><i> Advantages: </i></b>\n",
    "> * **Easy to understand and interpret**: Decision trees are relatively easy to understand and interpret. This makes them a good choice for tasks where the results need to be explained to stakeholders.\n",
    "> * **Robust to noise**: Decision trees are relatively robust to noise in the data. This means that they can still perform well even if the data contains some noise.\n",
    "> * **Scalable**: Decision trees can be scaled to large datasets. This makes them a good choice for tasks where the data is large.\n",
    "> * **Can be used for both classification and regression**: Decision trees can be used for both classification and regression tasks. This makes them a versatile tool that can be used for a variety of problems.\n",
    "\n",
    "<b><i> Disadvantages: </i></b>\n",
    "> * **Can be overfitting**: Decision trees can be prone to overfitting, especially if the data is not well-labeled. This means that the model can learn the noise in the data instead of the true underlying patterns.\n",
    "> * **Not as accurate as other models**: Decision trees are not as accurate as some other machine learning models, such as support vector machines (SVMs).\n",
    "> * **Can be computationally expensive**: Decision trees can be computationally expensive to train, especially for large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53356f6",
   "metadata": {},
   "source": [
    "## 15. Describe in depth the problems that are suitable for decision tree learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55bf036",
   "metadata": {},
   "source": [
    "> The problems that are suitable for decision tree learning include:\n",
    "> * **Classification**: Decision trees can be used to classify data into different categories. For example, a decision tree could be used to classify emails as spam or ham, or to classify patients as having a disease or not having a disease.\n",
    "> * **Regression**: Decision trees can also be used to predict the value of a continuous variable. For example, a decision tree could be used to predict the price of a house, or to predict the likelihood of a customer making a purchase.\n",
    "> * **Imbalanced data**: Decision trees can be used to deal with imbalanced data. Imbalanced data is data where there is a large difference in the number of samples in each class. For example, there may be many more samples of healthy patients than samples of patients with a disease. Decision trees can be used to deal with imbalanced data by using a technique called cost-sensitive learning.\n",
    "> * **Noisy data**: Decision trees can be used to deal with noisy data. Noisy data is data that contains errors. For example, the labels on the data may be incorrect, or the data may be missing values. Decision trees can be used to deal with noisy data by using a technique called ensemble learning.\n",
    "> * **Interpretability**: Decision trees are relatively easy to interpret. This makes them a good choice for tasks where the results need to be explained to stakeholders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59c5d8c",
   "metadata": {},
   "source": [
    "## 16. Describe in depth the random forest model. What distinguishes a random forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b18917d",
   "metadata": {},
   "source": [
    "> A **random forest** is an ensemble learning model that consists of a large number of decision trees. Each decision tree is trained on a different subset of the training data, and each tree makes a prediction for the test data. The final prediction of the random forest is made by averaging the predictions of the individual trees.\n",
    "\n",
    "> **Random forests** are a powerful machine learning algorithm that can be used for both classification and regression tasks. They are relatively easy to understand and interpret, and they can be scaled to large datasets. However, they can be prone to **overfitting**, especially if the data is not well-labeled.\n",
    "\n",
    "> * What **distinguishes a random forest** is that the decision trees are trained using a technique called **bootstrap aggregating**, also known as bagging. **Bagging** is a technique that randomly samples the training data with replacement. This means that each tree is trained on a different subset of the training data, which helps to reduce the variance of the model and to prevent overfitting.\n",
    "> * In addition to bagging, random forests also use a technique called feature randomizing. **Feature randomizing** is a technique that randomly selects a subset of features for each tree. This helps to prevent the trees from becoming too correlated and to improve the overall performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34933442",
   "metadata": {},
   "source": [
    "## 17. In a random forest, talk about OOB error and variable value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32fc0b1",
   "metadata": {},
   "source": [
    "> OOB error and variable importance are two important metrics that can be used to **evaluate the performance of the model**.\n",
    "\n",
    "> * **OOB error** stands for out-of-bag error. It is the error rate of the random forest on the out-of-bag samples. The out-of-bag samples are the samples that were not used to train any of the decision trees in the random forest. This means that the OOB error is a measure of how well the random forest generalizes to new data.\n",
    "> * **Variable importance** is a measure of how important each feature is to the random forest. It is calculated by measuring the decrease in the OOB error when the feature is excluded from the model. The higher the variable importance, the more important the feature is to the model.\n",
    "\n",
    "> Both OOB error and variable importance can be used to tune the **hyperparameters of the random forest**. For example, if the OOB error is high, we can increase the number of trees in the random forest. If a particular variable has a low variable importance, we can remove it from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad0f77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82ada34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
