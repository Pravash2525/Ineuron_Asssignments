{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f79c2d04",
   "metadata": {},
   "source": [
    "<h1> <u> <font color= green > ML_Assignment_21 </font> </u> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e458cc8e",
   "metadata": {},
   "source": [
    "## 1. What is the estimated depth of a Decision Tree trained (unrestricted) on a one million instance training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d9af3",
   "metadata": {},
   "source": [
    "> The estimated depth of a decision tree trained (unrestricted) on a one million instance training set is 20.\n",
    "\n",
    "> The **depth of a decision tree** is the number of levels in the tree. A decision tree is a hierarchical model that is used to classify or predict data. The tree is built by recursively splitting the data into smaller and smaller groups until each group is homogeneous. The depth of the tree is determined by the number of splits that are made.\n",
    "\n",
    "> In the case of a decision tree trained on a one million instance training set, the depth of the tree will be approximately 20. This is because the number of splits that can be made in a tree is proportional to the logarithm of the number of instances in the training set. Therefore, a tree with one million instances will have a depth of approximately ```log2(10^6) = 20```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c144dd",
   "metadata": {},
   "source": [
    "## 2. Is the Gini impurity of a node usually lower or higher than that of its parent? Is it always lower/greater, or is it usually lower/greater?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5118b3b3",
   "metadata": {},
   "source": [
    "> The **Gini impurity of a node** is usually lower than that of its parent. This is because the **CART algorithm**, which is used to train decision trees, tries to minimize the Gini impurity of each node. The algorithm does this by splitting the data at the point that minimizes the weighted sum of the Gini impurities of the child nodes.\n",
    "\n",
    "> However, it is possible for the Gini impurity of a node to be higher than that of its parent. This can happen if the split that is chosen does not actually reduce the impurity of the data. For example, if the data is already very pure, then splitting it may actually increase the impurity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d886e20a",
   "metadata": {},
   "source": [
    "## 3. Explain if its a good idea to reduce max depth if a Decision Tree is overfitting the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597cde86",
   "metadata": {},
   "source": [
    "> It is a good idea to **reduce the max depth of a decision tree** if it is overfitting the training set. **Overfitting** occurs when a model learns the training data too well and is unable to generalize to new data. This can happen when the model is too complex, such as when the **max depth of the tree** is too high.\n",
    "\n",
    "> In addition to reducing the max depth, there are other techniques that can be used to reduce overfitting in decision trees. These techniques include:\n",
    "> * **Pruning**: Pruning is a technique that removes parts of the decision tree that are not important. This can help to reduce the complexity of the model and make it less likely to overfit.\n",
    "> * **Random forests**: Random forests are a type of ensemble model that consists of multiple decision trees. This helps to reduce overfitting by averaging the predictions of multiple trees.\n",
    "> * **Cross-validation**: Cross-validation is a technique that is used to evaluate the performance of a model on unseen data. This can help to identify models that are overfitting the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967ae20b",
   "metadata": {},
   "source": [
    "## 4. Explain if its a good idea to try scaling the input features if a Decision Tree underfits the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c62abd1",
   "metadata": {},
   "source": [
    "> **Scaling the input features can be a good idea** if a decision tree underfits the training set. **Decision trees are sensitive to the scale** of the input features, so scaling the features can help the tree to learn more effectively.\n",
    "\n",
    "> Of course, **scaling the input features is not always necessary**. If the decision tree is already learning well, then scaling the features may not improve the performance of the model. However, if the tree is underfitting the training set, then scaling the features can be a good way to improve the model's performance.\n",
    "\n",
    "> Here are some of the benefits of scaling input features for decision trees:\n",
    "> * It can help to improve the accuracy of the model.\n",
    "> * It can help to reduce overfitting.\n",
    "> * It can make the model more interpretable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28fea10",
   "metadata": {},
   "source": [
    "## 5. How much time will it take to train another Decision Tree on a training set of 10 million instances if it takes an hour to train a Decision Tree on a training set with 1 million instances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b643db6",
   "metadata": {},
   "source": [
    "> If it takes an hour to train a decision tree on a training set of 1 million instances, then it will take 11.66 hours to train a decision tree on a training set of 10 million instances.\n",
    "\n",
    "> The **training time of a decision tree** is proportional to the number of instances in the training set. This is because the algorithm has to consider all of the instances in the training set when it is making decisions about how to split the tree.\n",
    "\n",
    "> Here is the calculation:\n",
    "\n",
    "                    Training time = (number of instances * time per instance)\n",
    "                    Training time = (10 million * 1 hour/million)\n",
    "                    Training time = 11.66 hours\n",
    "\n",
    "> The actual training time may be slightly different, depending on the specific hardware and software that is being used. > "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eadafa",
   "metadata": {},
   "source": [
    "## 6. Will setting presort=True speed up training if your training set has 100,000 instances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c333576",
   "metadata": {},
   "source": [
    "> No, setting ```presort = True``` will not speed up training if your training set has 100,000 instances. In fact, it will slow down training.\n",
    "\n",
    "> The ```presort = True``` parameter is used to pre-sort the training set before the decision tree is trained. This can speed up training if the training set is very large and the features are not well-scaled. However, if the training set is small, such as 100,000 instances, then the pre-sorting step will actually slow down training.\n",
    "\n",
    "> This is because the pre-sorting step has to sort the entire training set, which can take a significant amount of time. For a training set of 100,000 instances, the pre-sorting step can take several minutes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123ad5fd",
   "metadata": {},
   "source": [
    "## 7. Follow these steps to train and fine-tune a Decision Tree for the moons dataset:\n",
    "\n",
    "        a. To build a moons dataset, use make moons(n samples=10000, noise=0.4).\n",
    "        b. Divide the dataset into a training and a test collection with train test split().\n",
    "        c. To find good hyperparameters values for a DecisionTreeClassifier, use grid search with cross-validation (with the                   GridSearchCV class). Try different values for max leaf nodes.\n",
    "        d. Use these hyperparameters to train the model on the entire training set, and then assess its output on the test set.               You can achieve an accuracy of 85 to 87 percent.\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d77fa0",
   "metadata": {},
   "source": [
    "> here are the steps to train and fine-tune a decision tree for the moons dataset:\n",
    "\n",
    "<b><i> a. To build a moons dataset, use make moons(n samples=10000, noise=0.4) </i></b> : <br>\n",
    "> This code will generate the moons dataset, which is a toy dataset that is often used to test decision trees. The dataset consists of 10,000 points, each of which is a two-dimensional vector. The labels for the points are either 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1041b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Generate the moons dataset\n",
    "X, y = make_moons(n_samples=10000, noise=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe7bc86",
   "metadata": {},
   "source": [
    "<b><i> b. Divide the dataset into a training and a test collection with train test split(): </i></b> \n",
    "> This code will split the dataset into a training set and a test set, with a 75/25 split. The training set will be used to train the decision tree, and the test set will be used to evaluate the performance of the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275c46c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f51d60",
   "metadata": {},
   "source": [
    "<b><i> c. To find good hyperparameters values for a DecisionTreeClassifier, use grid search with cross-\n",
    "validation (with the GridSearchCV class). Try different values for max leaf nodes: </i></b> \n",
    "> This code will use grid search to find the best hyperparameters for the decision tree. The grid search will try different values for the max_leaf_nodes parameter and will select the parameters that give the best accuracy on the training data.\n",
    "\n",
    "> The code will then print the best hyperparameters that were found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575b836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "parameters = {'max_leaf_nodes': list(range(2, 50))}\n",
    "\n",
    "# Create a grid search object\n",
    "clf = GridSearchCV(DecisionTreeClassifier(), parameters, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search object to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c604ba3",
   "metadata": {},
   "source": [
    "<b><i> d. Use these hyperparameters to train the model on the entire training set, and then assess its\n",
    "output on the test set. You can achieve an accuracy of 85 to 87 percent: </i></b> \n",
    "> This code will train the decision tree on the entire training set. The code will then evaluate the decision tree on the test set and print the accuracy.\n",
    "\n",
    "> The accuracy of the decision tree should be between 85 and 87 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd5a3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the decision tree on the entire training set\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the decision tree on the test set\n",
    "print(\"Accuracy:\", clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d79a4b",
   "metadata": {},
   "source": [
    "<b><i> Combining all the codes: </i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a9455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Generate the moons dataset\n",
    "X, y = make_moons(n_samples=10000, noise=0.4, random_state=21)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# Create a decision tree classifier\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier on the test set\n",
    "print(\"Accuracy:\", tree_clf.score(X_test, y_test))\n",
    "\n",
    "# Fine-tune the hyperparameters of the classifier\n",
    "parameters = {'max_depth': list(range(2, 50)), 'min_samples_split': [2, 3, 4]}\n",
    "clf = GridSearchCV(tree_clf, parameters, cv=5, scoring='accuracy')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier again\n",
    "print(\"Best accuracy:\", clf.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e592c921",
   "metadata": {},
   "source": [
    "## 8. Follow these steps to grow a forest:\n",
    "\n",
    "        a. Using the same method as before, create 1,000 subsets of the training set, each containing\n",
    "            100 instances chosen at random. You can do this with Scikit-ShuffleSplit Learn's class.\n",
    "        b. Using the best hyperparameter values found in the previous exercise, train one Decision\n",
    "            Tree on each subset. On the test collection, evaluate these 1,000 Decision Trees. These Decision\n",
    "            Trees would likely perform worse than the first Decision Tree, achieving only around 80% accuracy,\n",
    "            since they were trained on smaller sets.\n",
    "        c. Now the magic begins. Create 1,000 Decision Tree predictions for each test set case, and\n",
    "            keep only the most common prediction (you can do this with SciPy's mode() function). Over the test\n",
    "            collection, this method gives you majority-vote predictions.\n",
    "        d. On the test range, evaluate these predictions: you should achieve a slightly higher accuracy\n",
    "            than the first model (approx 0.5 to 1.5 percent higher). You've successfully learned a Random Forest\n",
    "            classifier\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4d2962",
   "metadata": {},
   "source": [
    "> here are the steps to grow a forest:\n",
    "\n",
    "<b><i> a. Using the same method as before, create 1,000 subsets of the training set, each containing\n",
    "100 instances chosen at random. You can do this with Scikit-ShuffleSplit Learn's class. </i></b> \n",
    "> This code will use the ShuffleSplit class to split the training set into 1000 subsets. Each subset will contain 100 instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070fa01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# Create a shuffle split object\n",
    "splitter = ShuffleSplit(n_splits=1000, test_size=0.25, random_state=21)\n",
    "\n",
    "# Split the training set into 1000 subsets\n",
    "X_subsets, _ = splitter.split(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1af3c7",
   "metadata": {},
   "source": [
    "<b><i> b. Using the best hyperparameter values found in the previous exercise, train one Decision\n",
    "Tree on each subset. On the test collection, evaluate these 1,000 Decision Trees. These Decision\n",
    "Trees would likely perform worse than the first Decision Tree, achieving only around 80% accuracy,\n",
    "since they were trained on smaller sets. </i></b> \n",
    "\n",
    "> This code will train 1000 decision trees on the 1000 subsets. The code will then evaluate the decision trees on the test set and print the mean accuracy.\n",
    "\n",
    "> The mean accuracy of the decision trees should be around 80 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff6cdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create a decision tree classifier\n",
    "clf = DecisionTreeClassifier(**clf.best_params_)\n",
    "\n",
    "# Train the decision tree on each subset\n",
    "for X_subset in X_subsets:\n",
    "    clf.fit(X_subset, y_train)\n",
    "\n",
    "# Evaluate the decision trees on the test set\n",
    "accuracies = []\n",
    "for clf in clfs:\n",
    "    accuracies.append(clf.score(X_test, y_test))\n",
    "\n",
    "print(\"Mean accuracy:\", np.mean(accuracies))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82b9fc6",
   "metadata": {},
   "source": [
    "<b><i> c. Now the magic begins. Create 1,000 Decision Tree predictions for each test set case, and\n",
    "keep only the most common prediction (you can do this with SciPy's mode() function). Over the test\n",
    "collection, this method gives you majority-vote predictions. </i></b> \n",
    "\n",
    "> This code will create a list to store the predictions for each test set case. The code will then iterate through the test set cases and get the predictions for the 1000 decision trees for each case. The code will then get the most common prediction for each case and add it to the list of predictions.\n",
    "\n",
    "> The code will then calculate the accuracy of the majority-vote predictions and print the accuracy.\n",
    "\n",
    "> The accuracy of the majority-vote predictions should be slightly higher than the accuracy of the individual decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "# Create a list to store the predictions for each test set case\n",
    "predictions = []\n",
    "\n",
    "# For each test set case\n",
    "for X_test_instance in X_test:\n",
    "    # Get the predictions for the 1000 decision trees\n",
    "    predictions_for_instance = [clf.predict(X_test_instance) for clf in clfs]\n",
    "\n",
    "    # Get the most common prediction\n",
    "    most_common_prediction = mode(predictions_for_instance)[0][0]\n",
    "\n",
    "    # Add the prediction to the list of predictions\n",
    "    predictions.append(most_common_prediction)\n",
    "\n",
    "# Calculate the accuracy of the majority-vote predictions\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e689613",
   "metadata": {},
   "source": [
    "<b><i> d. On the test range, evaluate these predictions: you should achieve a slightly higher accuracy\n",
    "than the first model (approx 0.5 to 1.5 percent higher). You've successfully learned a Random Forest\n",
    "classifier </i></b> \n",
    "\n",
    "> The accuracy of the majority-vote predictions should be between 85 and 87 percent. This is slightly higher than the accuracy of the individual decision trees, which was around 80 percent.\n",
    "\n",
    "> This is because the majority-vote predictions are more likely to be correct than the predictions of any individual decision tree. This is because the majority-vote predictions take into account the predictions of all of the decision trees, and are therefore less likely to be biased by the predictions of any individual decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c94fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccc6be8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
