{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "130dc62d",
   "metadata": {},
   "source": [
    "<h1> <u> <font color= green > ML_Assignment_9 </font> </u> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d504c1",
   "metadata": {},
   "source": [
    "## 1. What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c81b00",
   "metadata": {},
   "source": [
    "> **Feature engineering** is the process of transforming raw data into features that are more suitable for machine learning algorithms. This can involve a variety of tasks, such as:\n",
    "\n",
    "> 1. **Data cleaning**: Data cleaning is the process of removing noise and errors from the data. This is an important step in feature engineering because it can improve the accuracy of machine learning models. There are a number of different techniques that can be used for data cleaning, such as:\n",
    ">> * **Removing duplicate data**: This involves removing data points that are identical or nearly identical.\n",
    ">> * **Correcting errors**: This involves correcting data points that contain errors, such as typos or missing values.\n",
    ">> * **Imputing missing values**: This involves filling in missing values with estimates or averages.\n",
    "\n",
    "> 2. **Feature selection**: Feature selection is the process of selecting a subset of features that are most relevant to the target variable. This is an important step in feature engineering because it can improve the performance of machine learning models by reducing the noise in the data and making the data more focused on the target variable. There are a number of different techniques that can be used for feature selection, such as:\n",
    ">> * **Filter methods**: These methods select features based on their statistical properties, such as their correlation with the target variable or their variance.\n",
    ">> * **Wrapper methods**: These methods select features by iteratively building and evaluating machine learning models with different subsets of features.\n",
    ">> * **Embedded methods**: These methods select features as part of the machine learning algorithm.\n",
    "\n",
    "> 3. **Feature extraction**: Feature extraction is the process of creating new features from existing features. This is an important step in feature engineering because it can improve the performance of machine learning models by extracting more information from the data. There are a number of different techniques that can be used for feature extraction, such as:\n",
    ">> * **Principal component analysis (PCA)**: This technique transforms the data into a new set of features that are uncorrelated with each other.\n",
    ">> * **Independent component analysis (ICA)**: This technique transforms the data into a new set of features that are independent of each other.\n",
    ">> * **Feature hashing**: This technique creates new features by hashing the existing features.\n",
    "\n",
    "> 4. **Feature transformation**: Feature transformation is the process of changing the representation of features to make them more suitable for machine learning algorithms. This is an important step in feature engineering because it can improve the performance of machine learning models by making the data more compatible with the machine learning algorithm. There are a number of different techniques that can be used for feature transformation, such as:\n",
    ">> * **Scaling**: This technique normalizes the features so that they have a similar scale.\n",
    ">> * **Encoding**: This technique converts categorical features into numeric features.\n",
    ">> * **Derivation**: This technique creates new features from existing features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd006b8",
   "metadata": {},
   "source": [
    "## 2. What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa286a6",
   "metadata": {},
   "source": [
    "> **Feature selection** is the process of selecting a subset of features from a dataset that are most relevant to the target variable. The aim of feature selection are:\n",
    "> * To improve the accuracy of machine learning models.\n",
    "> * To reduce the complexity of machine learning models.\n",
    "> * To make machine learning models more interpretable.\n",
    "\n",
    "> The various methods of feature selection are:\n",
    "> * **Filter methods**: Filter methods select features based on their statistical properties. These methods are relatively simple to implement and can be used with any machine learning algorithm. However, filter methods can sometimes select irrelevant features and may not be able to capture the complex relationships between features.\n",
    "> * **Wrapper methods**: Wrapper methods select features by iteratively building and evaluating machine learning models with different subsets of features. These methods are more complex to implement than filter methods, but they can be more effective at selecting relevant features. However, wrapper methods can be computationally expensive and may not be suitable for large datasets.\n",
    "> * **Embedded methods**: Embedded methods select features as part of the machine learning algorithm. These methods are relatively efficient and can be used with any machine learning algorithm. However, embedded methods can be difficult to interpret and may not be suitable for all problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66a0f0f",
   "metadata": {},
   "source": [
    "## 3. Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d821c2",
   "metadata": {},
   "source": [
    "> **Filter methods** select features based on their statistical properties, such as their correlation with the target variable or their variance. These methods are relatively simple to implement and can be used with any machine learning algorithm. However, filter methods can sometimes select irrelevant features and may not be able to capture the complex relationships between features.\n",
    "\n",
    ">> Advantages of filter methods:\n",
    ">> * Simple to implement: Filter methods are relatively easy to implement and can be used with any machine learning algorithm.\n",
    ">> * Fast: Filter methods are fast to run and can be used to select features from large datasets.\n",
    ">> * Interpretable: Filter methods are relatively interpretable, which can be helpful for understanding the results of the feature selection process.\n",
    "\n",
    ">> Disadvantages of filter methods:\n",
    ">> * Can select irrelevant features: Filter methods may select irrelevant features, which can reduce the accuracy of the machine learning model.\n",
    ">> * May not capture complex relationships: Filter methods may not be able to capture the complex relationships between features, which can also reduce the accuracy of the machine learning model.\n",
    "---\n",
    "> **Wrapper methods** select features by iteratively building and evaluating machine learning models with different subsets of features. These methods are more complex to implement than filter methods, but they can be more effective at selecting relevant features. However, wrapper methods can be computationally expensive and may not be suitable for large datasets.\n",
    "\n",
    ">> Advantages of wrapper methods:\n",
    ">> * More effective at selecting relevant features: Wrapper methods are more effective at selecting relevant features than filter methods.\n",
    ">> * Can capture complex relationships: Wrapper methods can capture the complex relationships between features, which can improve the accuracy of the machine learning model.\n",
    "\n",
    ">> Disadvantages of wrapper methods:\n",
    ">> * Computationally expensive: Wrapper methods can be computationally expensive, especially for large datasets.\n",
    ">> * Not interpretable: Wrapper methods are not as interpretable as filter methods, which can make it difficult to understand the results of the feature selection process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789ed75c",
   "metadata": {},
   "source": [
    "## 4.\n",
    "## i. Describe the overall feature selection process.\n",
    "## ii. Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac03d94",
   "metadata": {},
   "source": [
    "<b><i> i. Describe the overall feature selection process. </i></b> \n",
    "> The overall feature selection process can be divided into the following steps:\n",
    "> * **Data pre-processing**: This step involves cleaning the data and removing any irrelevant or noisy features.\n",
    "> * **Feature selection**: This step involves selecting a subset of features that are most relevant to the target variable.\n",
    "> * **Model training**: This step involves training a machine learning model on the selected features.\n",
    "> * **Model evaluation**: This step involves evaluating the performance of the machine learning model on a held-out dataset.\n",
    "\n",
    "<b><i> ii. Explain the key underlying principle of feature extraction using an example. </i></b> \n",
    "> The key underlying principle of feature extraction is to transform the original features into a new set of features that are more informative and relevant to the target variable. This can be done by using a variety of techniques, such as:\n",
    "> * **Principal component analysis (PCA)**: PCA transforms the data into a new set of features that are uncorrelated with each other.\n",
    "> * **Independent component analysis (ICA)**: ICA transforms the data into a new set of features that are independent of each other.\n",
    "> * **Feature hashing**: Feature hashing creates new features by hashing the existing features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a79f15",
   "metadata": {},
   "source": [
    "## 5. Describe the feature engineering process in the sense of a text categorization issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c27aeb",
   "metadata": {},
   "source": [
    "> **Feature engineering** is the process of transforming raw data into features that are more suitable for machine learning algorithms. In the context of text categorization, this can involve a variety of tasks, such as:\n",
    "> * **Tokenization**: This involves breaking the text into individual words or phrases.\n",
    "> * **Stop word removal**: This involves removing common words that do not provide much information, such as \"the\", \"is\", and \"are\".\n",
    "> * **Stemming**: This involves reducing words to their root form, such as \"running\" to \"run\".\n",
    "> * **Lemmatization**: This involves grouping together words that have the same meaning, such as \"walking\" and \"walked\".\n",
    "> * **Feature extraction**: This involves creating new features from the original text, such as the number of words in a document or the frequency of certain words.\n",
    "\n",
    "> The **goal of feature engineering in text categorization** is to create features that are informative and relevant to the target variable. This can help to improve the performance of machine learning models that are used to classify text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4bbb1a",
   "metadata": {},
   "source": [
    "## 6. What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fcac49",
   "metadata": {},
   "source": [
    "> **Cosine similarity** is a good metric for text categorization because it measures the similarity between two documents based on the terms that they share. This is important in text categorization because it allows us to compare documents that are not necessarily in the same format or that do not use the same words.\n",
    "\n",
    "> For example, let's say we have two documents, one about \"dogs\" and one about \"cats\". The documents may not use the same words, but they will likely share some common terms, such as \"animal\", \"pet\", and \"furry\". Cosine similarity can be used to measure the similarity between these two documents, even though they are not in the same format or that do not use the same words.\n",
    "---\n",
    "> <b><i> The cosine similarity between two documents is calculated as follows: </i></b> <br>\n",
    "```cosine_similarity = (dot_product)/(norm_1 * norm_2)```\n",
    "\n",
    "            where:\n",
    "               dot_product is the dot product of the two vectors.\n",
    "               norm_1 is the norm of the first vector.\n",
    "               norm_2 is the norm of the second vector.\n",
    "               The dot product is calculated by multiplying the corresponding elements of the two vectors and then summing the products. \n",
    "               The norm of a vector is calculated by taking the square root of the sum of the squares of the elements of the vector.\n",
    "\n",
    "> <b><i> The cosine similarity between the two documents in your example is calculated as follows: </i></b>\n",
    "\n",
    "            dot_product = (2*2) + (3*1) + (2*0) + (0*0) + (2*3) + (3*2) + (3*1) + (0*3) + (1*1) = 24\n",
    "            norm_1 = sqrt((2^2) + (3^2) + (2^2) + (0^2) + (2^2) + (3^2) + (3^2) + (0^2) + (1^2)) = sqrt(58)\n",
    "            norm_2 = sqrt((2^2) + (1^2) + (0^2) + (0^2) + (3^2) + (2^2) + (1^2) + (3^2) + (1^2)) = sqrt(29)\n",
    "            cosine_similarity = (24)/(sqrt(58) * sqrt(29)) = 0.927\n",
    "\n",
    "> The cosine similarity between the two documents is 0.927, which indicates that they are highly similar. This is because they share many of the same terms, even though they are not in the same format or that do not use the same words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31089dc1",
   "metadata": {},
   "source": [
    "## 7.\n",
    "### i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap.\n",
    "### ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0, 0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea19ec3",
   "metadata": {},
   "source": [
    "<b><i> i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap. </i></b> \n",
    "> The **Hamming distance** is a measure of the difference between two binary strings. It is calculated by counting the number of bits that are different between the two strings.\n",
    "\n",
    "> The formula for calculating the Hamming distance is: <br>\n",
    "```hamming_distance = sum(x != y for x, y in zip(str1, str2))```\n",
    "\n",
    "            where:\n",
    "                str1 = the first binary string.\n",
    "                str2 = the second binary string.\n",
    "                zip = a function that takes two iterables and returns a new iterable that combines elements from the two iterables.\n",
    "                sum = a function that takes an iterable and returns the sum of the elements in the iterable.\n",
    "\n",
    "> In this example, the Hamming distance between 10001011 and 11001111 is 4. This is because there are 4 bits that are different between the two strings: the first bit, the third bit, the fifth bit, and the seventh bit.\n",
    "---\n",
    "<b><i> ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0, 0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1). </i></b> \n",
    "> The **Jaccard index** and **similarity matching coefficient** are two metrics that are used to measure the similarity between two sets. The **Jaccard index** is calculated by dividing the intersection of the two sets by the union of the two sets. The **similarity matching coefficient** is calculated by dividing the number of matching elements in the two sets by the total number of elements in the two sets.\n",
    "\n",
    "> * In your example, the **Jaccard index** of the two features is 0.75. This is because there are 6 matching elements in the two sets (the first two elements, the fourth element, the sixth element, and the seventh element) and a total of 8 elements in the two sets.\n",
    "> * The similarity **matching coefficient** of the two features is 0.5. This is because there are 4 matching elements in the two sets (the first two elements, the fourth element, and the seventh element) and a total of 8 elements in the two sets.\n",
    "\n",
    "> The **Jaccard index** is a more sensitive metric than the similarity matching coefficient. This is because the Jaccard index takes into account the size of the two sets, while the similarity matching coefficient does not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ded9af5",
   "metadata": {},
   "source": [
    "## 8. State what is meant by \"high-dimensional data set\"? Could you offer a few real-life examples? What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b45686a",
   "metadata": {},
   "source": [
    "> A **high-dimensional dataset** is a dataset where the number of features is much larger than the number of samples. This can make it difficult to use machine learning techniques on the dataset, as the models may not be able to learn the relationships between the features.\n",
    "\n",
    "> few real-life examples of high-dimensional datasets are:\n",
    "> * **Image datasets**: Image datasets typically have hundreds or thousands of features, corresponding to the different pixels in the image.\n",
    "> * **Gene expression datasets**: Gene expression datasets typically have thousands or even millions of features, corresponding to the different genes that are expressed in the sample.\n",
    "> * **Text datasets**: Text datasets typically have thousands or even millions of features, corresponding to the different words that appear in the text.\n",
    "---\n",
    "> The main difficulties in using machine learning techniques on a high-dimensional dataset are:\n",
    "> * **The curse of dimensionality**: The curse of dimensionality refers to the fact that as the number of dimensions increases, the volume of the space increases exponentially. This can make it difficult to find patterns in the data, as the patterns may be too small to be detected.\n",
    "> * **The computational complexity of machine learning algorithms**: Machine learning algorithms can be computationally expensive to train, especially on high-dimensional datasets. This is because the algorithms need to search through a large space of possible solutions.\n",
    "> * **The difficulty of interpreting machine learning models**: Machine learning models can be difficult to interpret on high-dimensional datasets, as the models may learn complex relationships between the features that are difficult to understand.\n",
    "\n",
    "> To address the difficulties of using machine learning techniques on high-dimensional datasets are:\n",
    "> * **Feature selection**: Feature selection is the process of selecting a subset of features that are most relevant to the target variable. This can help to reduce the dimensionality of the dataset and make it easier to find patterns in the data.\n",
    "> * **Dimensionality reduction**: Dimensionality reduction is the process of transforming the dataset into a lower-dimensional space. This can help to reduce the computational complexity of machine learning algorithms and make it easier to interpret the models.\n",
    "> * **Ensemble learning**: Ensemble learning is the process of combining multiple machine learning models to improve the performance of the models. This can be helpful for high-dimensional datasets, as it can help to reduce the variance of the models and make them more robust to noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0285330",
   "metadata": {},
   "source": [
    "## 9. Make a few quick notes on:\n",
    "1. PCA is an acronym for Principle Component Analysis.\n",
    "2. Use of vectors\n",
    "3. Embedded technique\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c285286",
   "metadata": {},
   "source": [
    "<b><i> 1. PCA is an acronym for Principle Component Analysis. </i></b> \n",
    "> Principle Component Analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.\n",
    "\n",
    "<b><i> 2. Use of vectors </i></b> \n",
    "> PCA uses vectors to represent the features of a dataset. A vector is a mathematical object that has both magnitude and direction. In PCA, the vectors are used to represent the directions in which the data varies the most.\n",
    "\n",
    "<b><i> 3. Embedded technique </i></b> \n",
    "> PCA is an embedded technique. This means that PCA is used as part of another machine learning algorithm. For example, PCA can be used to reduce the dimensionality of a dataset before it is used to train a classification algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2553094",
   "metadata": {},
   "source": [
    "## 10. Make a comparison between:\n",
    "1. Sequential backward exclusion vs. sequential forward selection\n",
    "2. Function selection methods: filter vs. wrapper\n",
    "3. SMC vs. Jaccard coefficient\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88fcbd6",
   "metadata": {},
   "source": [
    "<b><i> Function selection methods: filter vs. wrapper </i></b>\n",
    "\n",
    "<table style=\"border:1px solid black; \">\n",
    "  <tr style=\"border:1px solid black\">\n",
    "    <th style=\"border:1px solid black\">Feature</th>\n",
    "    <th style=\"border:1px solid black\">Filter methods</th>\n",
    "    <th style=\"border:1px solid black\">Wrapper methods</th>\n",
    "  </tr>\n",
    "  <tr style=\"border:1px solid black\">\n",
    "    <td style=\"border:1px solid black\">Algorithm</td>\n",
    "    <td style=\"border:1px solid black\">Select features based on their statistical properties</td>\n",
    "    <td style=\"border:1px solid black\">Select features by building and evaluating a machine learning model</td>\n",
    "  <tr style=\"border:1px solid black\">\n",
    "    <td style=\"border:1px solid black\">Advantages</td>\n",
    "    <td style=\"border:1px solid black\">Simple to implement and interpret</td>\n",
    "    <td style=\"border:1px solid black\">Can be more effective than filter methods if the machine learning model is well-chosen</td>\n",
    "  </tr>\n",
    "  <tr style=\"border:1px solid black\">\n",
    "    <td style=\"border:1px solid black\">Disadvantages</td>\n",
    "    <td style=\"border:1px solid black\">Can select irrelevant features</td>\n",
    "    <td style=\"border:1px solid black\">Can be computationally expensive</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8685a8d6",
   "metadata": {},
   "source": [
    "<b><i> SMC vs. Jaccard coefficient </i></b>\n",
    "\n",
    "<table style=\"border:1px solid black;\">\n",
    "  <tr style=\"border:1px solid black\">\n",
    "    <th style=\"border:1px solid black\">Feature</th>\n",
    "    <th style=\"border:1px solid black\">SMC</th>\n",
    "    <th style=\"border:1px solid black\">Jaccard coefficient</th>\n",
    "  </tr>\n",
    "  <tr style=\"border:1px solid black\">\n",
    "    <td style=\"border:1px solid black\">Definition</td>\n",
    "    <td style=\"border:1px solid black\">The similarity between two sets based on the number of common elements</td>\n",
    "    <td style=\"border:1px solid black\">The similarity between two sets based on the size of the intersection and the union of the two sets</td>\n",
    "  <tr style=\"border:1px solid black\">\n",
    "    <td style=\"border:1px solid black\">Advantages</td>\n",
    "    <td style=\"border:1px solid black\">Simple to calculate</td>\n",
    "    <td style=\"border:1px solid black\">More sensitive to the difference between two sets</td>\n",
    "  </tr>\n",
    "  <tr style=\"border:1px solid black\">\n",
    "    <td style=\"border:1px solid black\">Disadvantages</td>\n",
    "    <td style=\"border:1px solid black\">Not as sensitive to the size of the two sets</td>\n",
    "    <td style=\"border:1px solid black\">Not as interpretable as SMC</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe4d979",
   "metadata": {},
   "source": [
    "<b><i> Sequential backward exclusion vs. sequential forward selection </i></b>\n",
    "\n",
    "<table style=\"border:1px solid black;\">\n",
    "  <tr style=\"border:1px solid black\">\n",
    "    <th style=\"border:1px solid black\">Feature</th>\n",
    "    <th style=\"border:1px solid black\">Sequential Backward Elimination</th>\n",
    "    <th style=\"border:1px solid black\">Sequential Forward Selection</th>\n",
    "  </tr>\n",
    "  <tr style=\"border:1px solid black\">\n",
    "    <td style=\"border:1px solid black\">Algorithm</td>\n",
    "    <td style=\"border:1px solid black\">Starts with all features and removes the least important one at a time</td>\n",
    "    <td style=\"border:1px solid black\">Starts with no features and adds the most important one at a time</td>\n",
    "  <tr style=\"border:1px solid black\">\n",
    "    <td style=\"border:1px solid black\">Advantages</td>\n",
    "    <td style=\"border:1px solid black\">Can be more efficient than sequential forward selection if the least important features are not highly correlated</td>\n",
    "    <td style=\"border:1px solid black\">Can be more effective than sequential backward elimination if the most important features are highly correlated</td>\n",
    "  </tr>\n",
    "  <tr style=\"border:1px solid black\">\n",
    "    <td style=\"border:1px solid black\">Disadvantages</td>\n",
    "    <td style=\"border:1px solid black\">Can be more prone to overfitting if the least important features are correlated with the target variable</td>\n",
    "    <td style=\"border:1px solid black\">Can be more time-consuming than sequential backward elimination if the dataset is large</td>\n",
    "  </tr>\n",
    " </table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a91965b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f93e09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
