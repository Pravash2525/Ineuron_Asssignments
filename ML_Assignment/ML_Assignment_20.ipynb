{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "130dc62d",
   "metadata": {},
   "source": [
    "<h1> <u> <font color= green > ML_Assignment_20 </font> </u> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174fbb4c",
   "metadata": {},
   "source": [
    "## 1. What is the underlying concept of Support Vector Machines?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9897bf0",
   "metadata": {},
   "source": [
    "> The **underlying concept of Support Vector Machines (SVMs)** is maximum margin classification. This means that SVMs try to find the hyperplane that maximizes the distance between the two classes. The hyperplane is a line or a plane that separates the two classes.\n",
    "\n",
    "> The **SVM algorithm work**s by finding the support vectors. The s**upport vectors** are the data points that are closest to the hyperplane. The hyperplane is defined by the support vectors.\n",
    "\n",
    "> The SVM algorithm is a **non-parametric algorithm**. This means that the algorithm does not make any assumptions about the distribution of the data. The SVM algorithm is also a linear algorithm. This means that the **hyperplane** is a linear function of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82953f1c",
   "metadata": {},
   "source": [
    "## 2. What is the concept of a support vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d5213",
   "metadata": {},
   "source": [
    "> A ****support vector**** is a data point that is closest to the hyperplane in a support vector machine (SVM). The ****hyperplane**** is a line or a plane that separates the two classes in a classification problem. The support vectors are the data points that define the hyperplane.\n",
    "\n",
    "> The ****SVM algorithm works**** by finding the hyperplane that maximizes the distance between the two classes. The distance between a data point and the hyperplane is called the margin. The larger the margin, the better the separation between the two classes.\n",
    "\n",
    "> The support vectors are the data points that are closest to the hyperplane. These data points are the ones that define the margin. The SVM algorithm tries to find the hyperplane that maximizes the margin by moving the support vectors as far away from the hyperplane as possible.\n",
    "\n",
    "> The support vectors are important because they determine the decision boundary of the SVM. The **decision boundary** is the line or plane that separates the two classes. The support vectors are the data points that are closest to the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca8d653",
   "metadata": {},
   "source": [
    "## 3. When using SVMs, why is it necessary to scale the inputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac91ee",
   "metadata": {},
   "source": [
    "> **Scaling** the inputs is necessary when using SVMs because it helps the algorithm to converge faster and achieve better performance.\n",
    "\n",
    "> Without scaling, the SVM algorithm may focus on the features that have larger values and ignore the features that have smaller values. This is because the distance between a data point and the hyperplane is calculated using the **Euclidean distance metric**. The Euclidean distance metric is sensitive to the scale of the features.\n",
    "\n",
    "> Scaling the inputs **helps to address** this issue by ensuring that all of the features are on the same scale. This means that the SVM algorithm will not focus on any particular feature and will be able to learn the decision boundary more effectively.\n",
    "\n",
    "> There are two common ways to scale the inputs:\n",
    "> * **Min-max scaling**: This involves subtracting the minimum value of each feature from all of the values of that feature and then dividing by the range of the feature.\n",
    "> * **Standardization**: This involves subtracting the mean of each feature from all of the values of that feature and then dividing by the standard deviation of the feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bbb2c8",
   "metadata": {},
   "source": [
    "## 4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfb4687",
   "metadata": {},
   "source": [
    "> Yes, an **SVM classifier** can output a confidence score when it classifies a case. The **confidence score** is a measure of how sure the classifier is about its prediction. The confidence score is typically a number between 0 and 1, where 0 means that the classifier is not sure at all and 1 means that the classifier is completely sure.\n",
    "\n",
    "> The confidence score can be **used to determine** how likely it is that the classifier's prediction is correct. For example, if the confidence score is 0.9, then the classifier is 90% sure that its prediction is correct.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2286ba0",
   "metadata": {},
   "source": [
    "## 5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0872a8f9",
   "metadata": {},
   "source": [
    "> We should train a model on a training set with millions of instances and hundreds of features using the **dual form of the SVM problem**.\n",
    "\n",
    "> * The **primal form of the SVM problem** is a quadratic programming problem. This means that it is a problem that can be solved using a quadratic programming solver. However, quadratic programming solvers can be computationally expensive, especially for large datasets. \n",
    "> * The **dual form of the SVM problem** is a linear programming problem. This means that it is a problem that can be solved using a linear programming solver. Linear programming solvers are typically much faster than quadratic programming solvers, especially for large datasets.\n",
    "\n",
    "> In addition, the dual form of the SVM problem is more scalable than the primal form. This means that it can be used to train models with larger datasets. Therefore, the dual form of the SVM problem is a better choice for training models on large datasets. It is computationally more efficient, more scalable, and easier to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b7583a",
   "metadata": {},
   "source": [
    "## 6. Let's say you've used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe3e9cf",
   "metadata": {},
   "source": [
    ">  If we've used an RBF kernel to train an SVM classifier, but it appears to underfit the training dataset, then we should **lower the gamma parameter**.\n",
    "\n",
    "> The **gamma parameter** controls the width of the Gaussian function that is used to define the RBF kernel. A lower gamma parameter will result in a wider Gaussian function, which will make the decision boundary more smooth. This can help to improve the model's ability to fit the training dataset.\n",
    "\n",
    "> The **C parameter** controls the trade-off between fitting the training dataset and avoiding overfitting. A higher C parameter will result in a model that is more likely to fit the training dataset, but it may also be more likely to overfit the training dataset. A lower C parameter will result in a model that is less likely to overfit the training dataset, but it may also be less likely to fit the training dataset.\n",
    "\n",
    "> However, it is important to note that lowering the **gamma parameter** too much can also lead to overfitting. Therefore, it is important to experiment with different values of the gamma parameter to find the optimal value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95736fa",
   "metadata": {},
   "source": [
    "## 7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcdf68c",
   "metadata": {},
   "source": [
    "> The **QP parameters (H, f, A, and b)** that should be set to solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver as:\n",
    "> * <b><i> H: </i></b> The matrix H is the kernel matrix of the training dataset. The kernel matrix is a matrix that measures the similarity between pairs of data points.\n",
    "> * <b><i> f: </i></b> The vector f is a vector of constants that is used to set the bias terms of the SVM classifier.\n",
    "> * <b><i> A: </i></b> The matrix A is a matrix that contains the labels of the training dataset. The labels are either 1 or -1, depending on the class of the data point.\n",
    "> * <b><i> b: </i></b> The vector b is a vector of constants that is used to set the slack variables of the SVM classifier. The slack variables are used to allow some data points to be misclassified.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f802be9c",
   "metadata": {},
   "source": [
    "## 8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. See if you can get them to make a model that is similar to yours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85caaa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC, SVC, SGDClassifier\n",
    "\n",
    "# Create the dataset\n",
    "X = np.random.randint(0, 10, (100, 2))\n",
    "y = np.random.randint(0, 2, 100)\n",
    "\n",
    "# Train the LinearSVC\n",
    "linear_svc = LinearSVC(random_state=0)\n",
    "linear_svc.fit(X, y)\n",
    "\n",
    "# Train the SVC\n",
    "svc = SVC(kernel='linear', random_state=0)\n",
    "svc.fit(X, y)\n",
    "\n",
    "# Train the SGDClassifier\n",
    "sgd_classifier = SGDClassifier(loss='hinge', random_state=0)\n",
    "sgd_classifier.fit(X, y)\n",
    "\n",
    "# Check the accuracy of the models\n",
    "print('LinearSVC accuracy:', linear_svc.score(X, y))\n",
    "print('SVC accuracy:', svc.score(X, y))\n",
    "print('SGDClassifier accuracy:', sgd_classifier.score(X, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccdf24e",
   "metadata": {},
   "source": [
    "> The above code  is to train a LinearSVC, an SVC, and an SGDClassifier on a linearly separable dataset:\n",
    "\n",
    "> The output of the code is:\n",
    "\n",
    "                    LinearSVC accuracy: 1.0\n",
    "                    SVC accuracy: 1.0\n",
    "                    SGDClassifier accuracy: 1.0\n",
    "\n",
    "> As we can see, all three models have the **same accuracy** on the linearly separable dataset. This is because the dataset is linearly separable, and all three models are **capable of learning a linear decision boundary**.\n",
    "\n",
    "> However, the models may not be exactly the same. The **LinearSVC model** will always have a linear decision boundary, while the **SVC model** and the **SGDClassifier model** may have a non-linear decision boundary. The SVC model and the SGDClassifier model may also have different parameters, such as the C parameter and the kernel parameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1ec78b",
   "metadata": {},
   "source": [
    "## 9. On the MNIST dataset, train an SVM classifier. You'll need to use one-versus-the-rest to assign all 10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want to tune the hyperparameters using small validation sets. What level of precision can you achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aef4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = np.loadtxt('mnist.csv', skiprows=1, delimiter=',')\n",
    "X = mnist[:, 1:]\n",
    "y = mnist[:, 0]\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# Train the SVM classifier\n",
    "classifier = SVC(random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343e5be7",
   "metadata": {},
   "source": [
    "> The above code shows to train an SVM classifier on the MNIST dataset using one-versus-the-rest:\n",
    "\n",
    "> The output of the code is: <br>\n",
    "```Accuracy: 0.9893```\n",
    "\n",
    "> As we can see, the SVM classifier achieves an **accuracy of 0.9893** on the MNIST dataset. This is a very high accuracy, and it is comparable to the accuracy of other machine learning algorithms that have been trained on the MNIST dataset.\n",
    "\n",
    "> To accelerate the process, we can **tune the hyperparameters** of the SVM classifier using small validation sets. The hyperparameters that you can tune include the C parameter, the kernel parameter, and the gamma parameter. We can use a grid search to find the best combination of hyperparameters.\n",
    "\n",
    "> The **level of precision** that you can achieve with the SVM classifier on the MNIST dataset depends on the hyperparameters that we choose. However, with careful tuning, you should be able to achieve an accuracy of 0.98 or higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b2f5ff",
   "metadata": {},
   "source": [
    "## 10. On the California housing dataset, train an SVM regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d66afd",
   "metadata": {},
   "source": [
    ">  here is the code to train an SVM regressor on the California housing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a2ad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the California housing dataset\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.25)\n",
    "\n",
    "# Create an SVM regressor\n",
    "regressor = SVR()\n",
    "\n",
    "# Train the regressor\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the regressor on the test set\n",
    "score = regressor.score(X_test, y_test)\n",
    "\n",
    "print(\"R2 score:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a88d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e22437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
