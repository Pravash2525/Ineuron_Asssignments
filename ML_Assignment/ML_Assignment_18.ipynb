{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f79c2d04",
   "metadata": {},
   "source": [
    "<h1> <u> <font color= green > ML_Assignment_18 </font> </u> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d4f007",
   "metadata": {},
   "source": [
    "## 1. What is the difference between supervised and unsupervised learning? Give some examples to illustrate your point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2b97d7",
   "metadata": {},
   "source": [
    "> **Supervised learning** is a type of machine learning where the model is trained on **labeled data**. This means that the data has known output values, which the model learns to predict. For example, if we want to train a model to classify images of cats and dogs, we would need to provide the model with a dataset of images that have already been labeled as cats or dogs. The model would then learn to predict the category of new images based on the features of the images.\n",
    "\n",
    "> **Unsupervised learnin**g is a type of machine learning where the model is trained on **unlabeled data**. This means that the data does not have known output values, so the model must learn to identify patterns in the data on its own. For example, if we want to train a model to cluster similar images together, we would provide the model with a dataset of images that have not been labeled. The model would then learn to identify clusters of images that are similar to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4386dc6b",
   "metadata": {},
   "source": [
    "## 2. Mention a few unsupervised learning applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b0984d",
   "metadata": {},
   "source": [
    "> unsupervised learning applications:\n",
    "> * **Customer segmentation**: Unsupervised learning can be used to segment customers into different groups based on their behavior or characteristics. This can be used to target marketing campaigns more effectively or to understand customer needs better.\n",
    "> * **Recommendation systems**: Unsupervised learning can be used to recommend products or services to users based on their past behavior or interests. This is a common application of unsupervised learning in e-commerce and other industries.\n",
    "> * **Anomaly detection**: Unsupervised learning can be used to identify anomalies in data, such as fraudulent transactions or unusual patterns of behavior. This can be used to improve the security of a system or to identify potential problems early on.\n",
    "> * **Dimensionality reduction**: Unsupervised learning can be used to reduce the dimensionality of data, which can make it easier to visualize or analyze the data. This is a common application of unsupervised learning in machine learning and data science.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a16163f",
   "metadata": {},
   "source": [
    "## 3. What are the three main types of clustering methods? Briefly describe the characteristics of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ac53fc",
   "metadata": {},
   "source": [
    "> The three main types of clustering methods are:\n",
    "> * **Hierarchical clustering**: Hierarchical clustering is a recursive algorithm that builds a hierarchy of clusters. The algorithm starts with each data point as its own cluster, and then merges clusters together until there is only one cluster left. There are two main types of hierarchical clustering: agglomerative and divisive. Agglomerative hierarchical clustering starts with each data point as its own cluster, and then merges clusters together based on their similarity. Divisive hierarchical clustering starts with all of the data points in one cluster, and then divides the cluster into smaller and smaller clusters based on their dissimilarity.\n",
    "> * **Density-based clustering**: Density-based clustering algorithms identify clusters based on the density of data points in a given area. These algorithms start with a seed point, and then grow clusters around the seed point until the density of data points drops below a certain threshold. There are two main types of density-based clustering algorithms: DBSCAN and OPTICS. DBSCAN is a simple algorithm that works well for finding clusters of arbitrary shapes. OPTICS is a more sophisticated algorithm that can find clusters of different shapes and sizes.\n",
    "> * **Centroid-based clustering**: Centroid-based clustering algorithms identify clusters based on the centroids of the clusters. The centroid of a cluster is the average of all of the data points in the cluster. These algorithms start by randomly selecting a set of centroids, and then assign each data point to the cluster with the closest centroid. The centroids are then recalculated, and the process is repeated until the clusters no longer change. There are two main types of centroid-based clustering algorithms: k-means and Gaussian mixture models. k-means is a simple algorithm that is often used for finding clusters of spherical shapes. Gaussian mixture models are a more sophisticated algorithm that can find clusters of different shapes and sizes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd637b5c",
   "metadata": {},
   "source": [
    "## 4. Explain how the k-means algorithm determines the consistency of clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0023756f",
   "metadata": {},
   "source": [
    "> The **k-means algorithm** determines the consistency of clustering by calculating the **within-cluster sum of squares (WSS)**. The WSS is a measure of the variation within a cluster. A low WSS indicates that the data points in a cluster are close together, while a high WSS indicates that the data points in a cluster are spread out.\n",
    "\n",
    "> The **k-means algorithm iterates** until the WSS does not decrease significantly. This means that the algorithm is trying to find a set of clusters that minimizes the variation within the clusters.\n",
    "\n",
    "> The consistency of clustering can be assessed by looking at the WSS after each iteration of the k-means algorithm. If the WSS does not decrease significantly after a few iterations, then the algorithm has probably converged to a good solution. However, if the WSS continues to decrease, then the algorithm may be overfitting the data and the clusters may not be very consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6345be36",
   "metadata": {},
   "source": [
    "## 5. With a simple illustration, explain the key difference between the k-means and k-medoids algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41683447",
   "metadata": {},
   "source": [
    "> The key difference between the k-means and k-medoids algorithms is that **k-means** uses the centroids of the clusters to assign data points, while **k-medoids** uses the medoids of the clusters to assign data points.\n",
    "\n",
    "> The **centroid of a cluste**r is the average of all of the data points in the cluster. The **medoid of a cluster** is the data point in the cluster that is closest to the other data points in the cluster.\n",
    "\n",
    "> This difference can be illustrated with a simple example. Consider the following dataset of 4 data points: <br>\n",
    "```[1, 2], [2, 3], [3, 4], [4, 5]```\n",
    "\n",
    "> The **k-means algorithm** would assign these data points to two clusters, with the first cluster containing the first two data points and the second cluster containing the last two data points. The centroids of these clusters would be (1.5, 2.5) and (3.5, 4.5), respectively.\n",
    "\n",
    "> The **k-medoids algorithm** would assign these data points to two clusters, with the first cluster containing the first and third data points and the second cluster containing the second and fourth data points. The medoids of these clusters would be (1, 2) and (4, 5), respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53e39e5",
   "metadata": {},
   "source": [
    "## 6. What is a dendrogram, and how does it work? Explain how to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ee5b0",
   "metadata": {},
   "source": [
    "> A **dendrogram** is a tree-like diagram that shows the hierarchical relationships between clusters. It is a common way to visualize the results of hierarchical clustering algorithms.\n",
    "\n",
    "> **Dendrograms are created by** recursively merging clusters together based on their similarity. The clusters are merged together starting with the most dissimilar clusters and ending with the most similar clusters. The dendrogram shows the order in which the clusters were merged together.\n",
    "\n",
    "> The **height of a node in the dendrogram represents** the distance between the clusters that were merged at that node. The closer the two clusters are, the shorter the distance between them, and the higher the node will be in the dendrogram.\n",
    "\n",
    "> Dendrograms can be used to **visualize the results of hierarchical clustering algorithms** and to identify the clusters that are most similar to each other. They can also be used to select the number of clusters to use in a hierarchical clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbff0c1d",
   "metadata": {},
   "source": [
    "## 7. What exactly is SSE? What role does it play in the k-means algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7843295a",
   "metadata": {},
   "source": [
    "> **SSE stands for sum of squared errors**. It is a measure of the error between the data points and the centroids of their respective clusters in k-means clustering.\n",
    "\n",
    "> In **k-means clustering**, the goal is to minimize the SSE. This means that the algorithm tries to find a set of clusters such that the sum of the squared distances between the data points and the centroids of their respective clusters is as small as possible.\n",
    "\n",
    "> The SSE is a measure of how well the data points are clustered. A low SSE indicates that the data points are well-clustered, while a high SSE indicates that the data points are not well-clustered.\n",
    "\n",
    "> The SSE plays an important role in the k-means algorithm. The algorithm iterates until the SSE does not decrease significantly. This means that the algorithm is trying to find a set of clusters such that the SSE is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79da3569",
   "metadata": {},
   "source": [
    "## 8. With a step-by-step algorithm, explain the k-means procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc6d92e",
   "metadata": {},
   "source": [
    "> Here is a step-by-step algorithm for k-means clustering:\n",
    "> * **Choose the number of clusters, k**: The number of clusters is a hyperparameter that must be specified by the user.\n",
    "> * **Initialize k centroids**: The centroids are the points that will represent the clusters. They can be initialized randomly or using some other method.\n",
    "> * **Assign the data points to clusters**: Each data point is assigned to the cluster whose centroid is closest to it.\n",
    "> * **Update the centroids**: The centroids are updated by averaging the data points that are assigned to them.\n",
    "> * **Repeat steps 3 and 4 until the centroids do not change significantly**: This means that the algorithm has converged to a set of clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f4a821",
   "metadata": {},
   "source": [
    "## 9. In the sense of hierarchical clustering, define the terms single link and complete link."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e72a7ca",
   "metadata": {},
   "source": [
    "> In the sense of hierarchical clustering, single link and complete link are two different linkage criteria that can be used to merge clusters.\n",
    "\n",
    "> **Single link clustering** merges two clusters together if the distance between the closest data points in the two clusters is minimized. This means that single link clustering tends to merge clusters that are chain-like.\n",
    "\n",
    "> **Complete link clustering** merges two clusters together if the distance between the farthest data points in the two clusters is minimized. This means that complete link clustering tends to merge clusters that are compact.\n",
    "\n",
    "---\n",
    "<table style=\"border:1px solid black;\">\n",
    "  <tr style=\"border:1px solid black\">\n",
    "    <th style=\"border:1px solid black\">Feature</th>\n",
    "    <th style=\"border:1px solid black\">Single link</th>\n",
    "    <th style=\"border:1px solid black\">Complete link\n",
    "</th>\n",
    "  </tr>\n",
    "  <tr style=\"border:1px solid black\">\n",
    "    <td style=\"border:1px solid black\">Distance metric</td>\n",
    "    <td style=\"border:1px solid black\">Minimum distance between two data points</td>\n",
    "    <td style=\"border:1px solid black\">Maximum distance between two data points</td>\n",
    "  <tr style=\"border:1px solid black\">\n",
    "    <td style=\"border:1px solid black\">Cluster shape</td>\n",
    "    <td style=\"border:1px solid black\">Chain-like</td>\n",
    "    <td style=\"border:1px solid black\">Compact</td>\n",
    "  </tr>\n",
    "  <tr style=\"border:1px solid black\">\n",
    "    <td style=\"border:1px solid black\">Sensitivity to noise</td>\n",
    "    <td style=\"border:1px solid black\">Sensitive to noise</td>\n",
    "    <td style=\"border:1px solid black\">Less sensitive to noise</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f2b24b",
   "metadata": {},
   "source": [
    "## 10. How does the apriori concept aid in the reduction of measurement overhead in a business basket analysis? Give an example to demonstrate your point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78546173",
   "metadata": {},
   "source": [
    "> The Apriori concept helps in the reduction of measurement overhead in a business basket analysis by identifying frequent itemsets. **Frequent itemsets** are sets of items that appear together in a transaction more often than would be expected by chance.\n",
    "\n",
    "> The **Apriori algorithm works** by generating a set of candidate frequent itemsets and then checking each candidate to see if it is actually frequent. The Apriori algorithm uses a pruning technique to reduce the number of candidate frequent itemsets that need to be checked.\n",
    "\n",
    "> The **pruning technique** is based on the Apriori property, which states that if an itemset is not frequent, then no subset of that itemset can be frequent. This means that the Apriori algorithm can safely ignore any candidate frequent itemset that does not contain any frequent itemsets as a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55f6dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3583fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
