{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "969f761b",
   "metadata": {},
   "source": [
    "<h1> <u> <font color= green > ML_Assignment_17 </font> </u> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d17df4c",
   "metadata": {},
   "source": [
    "## 1. Using a graph to illustrate slope and intercept, define basic linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef591f0",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "<img src=\"https://online.stat.psu.edu/stat200/sites/stat200_fa21/files/inline-images/RegressionLineExample2_0.png\" style=\"height:300px\">\n",
    "\n",
    "> In basic **linear regression**, the relationship between the dependent variable and the independent variables is assumed to be linear. This means that the dependent variable can be modeled as a straight line function of the independent variables.\n",
    "\n",
    "> The **slope of the line** is a measure of how much the dependent variable changes as the independent variable changes. The intercept is the point at which the **line crosses the y-axis**.\n",
    "\n",
    "> The slope and intercept of a linear regression model can be estimated using least squares regression. **Least squares regression **is a statistical method for finding the line that best fits the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dac3be2",
   "metadata": {},
   "source": [
    "## 2. In a graph, explain the terms rise, run, and slope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2343d115",
   "metadata": {},
   "source": [
    "> In a graph, **the rise** is the change in the dependent variable as the independent variable increases by one unit. The **run** is the change in the independent variable as the dependent variable increases by one unit. The **slope** is the ratio of the rise to the run.\n",
    "\n",
    "> For example, if the rise is 2 units and the run is 3 units, then the slope is 2/3. This means that for every 2 units the dependent variable increases, the independent variable increases by 3 units.\n",
    "\n",
    "> The slope can be calculated using the following formula: ```slope = rise / run```\n",
    "\n",
    "> **The rise** is the change in the dependent variable between the two points, and the **run** is the change in the independent variable between the two points.\n",
    "\n",
    "> The slope of a line can be used to determine the direction and steepness of the line. \n",
    "> * A **positive slope** indicates that the line is increasing as the independent variable increases, \n",
    "> * A **negative slope** indicates that the line is decreasing as the independent variable increases. \n",
    "> * A **steeper slope** indicates that the line is increasing or decreasing more rapidly.\n",
    "\n",
    "> Here is an example of a graph that illustrates the rise, run, and slope:\n",
    "graph showing a line with a rise of 2 units and a run of 3 units. The slope of the line is 2/3. \n",
    "\n",
    "<img src = \"https://math.libretexts.org/@api/deki/files/62514/Screen_Shot_2021-06-24_at_5.06.34_PM.png?revision=1&size=bestfit&width=400&height=386\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265399a1",
   "metadata": {},
   "source": [
    "## 3. Demonstrate slope, linear positive slope, and linear negative slope, as well as the different conditions that contribute to the slope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0cf03b",
   "metadata": {},
   "source": [
    "> * The **line with the linear positive slope** goes from bottom left to top right. This means that as the independent variable increases, the dependent variable also increases. The slope of this line is positive, because the rise is positive and the run is positive.\n",
    "> * The **line with the linear negative slope** goes from top left to bottom right. This means that as the independent variable increases, the dependent variable decreases. The slope of this line is negative, because the rise is negative and the run is positive.\n",
    "> * The **line with no slope** is a horizontal line. This means that the dependent variable does not change as the independent variable changes. The slope of this line is 0, because the rise is 0 and the run is positive.\n",
    "\n",
    "> The different conditions that contribute to the slope of a line include the following:\n",
    "> * **The direction of the line**: If the line goes from bottom left to top right, the slope is positive. If the line goes from top left to bottom right, the slope is negative.\n",
    "> * **The steepness of the line**: The steeper the line, the more rapid the change in the dependent variable as the independent variable changes.\n",
    "> * **The number of data points**: The more data points that are used to draw the line, the more accurate the slope will be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a036642",
   "metadata": {},
   "source": [
    "## 4. Use a graph to demonstrate curve linear negative slope and curve linear positive slope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88135f84",
   "metadata": {},
   "source": [
    "> here is a graph that demonstrates curve linear negative slope and curve linear positive slope:\n",
    "\n",
    "<img src = \"https://s3-us-west-2.amazonaws.com/courses-images/wp-content/uploads/sites/2043/2017/07/01211737/IMG_Econ_01_008.png\" style=\"width:300px\" > \n",
    "<img src = \"https://s3-us-west-2.amazonaws.com/courses-images/wp-content/uploads/sites/2043/2017/07/01212014/IMG_Econ_01_009.png\" style=\"width:300px\" >\n",
    "\n",
    "> The **curve with the linear negative slope** starts at the top left and curves down as it moves to the right. This means that as the independent variable increases, the dependent variable decreases. The slope of this curve is negative, because the rise is negative and the run is positive.\n",
    "\n",
    "> The **curve with the linear positive slope** starts at the bottom left and curves up as it moves to the right. This means that as the independent variable increases, the dependent variable increases. The slope of this curve is positive, because the rise is positive and the run is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34b7df1",
   "metadata": {},
   "source": [
    "## 5. Use a graph to show the maximum and low points of curves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f37761",
   "metadata": {},
   "source": [
    "<img src = \"https://www.radfordmathematics.com/calculus/Differentiation/stationary-points/stationary-points-different-types-of-turning-points-illustration.png\" >\n",
    "\n",
    "> The **maximum point of the curve** is the point where the curve reaches its highest value. The maximum point of the curve is indicated by the peak of the curve.\n",
    "\n",
    "> The **minimum point of the curve** is the point where the curve reaches its lowest value.  The minimum point of the curve is indicated by the valley of the curve.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd57bdd6",
   "metadata": {},
   "source": [
    "## 6. Use the formulas for a and b to explain ordinary least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40b1932",
   "metadata": {},
   "source": [
    "> **Ordinary least squares (OLS)** is a statistical method for estimating the parameters of a linear regression model. The **OLS** method minimizes the sum of the squared residuals, which are the differences between the observed values of the dependent variable and the predicted values of the dependent variable.\n",
    "\n",
    "> The formulas for a and b in OLS are as follows:\n",
    "> * a: The **intercept of the line**, which is the point at which the line crosses the y-axis.\n",
    "> * b: The **slope of the line**, which is a measure of how much the dependent variable changes as the independent variable changes.\n",
    "\n",
    "> The formulas for a and b are derived from the OLS objective function, which is the sum of the squared residuals. The **OLS objective function** is minimized by setting the partial derivatives of the objective function with respect to a and b equal to 0.\n",
    "\n",
    "> The formulas for a and b can be written in matrix notation as follows:\n",
    "\n",
    "                    a = (X'X)^(-1)X'y\n",
    "                    b = (X'X)^(-1)y\n",
    "                    \n",
    "                        where:\n",
    "\n",
    "                            X: The matrix of independent variables.\n",
    "                            y: The vector of dependent variables.\n",
    "                            ': The transpose operator.\n",
    "                            ^(-1): The inverse operator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3351aa",
   "metadata": {},
   "source": [
    "## 7. Provide a step-by-step explanation of the OLS algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d403e49",
   "metadata": {},
   "source": [
    "> step-by-step explanation of the OLS algorithm are:\n",
    "> 1. **Gather the data**: The first step is to gather the data that we will use to train the model. The data should include the dependent variable that we want to predict, as well as the independent variables that we think will be predictive of the dependent variable.\n",
    "> 2. **Create the regression model**: The next step is to create the regression model. This involves specifying the form of the model, as well as the values of the parameters. The most common form of the regression model is a linear model, but other forms are also possible.\n",
    "> 3. **Estimate the parameters**: The next step is to estimate the parameters of the model. This is done using the OLS algorithm. The OLS algorithm minimizes the sum of the squared residuals, which are the differences between the observed values of the dependent variable and the predicted values of the dependent variable.\n",
    "> 4. **Make predictions**: Once the parameters of the model have been estimated, we can use the model to make predictions. This involves substituting the values of the independent variables into the model to predict the value of the dependent variable.\n",
    "> 5. **Evaluate the model**: The final step is to evaluate the model. This involves assessing the accuracy of the predictions that the model makes. There are a number of different ways to evaluate a regression model, such as the mean squared error and the R-squared statistic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783bb4ab",
   "metadata": {},
   "source": [
    "## 8. What is the regression's standard error? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4247936",
   "metadata": {},
   "source": [
    "> The **regression's standard error** is a measure of the uncertainty in the regression model's predictions. It is calculated as the square root of the mean squared error. The smaller the standard error, the more confident we can be in the model's predictions.\n",
    "\n",
    "> * The **standard error can be represented graphically** by plotting the residuals from the regression model against the predicted values. The **residuals** are the differences between the observed values of the dependent variable and the predicted values of the dependent variable. \n",
    "> * If the standard error is small, the residuals will be scattered randomly around the line of best fit. If the standard error is large, the residuals will be more spread out.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92487d2",
   "metadata": {},
   "source": [
    "## 9. Provide an example of multiple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0515d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create the data\n",
    "x1 = np.linspace(0, 10, 100)\n",
    "x2 = np.linspace(0, 10, 100)\n",
    "y = 2 * x1 + 3 * x2 + np.random.randn(100)\n",
    "\n",
    "# Create the regression model\n",
    "model = LinearRegression()\n",
    "model.fit(np.stack([x1, x2], axis=1), y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(np.stack([x1, x2], axis=1))\n",
    "\n",
    "# Plot the data and the predictions\n",
    "plt.scatter(x1, y)\n",
    "plt.plot(x1, y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d043646c",
   "metadata": {},
   "source": [
    "> This code creates a **multiple linear regression model** that predicts the value of y as a function of x1 and x2. The model is fit to the data using the LinearRegression class from the <b><i> sklearn.linear_model library </i></b>. The predictions are made using the **predict() method** of the model. The code then plots the data and the predictions.\n",
    "\n",
    "> The **output of the code** is a graph that shows the data and the predictions. The **predictions** are a line that fits the data well. This indicates that the model is able to predict the value of y as a function of x1 and x2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12161ab",
   "metadata": {},
   "source": [
    "## 10. Describe the regression analysis assumptions and the BLUE principle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e771c8",
   "metadata": {},
   "source": [
    "> **Regression analysis** is a statistical method that is used to model the relationship between a dependent variable and one or more independent variables. The **regression analysis assumptions** are a set of conditions that must be met in order for the regression model to be valid.\n",
    "\n",
    "> The regression analysis assumptions are as follows:\n",
    "> * **Linearity**: The relationship between the independent variables and the dependent variable must be linear.\n",
    "> * **Homoscedasticity**: The variance of the residuals must be constant across all values of the independent variables.\n",
    "> * **Normality**: The residuals must be normally distributed.\n",
    "> * **Independence**: The residuals must be independent of each other.\n",
    "> * **Multicollinearity**: The independent variables must not be too highly correlated with each other.\n",
    "\n",
    "> The **BLUE principle** states that the ordinary least squares (OLS) estimator is the **best linear unbiased estimator (BLUE)** of the regression coefficients. This means that the **OLS estimator** is the most accurate estimator of the regression coefficients, given the assumptions of the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64267446",
   "metadata": {},
   "source": [
    "## 11. Describe two major issues with regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b566de",
   "metadata": {},
   "source": [
    "> two major issues with regression analysis are:\n",
    "\n",
    "> **Multicollinearity**: Multicollinearity is a problem that occurs when two or more independent variables are highly correlated with each other. This can cause problems with the regression model, such as making the coefficients of the independent variables unstable and making the predictions of the model less accurate.\n",
    "\n",
    "> **Outliers**: Outliers are data points that are very different from the rest of the data. Outliers can cause problems with the regression model, such as making the coefficients of the independent variables biased and making the predictions of the model less accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749a5db0",
   "metadata": {},
   "source": [
    "## 12. How can the linear regression model's accuracy be improved?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7e17c4",
   "metadata": {},
   "source": [
    "> There are a number of ways to improve the accuracy of a linear regression model. Some of the most common methods include:\n",
    "> * **Data preprocessing**: Data preprocessing is the process of cleaning the data, removing outliers, and imputing missing values. This is an important step in improving the accuracy of a linear regression model because it can help to remove noise from the data and make the model more stable.\n",
    "> * **Feature selection**: Feature selection is the process of selecting the most relevant features for the model. This is an important step because it can help to reduce the complexity of the model and improve its accuracy.\n",
    "> * **Model selection**: Model selection is the process of choosing the right type of model for the data. There are a number of different types of linear regression models, such as simple linear regression, multiple linear regression, and polynomial regression. The best type of model for the data will depend on the nature of the data and the goal of the analysis.\n",
    "> * **Model tuning**: Model tuning is the process of adjusting the parameters of the model to improve its accuracy. This can be done by trial and error or by using a more systematic approach, such as grid search or random search.\n",
    "> * **Ensemble methods**: Ensemble methods are techniques that combine multiple models to improve their accuracy. There are a number of different ensemble methods, such as bagging, boosting, and stacking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d243bb9a",
   "metadata": {},
   "source": [
    "## 13. Using an example, describe the polynomial regression model in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34906276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create the data\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = x**2 + np.random.randn(100)\n",
    "\n",
    "# Create the regression model\n",
    "model = LinearRegression()\n",
    "model.fit(x[:, np.newaxis], y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(x[:, np.newaxis])\n",
    "\n",
    "# Plot the data and the predictions\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508af70e",
   "metadata": {},
   "source": [
    "> This code creates a **polynomial regression model** that predicts the value of y as a function of x. The **model** is fit to the data using the **LinearRegression class** from the <b><i> sklearn.linear_model library </i></b>. The **predictions** are made using the **predict()** method of the model. The code then plots the data and the predictions.\n",
    "\n",
    "> The output of the code is a graph that shows the data and the predictions. The **predictions** are a **quadratic curve** that fits the data well. This indicates that the model is able to predict the value of y as a function of x."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e01540",
   "metadata": {},
   "source": [
    "## 14. Provide a detailed explanation of logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f3d9e1",
   "metadata": {},
   "source": [
    "> **Logistic regression** is a statistical method that is used to predict the probability of a **binary outcome**. The outcome can be any categorical variable with two possible values, such as \"yes\" or \"no,\" \"success\" or \"failure,\" or \"male\" or \"female.\"\n",
    "\n",
    "> Logistic regression is a type of regression analysis, but it is different from linear regression in that it is used to **predict probabilities** rather than continuous values. Logistic regression models the relationship between the independent variables and the dependent variable using a logistic function.\n",
    "\n",
    "> The **logistic function** is a sigmoid function that takes a real number as input and returns a value between 0 and 1. The **sigmoid function** is often used to model the probability of an event occurring.\n",
    "\n",
    "> The logistic regression model is fit to the data using the **maximum likelihood estimation (MLE) method**. The MLE method estimates the parameters of the model that maximize the likelihood of the data.\n",
    "\n",
    "> Once the model is fit, it can be used to predict the probability of the outcome for a new observation. The **predicted probability** is calculated by using the logistic function to transform the output of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29d0a1f",
   "metadata": {},
   "source": [
    "## 15. What are the logistic regression assumptions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a0eec9",
   "metadata": {},
   "source": [
    "> Here are the **logistic regression** assumptions:\n",
    "> * **Linearity**: The relationship between the independent variables and the log odds of the dependent variable is linear.\n",
    "> * **Homoscedasticity**: The variance of the errors is constant across all values of the independent variables.\n",
    "> * **Normality**: The errors are normally distributed.\n",
    "> * **Independence**: The errors are independent of each other.\n",
    "> * **Multicollinearity**: The independent variables are not too highly correlated with each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a104f027",
   "metadata": {},
   "source": [
    "## 16. Go through the details of maximum likelihood estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081a11cb",
   "metadata": {},
   "source": [
    "> **Maximum likelihood estimation (MLE)** is a statistical method for estimating the parameters of a model. MLE estimates the parameters that maximize the likelihood of the data. The **likelihood of the data** is a measure of how likely the data is given the model. The higher the likelihood, the more likely the data is given the model.\n",
    "\n",
    "> The MLE method is used to **estimate the parameters of a model** by finding the values of the parameters that maximize the likelihood of the data. The likelihood of the data is calculated using a likelihood function. The likelihood function is a mathematical function that depends on the parameters of the model and the observed data.\n",
    "\n",
    "> The MLE method is a **iterative method**. This means that it starts with an initial guess for the parameters of the model and then it iteratively updates the parameters until the likelihood of the data is maximized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61faefd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168226d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
