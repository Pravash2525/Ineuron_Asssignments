{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "130dc62d",
   "metadata": {},
   "source": [
    "<h1> <u> <font color= green > ML_Assignment_16 </font> </u> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6955c33a",
   "metadata": {},
   "source": [
    "## 1. In a linear equation, what is the difference between a dependent variable and an independent variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6e890e",
   "metadata": {},
   "source": [
    "> In a linear equation, the **independent variable** is the variable that is being manipulated, and the **dependent variable** is the variable that is being observed. The **independent variable** is typically denoted by the letter x, and the **dependent variable** is typically denoted by the letter y.\n",
    "\n",
    "> For example, in the equation: ```y = mx + b```, where m and b are constants, x is the independent variable and y is the dependent variable. x is being manipulated, and y is being observed.\n",
    "The independent variable is also known as the **predictor variable**, and the dependent variable is also known as the **response variable**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d7ff4c",
   "metadata": {},
   "source": [
    "## 2. What is the concept of simple linear regression? Give a specific example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dfadc0",
   "metadata": {},
   "source": [
    "> **Simple linear regression** is a statistical method that attempts to model the relationship between one dependent variable and one independent variable. The dependent variable is typically denoted by the letter y, and the independent variable is typically denoted by the letter x.\n",
    "\n",
    "> The model is a linear equation of the form: ```y = mx + b```\n",
    "\n",
    "                where m is the slope of the line and b is the y-intercept. \n",
    "                The slope of the line tells us how much y changes when x changes by one unit. \n",
    "                The y-intercept tells us the value of y when x is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e774cd",
   "metadata": {},
   "source": [
    "## 3. In a linear regression, define the slope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0575a7",
   "metadata": {},
   "source": [
    "> The **slope** is a measure of how much the dependent variable changes when the independent variable changes by one unit. It is typically denoted by the letter <b><i> m. </i></b>\n",
    "\n",
    "> The slope can be positive, negative, or zero. \n",
    "> * A **positive slope** means that the dependent variable increases as the independent variable increases. \n",
    "> * A **negative slope** means that the dependent variable decreases as the independent variable increases. \n",
    "> * A **zero slope** means that there is no relationship between the independent and dependent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d536701e",
   "metadata": {},
   "source": [
    "## 4. Determine the graph's slope, where the lower point on the line is represented as (3, 2) and the higher point is represented as (2, 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f5ec0e",
   "metadata": {},
   "source": [
    "> * The lower point on the line is represented as (3, 2) and the higher point is represented as (2, 2). \n",
    "> * The slope of a line can be calculated using the following formula:\n",
    "\n",
    "                    m = (y2 - y1)/(x2 - x1)\n",
    "                    m = (2 - 2)/(2 - 3) = 0/-1 = 0\n",
    "\n",
    "> Therefore, the slope of the graph is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dacd501",
   "metadata": {},
   "source": [
    "## 5. In linear regression, what are the conditions for a positive slope?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50900bc6",
   "metadata": {},
   "source": [
    "> In linear regression, a **positive slope** means that the dependent variable increases as the independent variable increases. \n",
    "> The conditions for a positive slope are:\n",
    "> * **A positive correlation**: If there is a positive correlation between the independent and dependent variables, then as the independent variable increases, the dependent variable is also likely to increase.\n",
    "> * **A causal relationship**: If there is a causal relationship between the independent and dependent variables, then as the independent variable increases, it will cause the dependent variable to increase.\n",
    "> * **A spurious relationship**: A spurious relationship is a relationship that appears to be causal, but is not actually causal. In a spurious relationship, the independent variable is correlated with the dependent variable, but the correlation is not caused by a causal relationship.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51015990",
   "metadata": {},
   "source": [
    "## 6. In linear regression, what are the conditions for a negative slope?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0251a5",
   "metadata": {},
   "source": [
    "> In linear regression, a **negative slope** means that the dependent variable decreases as the independent variable increases. TThe conditions for a negative slope are:\n",
    "> * **A negative correlation**: If there is a negative correlation between the independent and dependent variables, then as the independent variable increases, the dependent variable is also likely to decrease.\n",
    "> * **A causal relationship**: If there is a causal relationship between the independent and dependent variables, then as the independent variable increases, it will cause the dependent variable to decrease.\n",
    "> * **A spurious relationship**: A spurious relationship is a relationship that appears to be causal, but is not actually causal. In a spurious relationship, the independent variable is correlated with the dependent variable, but the correlation is not caused by a causal relationship.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f55148c",
   "metadata": {},
   "source": [
    "## 7. What is multiple linear regression and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe894412",
   "metadata": {},
   "source": [
    "> **Multiple linear regression** is a statistical method that attempts to model the relationship between one dependent variable and multiple independent variables. The dependent variable is typically denoted by the letter y, and the independent variables are typically denoted by the letters x1, x2, x3, and so on.\n",
    "\n",
    "> The model is a linear equation of the form:\n",
    "```y = m1x1 + m2x2 + m3x3 + ... + b```\n",
    "\n",
    "                    where   m1, m2, m3, and so on are the coefficients of the independent variables, and \n",
    "                            b is the y-intercept. \n",
    "            \n",
    "> * The coefficients tell us how much y changes when each independent variable changes by one unit. \n",
    "> * The y-intercept tells us the value of y when all of the independent variables are zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d48e98",
   "metadata": {},
   "source": [
    "## 8. In multiple linear regression, define the number of squares due to error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb0b5b4",
   "metadata": {},
   "source": [
    "> * The **number of squares due to error (SSE)** in multiple linear regression is a measure of how much variation in the dependent variable is not explained by the independent variables. \n",
    "> * It is calculated as the sum of the squared residuals, which are the differences between the observed values of the dependent variable and the predicted values of the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea242ac9",
   "metadata": {},
   "source": [
    "## 9. In multiple linear regression, define the number of squares due to regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205f7b86",
   "metadata": {},
   "source": [
    ">  * In multiple linear regression, the **number of squares due to regression (SSR)** is a measure of how much variation in the dependent variable is explained by the independent variables. \n",
    "> * It is calculated as the sum of the squared residuals, which are the differences between the predicted values of the dependent variable and the mean of the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b310c780",
   "metadata": {},
   "source": [
    "## 10. In a regression equation, what is multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00185736",
   "metadata": {},
   "source": [
    "> In a regression equation, **multicollinearity** is a condition in which two or more independent variables are highly correlated with each other. This can cause problems with the regression model, as it can make it difficult to determine which independent variable is actually causing the change in the dependent variable.\n",
    "\n",
    "> There are a few different **ways to check for multicollinearity** in a regression equation. \n",
    "> * One way is to **look at the correlation matrix** for the independent variables. If two or more independent variables have a correlation coefficient that is close to 1, then there is a good chance that they are collinear.\n",
    "> * Another way to check for multicollinearity is to look at the **variance inflation factors (VIFs)** for the independent variables. The VIF for an independent variable is a measure of how much the variance of that variable is inflated due to the collinearity with other independent variables. A VIF that is greater than 10 indicates that there is a significant amount of collinearity between the independent variable and the other independent variables.\n",
    "\n",
    "> to address multicollinearity: \n",
    "> * One option is to remove one of the collinear independent variables from the equation. \n",
    "> * Another option is to combine the collinear independent variables into a single composite variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35525a5f",
   "metadata": {},
   "source": [
    "## 11. What is heteroskedasticity, and what does it mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a413472",
   "metadata": {},
   "source": [
    "> In statistics, **heteroskedasticity** is a violation of the assumption of homoskedasticity. Homoskedasticity means that the variance of the error term is constant across all values of the independent variables. Heteroskedasticity means that the variance of the error term is not constant across all values of the independent variables.\n",
    "\n",
    "> **Heterogeneity** is present in linear regression when the variance of the dependent variable changes with the independent variable. This can be caused by a number of factors, such as outliers, non-linear relationships, or omitted variables.\n",
    "\n",
    "> **ways to check for heteroskedasticity** in a regression equation: \n",
    "> * One way is to look at the **residuals plot**. If the residuals plot shows a fan-shaped pattern, then there is a good chance that there is heteroskedasticity present.\n",
    "> * Another way to check for heteroskedasticity is to use the **Breusch-Pagan test** or **the White test**. These tests are statistical tests that can be used to determine whether there is heteroskedasticity present in a regression equation.\n",
    "\n",
    "> to address heteroskedasticity is present in a regression equation: \n",
    "> * One option is to use **weighted least squares (WLS)** regression. WLS regression is a type of regression that takes into account the variance of the error term.\n",
    "> * Another option is to **transform the dependent variable**. This can be done by taking the logarithm of the dependent variable, or by using a Box-Cox transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d81c471",
   "metadata": {},
   "source": [
    "## 12. Describe the concept of ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf1b4b2",
   "metadata": {},
   "source": [
    "> **Ridge regression** is a type of regularization, which is a technique that is used to prevent overfitting. Regularization works by adding a penalty to the objective function of the regression model. The **penalty** is typically proportional to the sum of the squares of the coefficients.\n",
    "\n",
    "> The **penalty term** in ridge regression is called the ridge penalty. The **ridge penalty** is a positive constant that is multiplied by the sum of the squares of the coefficients. The larger the ridge penalty, the more the coefficients are penalized.\n",
    "\n",
    "> Ridge regression can be used to solve the following problem:\n",
    "```min (RSS + λ * ||β||^2)```\n",
    "\n",
    "                where:\n",
    "\n",
    "                    RSS is the residual sum of squares\n",
    "                    λ is the ridge penalty\n",
    "                    β is the vector of coefficients\n",
    "\n",
    "> The **ridge penalty** has the effect of shrinking the coefficients towards zero. This helps to prevent overfitting, because it makes the coefficients less sensitive to noise in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d765dd5",
   "metadata": {},
   "source": [
    "## 13. Describe the concept of lasso regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64decf08",
   "metadata": {},
   "source": [
    "> **Lasso regression** is a type of regularization, which is a technique that is used to prevent overfitting. Regularization works by adding a penalty to the objective function of the regression model. The **penalty** is typically proportional to the sum of the absolute values of the coefficients.\n",
    "\n",
    "> The **penalty term** in lasso regression is called the lasso penalty. The **lasso penalty** is a positive constant that is multiplied by the sum of the absolute values of the coefficients. The larger the lasso penalty, the more the coefficients are penalized.\n",
    "\n",
    "> Lasso regression can be used to solve the following problem:\n",
    "```min (RSS + λ * ||β||1)```\n",
    "\n",
    "                where:\n",
    "                    RSS is the residual sum of squares\n",
    "                    λ is the lasso penalty\n",
    "                    β is the vector of coefficients\n",
    "\n",
    "> The **lasso penalty** has the effect of shrinking some of the coefficients towards zero, and setting others to zero completely. This helps to prevent overfitting, because it makes the coefficients less sensitive to noise in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5119de",
   "metadata": {},
   "source": [
    "## 14. What is polynomial regression and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c0a2ae",
   "metadata": {},
   "source": [
    "> **Polynomial regression** is a type of regression analysis in which the dependent variable is modeled as a polynomial function of the independent variable. This means that the relationship between the dependent and independent variables is not linear, but instead is curved.\n",
    "\n",
    "> **Polynomial regression** is a more powerful tool than linear regression for modeling curved relationships. However, it is also more complex, and it can be more difficult to interpret the results.\n",
    "\n",
    "> The **polynomial regression model** is a linear model, but the independent variable is raised to a power. \n",
    "> * For example, a **quadratic polynomial regression model** would have the form: ```y = ax^2 + bx + c```\n",
    "\n",
    "                        where:\n",
    "\n",
    "                            a is the coefficient of the x^2 term\n",
    "                            b is the coefficient of the x term\n",
    "                            c is the y-intercept\n",
    "\n",
    "> The coefficients of the polynomial regression model can be estimated using **least squares regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce99c317",
   "metadata": {},
   "source": [
    "## 15. Describe the basis function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960c77a5",
   "metadata": {},
   "source": [
    "> In statistics, a **basis function** is a function that is used to represent a more complex function. Basis functions are typically used in linear regression and other machine learning algorithms.\n",
    "\n",
    "> There are many different types of basis functions, but some of the most common include:\n",
    "> * **Polynomial basis functions**: Polynomial basis functions are functions that are raised to a power. For example, a quadratic polynomial basis function would be x^2.\n",
    "> * **Rational basis functions**: Rational basis functions are functions that are divided by another function. For example, a rational basis function would be x/(1+x^2).\n",
    "> * **Wavelet basis functions**: Wavelet basis functions are functions that are localized in both time and frequency. This makes them well-suited for representing signals that have both temporal and frequency components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07e657c",
   "metadata": {},
   "source": [
    "## 16. Describe how logistic regression works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbd96f6",
   "metadata": {},
   "source": [
    "> **Logistic regression** is a statistical method that is used to predict the probability of a **binary outcome**. The outcome can be any categorical variable with two possible values, such as \"yes\" or \"no,\" \"true\" or \"false,\" or \"success\" or \"failure.\"\n",
    "\n",
    "> Logistic regression is a type of regression analysis, but it is different from linear regression. In **linear regression**, the dependent variable is a continuous variable, such as height or weight. In **logistic regression**, the dependent variable is a binary variable, so it can only have two possible values.\n",
    "\n",
    "> Logistic regression works by modeling the **probability of the binary outcome** as a function of the independent variables. The independent variables can be any type of variable, such as continuous variables, categorical variables, or even ordinal variables.\n",
    "\n",
    "> The logistic regression model is a **logistic function**, which is a function that takes a continuous variable and outputs a probability. The logistic function is typically represented by the following equation:\n",
    "```p(x) = 1 / (1 + e^(-x))```\n",
    "\n",
    "                    where:\n",
    "\n",
    "                        p(x) is the probability of the binary outcome\n",
    "                        x is a continuous variable\n",
    "                        e is the exponential function\n",
    "\n",
    ">  The logistic regression model is estimated using a maximum likelihood estimation procedure. The maximum likelihood estimation procedure finds the coefficients of the logistic regression model that maximize the likelihood of the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06a9705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5ccbd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
