{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f79c2d04",
   "metadata": {},
   "source": [
    "<h1> <u> <font color= green > ML_Assignment_5 </font> </u> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324ee51a",
   "metadata": {},
   "source": [
    "## 1. What are the key tasks that machine learning entails? What does data pre-processing imply?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930fde19",
   "metadata": {},
   "source": [
    "> The key tasks that machine learning entails are:\n",
    "> * **Data pre-processing**: This is the process of preparing the data for machine learning algorithms. This includes tasks such as <b><i> cleaning the data, removing outliers, and transforming the data </i></b> into a format that can be understood by the machine learning algorithm.\n",
    "> * **Feature selection**: This is the process of selecting the most important features from the data. This is important because not all features are equally important, and some features may actually be harmful to the machine learning algorithm.\n",
    "> * **Model training**: This is the process of training the machine learning algorithm on the data. This involves feeding the data to the algorithm and allowing it to learn the relationships between the features and the target variable.\n",
    "> * **Model evaluation**: This is the process of evaluating the performance of the machine learning algorithm. This involves testing the algorithm on new data and seeing how well it performs.\n",
    " > * **Model deployment**: This is the process of deploying the machine learning algorithm into production. This involves making the algorithm available to users so that they can use it to make predictions.\n",
    "\n",
    "\n",
    "> The tasks that are typically involved in data pre-processing are:\n",
    "> * **Cleaning the data**: This includes <i> removing errors, outliers, and missing values </i> from the data.\n",
    "> * **Transforming the data**: This includes converting the data into a format that can be understood by the machine learning algorithm. For example, categorical data may need to be converted into numerical data.\n",
    "> * **Feature scaling**: This involves normalizing the data so that all of the features have a similar range of values. This is important because it helps to prevent the machine learning algorithm from being biased towards features with large values.\n",
    "> * **Feature engineering**: This involves creating new features from the existing features. This can be done to improve the performance of the machine learning algorithm.\n",
    "\n",
    "> **Data pre-processing** is a complex and time-consuming task, but it is essential for ensuring the success of the machine learning process. It is important to ensure that the data is clean and prepared correctly before it is used to train the machine learning algorithm. This will help to ensure that the algorithm is able to learn the relationships between the features and the target variable accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3284554",
   "metadata": {},
   "source": [
    "## 2. Describe quantitative and qualitative data in depth. Make a distinction between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac4535e",
   "metadata": {},
   "source": [
    "> **Quantitative data** is data that can be measured and counted, that means it is numerical. It is typically represented by numbers. quantitative data can be analyzed using statistical methods. For example, the number of people in a survey, the price of a product, or the weight of an object are all quantitative data.\n",
    "\n",
    "> **Qualitative data** is data that cannot be measured or counted, that means it is is non-numerical. It is typically represented by words or phrases. qualitative data can be analyzed using more qualitative methods, such as content analysis or thematic analysis. For example, the color of a car, the taste of a food, or the feeling of happiness are all qualitative data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8fb218",
   "metadata": {},
   "source": [
    "## 3. Create a basic data collection that includes some sample records. Have at least one attribute from each of the machine learning data types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23753288",
   "metadata": {},
   "source": [
    "> <table >\n",
    "  <tr>\n",
    "    <th>Attribute</th>\n",
    "    <th>Data type</th>\n",
    "    <th>Description</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Name</td>\n",
    "    <td>Text</td>\n",
    "    <td>The name of the person.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Age</td>\n",
    "    <td>Integer</td>\n",
    "    <td>The age of the person.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Gender</td>\n",
    "    <td>Categorical</td>\n",
    "    <td>The gender of the person (male, female, or other).</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Height</td>\n",
    "    <td>Float</td>\n",
    "    <td>The height of the person in meters.</td>\n",
    "  </tr> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c63e22",
   "metadata": {},
   "source": [
    "## 4. What are the various causes of machine learning data issues? What are the ramifications?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fe5ef6",
   "metadata": {},
   "source": [
    "> The various causes of machine learning data issues are:\n",
    "> * **Data quality**: The data used to train a machine learning model must be of high quality. This means that the data must be accurate, complete, and consistent. If the data is not of high quality, the machine learning model will not be able to learn effectively.\n",
    "> * **Data bias**: Bias can be introduced into machine learning data in a number of ways. For example, if the data is collected from a biased sample, the machine learning model will learn to reflect that bias. This can lead to the model making inaccurate predictions.\n",
    "> * **Data imbalance**: Data imbalance occurs when there is a disproportionate amount of data for one class or category. For example, if a machine learning model is being trained to predict whether a patient has cancer, there may be much more data for patients who do not have cancer than for patients who do. This can lead to the model making inaccurate predictions.\n",
    "> * **Data noise**: Noise is any irrelevant or extraneous data that can interfere with the learning process. For example, if a machine learning model is being trained to predict the price of a house, there may be noise in the data such as typos or errors in the measurements. This noise can lead to the model making inaccurate predictions.\n",
    "\n",
    "> The term **\"ramifications\"** typically refers to the consequences or outcomes of a particular action, decision, or event. The **ramifications of machine learning** data issues can be significant. If the data is not of high quality, the machine learning model will not be able to learn effectively. This can lead to the model making inaccurate predictions, which can have negative consequences. For example, a machine learning model that is used to predict whether a patient has cancer may make inaccurate predictions if the data is not of high quality. This could lead to patients not receiving the treatment they need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c7283b",
   "metadata": {},
   "source": [
    "## 5. Demonstrate various approaches to categorical data exploration with appropriate examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2bd7bd",
   "metadata": {},
   "source": [
    "> Approaches to categorical data exploration are:\n",
    "> * **Bar charts**: Bar charts are a simple and effective way to visualize categorical data. They can be used to show the frequency of different categories or to compare the frequencies of different categories. For example, a bar chart could be used to show the distribution of political affiliation in a population. \n",
    "> * **Pie charts**: Pie charts are another way to visualize categorical data. They are often used to show the relative sizes of different categories. For example, a pie chart could be used to show the percentage of people in a population who are male and female. \n",
    "> * **Frequency tables**: Frequency tables are a more detailed way to summarize categorical data. They show the number of times each category appears in the data. For example, a frequency table could be used to show the number of people in a population who are each political affiliation. \n",
    "> * **Cross-tabulations**: Cross-tabulations are a way to explore the relationship between two categorical variables. They show the frequency of each category in one variable for each category in another variable. For example, a cross-tabulation could be used to show the relationship between political affiliation and gender in a population. \n",
    "> * **Chi-squared tests**: Chi-squared tests are a statistical test that can be used to determine if there is a significant relationship between two categorical variables. For example, a chi-squared test could be used to determine if there is a significant relationship between political affiliation and gender in a population.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814af426",
   "metadata": {},
   "source": [
    "## 6. How would the learning activity be affected if certain variables have missing values? Having said that, what can be done about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e7499f",
   "metadata": {},
   "source": [
    "> Missing values in a dataset can affect the learning activity in a number of ways.\n",
    "> * **Reduce the accuracy of the model**: If the missing values are not dealt with properly, they can reduce the accuracy of the model. This is because the model will not be able to learn from the data that is missing.\n",
    "> * **Bias the model**: If the missing values are not distributed evenly across the dataset, they can bias the model. This is because the model will be more likely to learn from the data that is present, which may not be representative of the entire dataset.\n",
    "> * **Make the model unstable**: If there are too many missing values, the model may become unstable. This is because the model will not have enough data to learn from, and it may be too sensitive to small changes in the data.\n",
    "\n",
    "> There are a number of things that can be done to deal with missing values in a dataset.\n",
    "> * **Remove the rows with missing values**: This is the simplest approach, but it can also be the most drastic. If too many rows are removed, the dataset may become too small to be useful.\n",
    "> * **Impute the missing values**: This involves filling in the missing values with estimates. There are a number of different imputation methods available, and the best method to use will depend on the specific data.\n",
    "> * **Model the missing values**: This involves treating the missing values as a separate variable and modeling their distribution. This can be a more complex approach, but it can also be more effective in some cases.\n",
    "> * **Use a validation set**: A validation set is a set of data that is not used to train the model. The validation set can be used to evaluate the different imputation methods and to select the best method.\n",
    "> * **Monitor the model**: Once the model is trained, it is important to monitor its performance over time. If the model's performance starts to deteriorate, it may be necessary to revisit the issue of missing values.\n",
    "\n",
    "> The best approach to dealing with missing values will depend on the specific dataset and the learning activity. However, it is important to address the issue of missing values, as they can have a significant impact on the accuracy and stability of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af369657",
   "metadata": {},
   "source": [
    "## 7. Describe the various methods for dealing with missing data values in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d955bf44",
   "metadata": {},
   "source": [
    "> There are many different methods for dealing with missing data values. The best method to use will depend on the specific dataset and the learning activity.\n",
    "> * **Remove the rows with missing values**: This is the simplest approach to dealing with missing data values. It involves removing all rows that contain missing values. This can be done using the dropna() function in Python or the na.drop() function.\n",
    "> * **Impute the missing values**: This approach involves filling in the missing values with estimates. There are a number of different imputation methods available, including:\n",
    "    * <b><i> Mean imputation </i></b>: This involves filling in the missing values with the mean of the non-missing values.\n",
    "    * <b><i> Median imputation </i></b>: This involves filling in the missing values with the median of the non-missing values.\n",
    "    * <b><i> Mode imputation </i></b>: This involves filling in the missing values with the mode of the non-missing values.\n",
    "    * <b><i> KNN imputation </i></b>: This involves filling in the missing values with the values of the k nearest neighbors.\n",
    "    * <b><i> Regression Imputation </i></b>: Missing values can be imputed by predicting them from other variables using regression models. \n",
    "    * <b><i> Model-Based Imputation </i></b>: Model-based imputation involves creating a model to estimate the missing values based on the observed data. \n",
    "> * **Model the missing values**: This approach involves treating the missing values as a separate variable and modeling their distribution. This can be done using a variety of statistical models, such as a logistic regression model or a Bayesian model.\n",
    "> * **Use a mixture model**: This approach combines the first two approaches by removing rows with too many missing values and imputing the missing values in the remaining rows. This can be done using a variety of statistical models, such as a mixture of Gaussian models or a mixture of Bernoulli models.\n",
    "> * **Use a validation set**: A validation set is a set of data that is not used to train the model. The validation set can be used to evaluate the different imputation methods and to select the best method.\n",
    "> * **Monitor the model**: Once the model is trained, it is important to monitor its performance over time. If the model's performance starts to deteriorate, it may be necessary to revisit the issue of missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd80551",
   "metadata": {},
   "source": [
    "## 8. What are the various data pre-processing techniques? Explain dimensionality reduction and function selection in a few words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e0ab5",
   "metadata": {},
   "source": [
    "> Various data pre-processing techniques:\n",
    "> * **Data cleaning**: This involves removing errors, outliers, and missing values from the data.\n",
    "> * **Data transformation**: This involves converting the data into a format that can be understood by the machine learning algorithm. For example, categorical data may need to be converted into numerical data.\n",
    "> * **Feature selection**: This involves selecting the most important features from the data. This is important because not all features are equally important, and some features may actually be harmful to the machine learning algorithm.\n",
    "> * **Dimensionality reduction**: This involves reducing the number of features in the data. This can be done to improve the performance of the machine learning algorithm and to make the data more interpretable.\n",
    "> * **Feature engineering**: This involves creating new features from the existing features. This can be done to improve the performance of the machine learning algorithm and to make the data more interpretable.\n",
    "\n",
    "> Dimensionality reduction and function selection are two specific data pre-processing techniques that are often used to improve the performance of machine learning algorithms.\n",
    "> * **Dimensionality reduction**: This technique involves reducing the number of features in the data. This can be done by using a variety of methods, such as principal component analysis (PCA), linear discriminant analysis (LDA), and manifold learning. Dimensionality reduction can improve the performance of machine learning algorithms by making the data more manageable and by reducing the risk of overfitting.\n",
    "> * **Function selection**: This technique involves selecting the most important features from the data. This can be done by using a variety of methods, such as univariate feature selection, recursive feature elimination, and stepwise feature selection. Function selection can improve the performance of machine learning algorithms by making the data more interpretable and by reducing the risk of overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af52daf",
   "metadata": {},
   "source": [
    "## 9. \n",
    "## i. What is the IQR? What criteria are used to assess it? \n",
    "## ii. Describe the various components of a box plot in detail? When will the lower whisker surpass the upper whisker in length? How can box plots be used to identify outliers?\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4551f02c",
   "metadata": {},
   "source": [
    "<b><i> i. What is the IQR? What criteria are used to assess it? </i></b>\n",
    "> The **interquartile range (IQR)** is a measure of variability in a data set. It is calculated by taking the difference between the third quartile (Q3) and the first quartile (Q1). The IQR is often used to assess the spread of a data set and to identify outliers.\n",
    "\n",
    "> The criteria used to assess the IQR are:\n",
    "> * The IQR should be greater than 0: This means that there is some spread in the data set.\n",
    "> * The IQR should be relatively consistent across different data sets: This means that the data sets are similar in terms of their spread.\n",
    "> * The IQR should be used in conjunction with other measures of variability:, such as the mean and standard deviation.\n",
    "---\n",
    "<b><i> ii. Describe the various components of a box plot in detail? When will the lower whisker surpass the upper whisker in length? How can box plots be used to identify outliers? </i></b>\n",
    "\n",
    "> A **box plot** is a graphical representation of a data set. It shows the distribution of the data by dividing it into quartiles. The box plot consists of the following components:\n",
    "> * **The median**: This is the middle value of the data set. It is represented by a line in the box plot.\n",
    "> * **The first quartile (Q1)**: This is the value that cuts off the bottom 25% of the data set. It is represented by the bottom of the box plot.\n",
    "> * **The third quartile (Q3)**: This is the value that cuts off the top 25% of the data set. It is represented by the top of the box plot.\n",
    "> * **The interquartile range (IQR)**: This is the difference between Q3 and Q1. It is represented by the length of the box plot.\n",
    "> * **The whiskers**: These are the lines that extend from the box plot. They show the range of the data set.\n",
    "> * **The outliers**: These are the points that fall outside the whiskers. They are often considered to be unusual values.\n",
    "\n",
    "> The **lower whisker** will surpass the **upper whisker** in length when there are outliers in the data set that are below Q1 or above Q3. This can happen when the data set is skewed or when there are errors in the data.\n",
    "\n",
    "> **Box plots** can be used to identify outliers by looking for points that fall outside the whiskers. **Outliers** are often considered to be unusual values, and they can have a significant impact on the results of statistical analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6c5989",
   "metadata": {},
   "source": [
    "## 10. Make brief notes on any two of the following:\n",
    "1. Data collected at regular intervals\n",
    "2. The gap between the quartiles\n",
    "3. Use a cross-tab\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08502db5",
   "metadata": {},
   "source": [
    "<b><i> 1. Data collected at regular intervals </i></b>\n",
    "> * Data collected at regular intervals is often referred to as time series data. This type of data is often used to track changes over time, such as the stock market, the weather, or the number of people visiting a website.\n",
    "> * There are a number of benefits to collecting data at regular intervals. First, it allows you to see how the data changes over time. Second, it allows you to identify trends and patterns in the data. Third, it allows you to make predictions about the future.\n",
    "> * There are also a few challenges to collecting data at regular intervals. First, it can be time-consuming and expensive. Second, it can be difficult to collect data that is accurate and complete. Third, it can be difficult to keep up with the data if the changes are happening quickly.\n",
    "\n",
    "<b><i> 2. The gap between the quartiles </i></b>\n",
    "> * The gap between the quartiles is often referred to as the interquartile range (IQR). The IQR is a measure of variability in a data set. It is calculated by taking the difference between the third quartile (Q3) and the first quartile (Q1).\n",
    "> * The IQR is often used to identify outliers. Outliers are often considered to be unusual values, and they can have a significant impact on the results of statistical analyses.\n",
    "> * The IQR can also be used to compare different data sets. If the IQRs of two data sets are very different, it means that the data sets are spread out differently.\n",
    "\n",
    "<b><i> 3. Use a cross-tab </i></b>\n",
    "> * A cross-tab is a table that shows the distribution of two categorical variables. The variables are typically displayed in rows and columns, and the cells of the table show the number of observations that fall into each combination of categories.\n",
    "> * Cross-tabs are often used to explore the relationship between two variables. For example, you could use a cross-tab to explore the relationship between gender and political affiliation.\n",
    "> * Cross-tabs can also be used to identify potential outliers. For example, if you see a cell in a cross-tab that has a very high or very low number of observations, it may be an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9d3a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364b46fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
